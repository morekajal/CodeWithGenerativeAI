{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Problem : write a program which is going to be able to look at any web page on the internet, scrape the contents of the web page and then summarize it and present back a short summary of that web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found\")\n",
    "elif api_key[:8]!= \"sk-proj-\":\n",
    "    print(\"An API Key was found, but it doesn't start with 'sk-proj-', please check you're using right api key\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create openai instance\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    A utility class to represent a Website that we have scraped.\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\" \n",
    "        Create this website object from the given url using the BeautifulSoap library.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found.\"\n",
    "\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek - Wikipedia\n",
      "Jump to content\n",
      "Main menu\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "Navigation\n",
      "Main page\n",
      "Contents\n",
      "Current events\n",
      "Random article\n",
      "About Wikipedia\n",
      "Contact us\n",
      "Contribute\n",
      "Help\n",
      "Learn to edit\n",
      "Community portal\n",
      "Recent changes\n",
      "Upload file\n",
      "Search\n",
      "Search\n",
      "Appearance\n",
      "Donate\n",
      "Create account\n",
      "Log in\n",
      "Personal tools\n",
      "Donate\n",
      "Create account\n",
      "Log in\n",
      "Pages for logged out editors\n",
      "learn more\n",
      "Contributions\n",
      "Talk\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "(Top)\n",
      "1\n",
      "Background\n",
      "2\n",
      "Development and release history\n",
      "Toggle Development and release history subsection\n",
      "2.1\n",
      "DeepSeek LLM\n",
      "2.2\n",
      "V2\n",
      "2.3\n",
      "V3\n",
      "2.4\n",
      "R1\n",
      "3\n",
      "Assessment and reactions\n",
      "4\n",
      "Concerns\n",
      "Toggle Concerns subsection\n",
      "4.1\n",
      "Censorship\n",
      "4.2\n",
      "Security and privacy\n",
      "5\n",
      "See also\n",
      "6\n",
      "Notes\n",
      "7\n",
      "References\n",
      "8\n",
      "External links\n",
      "Toggle the table of contents\n",
      "DeepSeek\n",
      "55 languages\n",
      "Afrikaans\n",
      "العربية\n",
      "Aragonés\n",
      "অসমীয়া\n",
      "Azərbaycanca\n",
      "বাংলা\n",
      "Български\n",
      "Català\n",
      "Čeština\n",
      "Dansk\n",
      "الدارجة\n",
      "Deutsch\n",
      "Ελληνικά\n",
      "Español\n",
      "Esperanto\n",
      "Euskara\n",
      "فارسی\n",
      "Français\n",
      "Frysk\n",
      "Fulfulde\n",
      "Gaeilge\n",
      "Galego\n",
      "한국어\n",
      "Bahasa Indonesia\n",
      "Italiano\n",
      "עברית\n",
      "Kiswahili\n",
      "Magyar\n",
      "Македонски\n",
      "മലയാളം\n",
      "Nederlands\n",
      "नेपाली\n",
      "日本語\n",
      "Oʻzbekcha / ўзбекча\n",
      "Polski\n",
      "Português\n",
      "Qaraqalpaqsha\n",
      "Română\n",
      "Русский\n",
      "Simple English\n",
      "Српски / srpski\n",
      "Suomi\n",
      "Svenska\n",
      "Tagalog\n",
      "தமிழ்\n",
      "ၽႃႇသႃႇတႆး\n",
      "ไทย\n",
      "Türkçe\n",
      "Українська\n",
      "اردو\n",
      "ئۇيغۇرچە / Uyghurche\n",
      "Tiếng Việt\n",
      "吴语\n",
      "粵語\n",
      "中文\n",
      "Edit links\n",
      "Article\n",
      "Talk\n",
      "English\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "Tools\n",
      "Tools\n",
      "move to sidebar\n",
      "hide\n",
      "Actions\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "General\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Get shortened URL\n",
      "Download QR code\n",
      "Print/export\n",
      "Download as PDF\n",
      "Printable version\n",
      "In other projects\n",
      "Wikimedia Commons\n",
      "Wikidata item\n",
      "Appearance\n",
      "move to sidebar\n",
      "hide\n",
      "From Wikipedia, the free encyclopedia\n",
      "Chinese artificial intelligence company\n",
      "Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\n",
      "Native name\n",
      "杭州深度求索人工智能基础技术研究有限公司\n",
      "Company type\n",
      "Private\n",
      "Industry\n",
      "Information technology\n",
      "Artificial intelligence\n",
      "Founded\n",
      "May 16, 2023\n",
      "; 20 months ago\n",
      "(\n",
      "2023-05-16\n",
      ")\n",
      "Founder\n",
      "Liang Wenfeng\n",
      "Headquarters\n",
      "Hangzhou\n",
      ",\n",
      "Zhejiang\n",
      ", China\n",
      "Key people\n",
      "Liang Wenfeng (CEO)\n",
      "Owner\n",
      "High-Flyer\n",
      "Number of employees\n",
      "Under 200\n",
      "Website\n",
      "www\n",
      ".deepseek\n",
      ".com\n",
      "Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\n",
      ", commonly referred to as\n",
      "DeepSeek\n",
      ", (\n",
      "Chinese\n",
      ":\n",
      "深度求索\n",
      ";\n",
      "pinyin\n",
      ":\n",
      "Shēndù Qiúsuǒ\n",
      ") is a Chinese\n",
      "artificial intelligence\n",
      "company that develops\n",
      "open-source\n",
      "large language models\n",
      "(LLMs). Based in\n",
      "Hangzhou, Zhejiang\n",
      ", it is owned and funded by Chinese hedge fund\n",
      "High-Flyer\n",
      ", whose co-founder,\n",
      "Liang Wenfeng\n",
      ", established the company in 2023 and serves as its\n",
      "CEO\n",
      ".\n",
      "The DeepSeek-R1 model provides responses comparable to other contemporary\n",
      "large language models\n",
      ", such as\n",
      "OpenAI\n",
      "'s\n",
      "GPT-4o\n",
      "and\n",
      "o1\n",
      ".\n",
      "[\n",
      "1\n",
      "]\n",
      "It is\n",
      "trained\n",
      "at a significantly lower cost—stated at\n",
      "US$\n",
      "6 million compared to $100 million for OpenAI's\n",
      "GPT-4\n",
      "in 2023\n",
      "[\n",
      "2\n",
      "]\n",
      "—and requires a tenth of the computing power of a comparable LLM.\n",
      "[\n",
      "2\n",
      "]\n",
      "[\n",
      "3\n",
      "]\n",
      "[\n",
      "4\n",
      "]\n",
      "DeepSeek's AI models were developed amid United States sanctions on China for\n",
      "Nvidia\n",
      "chips,\n",
      "[\n",
      "5\n",
      "]\n",
      "which were intended to restrict the ability of the country to develop advanced AI systems.\n",
      "[\n",
      "6\n",
      "]\n",
      "[\n",
      "7\n",
      "]\n",
      "On 10 January 2025, DeepSeek released its first free\n",
      "chatbot app\n",
      ", based on the DeepSeek-R1 model, for\n",
      "iOS\n",
      "and\n",
      "Android\n",
      "; by 27 January, DeepSeek-R1 had surpassed\n",
      "ChatGPT\n",
      "as the most-downloaded free app on the\n",
      "iOS App Store\n",
      "in the United States,\n",
      "[\n",
      "8\n",
      "]\n",
      "causing Nvidia's share price to drop by 18%.\n",
      "[\n",
      "9\n",
      "]\n",
      "[\n",
      "10\n",
      "]\n",
      "DeepSeek's success against larger and more established rivals has been described as \"upending AI\",\n",
      "[\n",
      "8\n",
      "]\n",
      "constituting \"the first shot at what is emerging as a global AI space race\",\n",
      "[\n",
      "11\n",
      "]\n",
      "and ushering in \"a new era of AI\n",
      "brinkmanship\n",
      "\".\n",
      "[\n",
      "12\n",
      "]\n",
      "DeepSeek makes its\n",
      "generative artificial intelligence\n",
      "algorithms, models, and training details open-source, allowing its code to be freely available for use, modification, viewing, and designing documents for building purposes.\n",
      "[\n",
      "13\n",
      "]\n",
      "The company reportedly vigorously recruits young AI researchers from top Chinese universities,\n",
      "[\n",
      "8\n",
      "]\n",
      "and hires from outside the\n",
      "computer science\n",
      "field to diversify its models' knowledge and abilities.\n",
      "[\n",
      "3\n",
      "]\n",
      "Background\n",
      "[\n",
      "edit\n",
      "]\n",
      "In February 2016, High-Flyer was co-founded by AI enthusiast Liang Wenfeng, who had been trading since the\n",
      "2007–2008 financial crisis\n",
      "while attending\n",
      "Zhejiang University\n",
      ".\n",
      "[\n",
      "14\n",
      "]\n",
      "By 2019, he established High-Flyer as a hedge fund focused on developing and using AI trading algorithms. By 2021, High-Flyer exclusively used AI in trading\n",
      "[\n",
      "15\n",
      "]\n",
      ", often using Nvidia chips.\n",
      "[\n",
      "16\n",
      "]\n",
      "DeepSeek has made its\n",
      "generative artificial intelligence\n",
      "chatbot\n",
      "open source\n",
      ", meaning its code is freely available for use, modification, and viewing. This includes permission to access and use the source code, as well as design documents, for building purposes.\n",
      "[\n",
      "13\n",
      "]\n",
      "In 2021, while running High-Flyer, Liang began stockpiling Nvidia GPUs for an AI project.\n",
      "[\n",
      "16\n",
      "]\n",
      "According to\n",
      "36Kr\n",
      ", Liang had built up a store of 10,000\n",
      "Nvidia A100\n",
      "GPUs, which are used to train AI,\n",
      "[\n",
      "17\n",
      "]\n",
      "before the United States federal government imposed AI chip restrictions on China.\n",
      "[\n",
      "15\n",
      "]\n",
      "In April 2023, High-Flyer started an\n",
      "artificial general intelligence\n",
      "lab dedicated to research developing AI tools separate from High-Flyer's financial business.\n",
      "[\n",
      "18\n",
      "]\n",
      "[\n",
      "19\n",
      "]\n",
      "In May 2023, with High-Flyer as one of the investors, the lab became its own company, DeepSeek.\n",
      "[\n",
      "15\n",
      "]\n",
      "[\n",
      "20\n",
      "]\n",
      "[\n",
      "19\n",
      "]\n",
      "Venture capital\n",
      "firms were reluctant in providing funding as it was unlikely that it would be able to generate an\n",
      "exit\n",
      "in a short period of time.\n",
      "[\n",
      "15\n",
      "]\n",
      "After releasing DeepSeek-V2 in May 2024, which offered strong performance for a low price, DeepSeek became known as the catalyst for China's AI model\n",
      "price war\n",
      ". It was quickly dubbed the \"\n",
      "Pinduoduo\n",
      "of AI\", and other major tech giants such as\n",
      "ByteDance\n",
      ",\n",
      "Tencent\n",
      ",\n",
      "Baidu\n",
      ", and\n",
      "Alibaba\n",
      "began to cut the price of their AI models to compete with the company. Despite the low price charged by DeepSeek, it was profitable compared to its rivals that were losing money.\n",
      "[\n",
      "21\n",
      "]\n",
      "DeepSeek is focused on research and has no detailed plans for commercialization;\n",
      "[\n",
      "21\n",
      "]\n",
      "this also allows its technology to avoid the most stringent provisions of China's AI regulations, such as requiring consumer-facing technology to comply with the government’s controls on information.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek's hiring preferences target technical abilities rather than work experience, resulting in most new hires being either recent university graduates or developers whose AI careers are less established.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "3\n",
      "]\n",
      "Likewise, the company recruits individuals without any computer science background to help its technology understand other topics and knowledge areas, including being able to generate poetry and perform well on the notoriously difficult\n",
      "Chinese college admissions exams (Gaokao)\n",
      ".\n",
      "[\n",
      "3\n",
      "]\n",
      "Development and release history\n",
      "[\n",
      "edit\n",
      "]\n",
      "This section\n",
      "may be too technical for most readers to understand\n",
      ".\n",
      "Please\n",
      "help improve it\n",
      "to\n",
      "make it understandable to non-experts\n",
      ", without removing the technical details.\n",
      "(\n",
      "January 2025\n",
      ")\n",
      "(\n",
      "Learn how and when to remove this message\n",
      ")\n",
      "DeepSeek LLM\n",
      "[\n",
      "edit\n",
      "]\n",
      "On 2 November 2023, DeepSeek released its first series of model,\n",
      "DeepSeek-Coder\n",
      ", which is available for free to both researchers and commercial users. The code for the model was made open-source under the\n",
      "MIT license\n",
      ", with an additional license agreement (\"DeepSeek license\") regarding \"open and responsible downstream usage\" for the model itself.\n",
      "[\n",
      "22\n",
      "]\n",
      "They are of the same architecture as DeepSeek LLM detailed below. The series includes 8 models, 4 pretrained (\n",
      "Base\n",
      ") and 4 instruction-finetuned (\n",
      "Instruct\n",
      "). They all have 16K context lengths. The\n",
      "training\n",
      "was as follows:\n",
      "[\n",
      "23\n",
      "]\n",
      "[\n",
      "24\n",
      "]\n",
      "[\n",
      "25\n",
      "]\n",
      "Pretraining: 1.8T tokens (87% source code, 10% code-related English (GitHub markdown and\n",
      "Stack Exchange\n",
      "), and 3% code-unrelated Chinese).\n",
      "Long-context pretraining: 200B tokens. This extends the context length from 4K to 16K. This produced the\n",
      "Base\n",
      "models.\n",
      "Supervised\n",
      "finetuning\n",
      "(SFT): 2B tokens of instruction data. This produced the\n",
      "Instruct\n",
      "models.\n",
      "They were trained on clusters of A100 and\n",
      "H800\n",
      "Nvidia GPUs, connected by\n",
      "InfiniBand\n",
      ",\n",
      "NVLink\n",
      ",\n",
      "NVSwitch\n",
      ".\n",
      "[\n",
      "23\n",
      "]\n",
      "DeepSeek Coder properties\n",
      "[\n",
      "23\n",
      "]\n",
      ": Table 2\n",
      "[\n",
      "26\n",
      "]\n",
      "Params\n",
      ".\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "d\n",
      "model\n",
      "{\\displaystyle d_{\\text{model}}}\n",
      "d\n",
      "intermediate\n",
      "{\\displaystyle d_{\\text{intermediate}}}\n",
      "n\n",
      "heads\n",
      "{\\displaystyle n_{\\text{heads}}}\n",
      "n\n",
      "kv-heads\n",
      "{\\displaystyle n_{\\text{kv-heads}}}\n",
      "1.3B\n",
      "24\n",
      "2048\n",
      "5504\n",
      "16\n",
      "16\n",
      "5.7B\n",
      "32\n",
      "4096\n",
      "11008\n",
      "32\n",
      "1\n",
      "[\n",
      "note 1\n",
      "]\n",
      "6.7B\n",
      "32\n",
      "4096\n",
      "11008\n",
      "32\n",
      "32\n",
      "33B\n",
      "62\n",
      "7168\n",
      "19200\n",
      "56\n",
      "7\n",
      "[\n",
      "note 1\n",
      "]\n",
      "On 29 November 2023, DeepSeek released the\n",
      "DeepSeek-LLM\n",
      "series of models, with 7B and 67B parameters in both\n",
      "Base\n",
      "and\n",
      "Chat\n",
      "forms (no\n",
      "Instruct\n",
      "was released). It was developed to compete with other LLMs available at the time. The paper claimed benchmark results higher than most open source LLMs at the time, especially Llama 2.\n",
      "[\n",
      "27\n",
      "]\n",
      ": section 5\n",
      "Like DeepSeek Coder, the code for the model was under MIT license, with DeepSeek license for the model itself.\n",
      "[\n",
      "28\n",
      "]\n",
      "The architecture was essentially the same as those of the\n",
      "Llama\n",
      "series. They used the\n",
      "pre-norm\n",
      "decoder-only Transformer\n",
      "with\n",
      "RMSNorm\n",
      "as the normalization,\n",
      "SwiGLU\n",
      "in the feedforward layers,\n",
      "rotary positional embedding\n",
      "(RoPE), and\n",
      "grouped-query attention\n",
      "(GQA). Both had vocabulary size 102,400 (\n",
      "byte-level BPE\n",
      ") and context length of 4096. They trained on 2 trillion tokens of English and Chinese text obtained by deduplicating the\n",
      "Common Crawl\n",
      ".\n",
      "[\n",
      "27\n",
      "]\n",
      "DeepSeek LLM properties\n",
      "[\n",
      "27\n",
      "]\n",
      ": Table 2\n",
      "Params\n",
      ".\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "d\n",
      "model\n",
      "{\\displaystyle d_{\\text{model}}}\n",
      "d\n",
      "intermediate\n",
      "{\\displaystyle d_{\\text{intermediate}}}\n",
      "n\n",
      "heads\n",
      "{\\displaystyle n_{\\text{heads}}}\n",
      "n\n",
      "kv-heads\n",
      "{\\displaystyle n_{\\text{kv-heads}}}\n",
      "7B\n",
      "30\n",
      "4096\n",
      "11008\n",
      "32\n",
      "32\n",
      "67B\n",
      "95\n",
      "8192\n",
      "22016\n",
      "64\n",
      "8\n",
      "[\n",
      "note 1\n",
      "]\n",
      "The\n",
      "Chat\n",
      "versions of the two\n",
      "Base\n",
      "models was also released concurrently, obtained by training\n",
      "Base\n",
      "by\n",
      "supervised finetuning (SFT) followed by direct policy optimization (DPO)\n",
      ".\n",
      "[\n",
      "27\n",
      "]\n",
      "On 9 January 2024, they released 2\n",
      "DeepSeek-MoE\n",
      "models (\n",
      "Base\n",
      ",\n",
      "Chat\n",
      "), each of 16B parameters (2.7B activated per token, 4K context length). The training was essentially the same as\n",
      "DeepSeek-LLM 7B\n",
      ", and was trained on a part of its training dataset. They claimed comparable performance with a 16B MoE as a 7B non-MoE. In architecture, it is a variant of the standard\n",
      "sparsely-gated MoE\n",
      ", with \"shared experts\" that are always queried, and \"routed experts\" that might not be. They found this to help with expert balancing. In standard MoE, some experts can become overly relied on, while other experts might be rarely used, wasting parameters. Attempting to balance the experts so that they are equally used then causes experts to replicate the same capacity. They proposed the shared experts to learn core capacities that are often used, and let the routed experts to learn the peripheral capacities that are rarely used.\n",
      "[\n",
      "29\n",
      "]\n",
      "In April 2024, they released 3\n",
      "DeepSeek-Math\n",
      "models specialized for doing math:\n",
      "Base\n",
      ",\n",
      "Instruct\n",
      ",\n",
      "RL\n",
      ". It was trained as follows:\n",
      "[\n",
      "30\n",
      "]\n",
      "Initialize with a previously pretrained\n",
      "DeepSeek-Coder-Base-v1.5 7B\n",
      ".\n",
      "Further pretrain with 500B tokens (6% DeepSeekMath Corpus, 4% AlgebraicStack, 10% arXiv, 20% GitHub code, 10% Common Crawl). This produced the\n",
      "Base\n",
      "model.\n",
      "Train an instruction-following model by SFT\n",
      "Base\n",
      "with 776K math problems and their tool-use-integrated step-by-step solutions. This produced the\n",
      "Instruct\n",
      "model.\n",
      "Reinforcement learning\n",
      "(RL): The reward model was a\n",
      "process reward model\n",
      "(PRM) trained from\n",
      "Base\n",
      "according to the Math-Shepherd method.\n",
      "[\n",
      "31\n",
      "]\n",
      "This reward model was then used to train\n",
      "Instruct\n",
      "using\n",
      "group relative policy optimization\n",
      "(GRPO) on a dataset of 144K math questions \"related to GSM8K and MATH\". The reward model was continuously updated during training to avoid reward hacking. This resulted in the\n",
      "RL\n",
      "model.\n",
      "V2\n",
      "[\n",
      "edit\n",
      "]\n",
      "In May 2024, they released the\n",
      "DeepSeek-V2\n",
      "series. The series includes 4 models, 2 base models (\n",
      "DeepSeek-V2\n",
      ",\n",
      "DeepSeek-V2-Lite\n",
      ") and 2 chatbots (\n",
      "-Chat\n",
      "). The two larger models were trained as follows:\n",
      "[\n",
      "32\n",
      "]\n",
      "Pretrain on a dataset of 8.1T tokens, where Chinese tokens are 12% more than English ones.\n",
      "Extend context length from 4K to 128K using YaRN.\n",
      "[\n",
      "33\n",
      "]\n",
      "This resulted in\n",
      "DeepSeek-V2\n",
      ".\n",
      "SFT with 1.2M instances for helpfulness and 0.3M for safety. This resulted in\n",
      "DeepSeek-V2-Chat (SFT)\n",
      "which was not released.\n",
      "RL using GRPO in two stages. The first stage was trained to solve math and coding problems. This stage used 1 reward model, trained on compiler feedback (for coding) and ground-truth labels (for math). The second stage was trained to be helpful, safe, and follow rules. This stage used 3 reward models. The helpfulness and safety reward models were trained on human preference data. The rule-based reward model was manually programmed. All trained reward models were initialized from\n",
      "DeepSeek-V2-Chat (SFT)\n",
      ". This resulted in the released version of\n",
      "DeepSeek-V2-Chat\n",
      ".\n",
      "They opted for 2-staged RL, because they found that RL on reasoning data had \"unique characteristics\" different from RL on general data. For example, RL on reasoning could improve over more training steps.\n",
      "[\n",
      "32\n",
      "]\n",
      "The two\n",
      "V2-Lite\n",
      "models were smaller, and trained similarly, though\n",
      "DeepSeek-V2-Lite-Chat\n",
      "only underwent SFT, not RL. They trained the Lite version to help \"further research and development on MLA and DeepSeekMoE\".\n",
      "[\n",
      "32\n",
      "]\n",
      "Architecturally, the V2 models were significantly modified from the DeepSeek LLM series. They changed the standard attention mechanism by a\n",
      "low-rank approximation\n",
      "called\n",
      "multi-head latent attention\n",
      "(MLA), and used the\n",
      "mixture of experts\n",
      "(MoE) variant previously published in January.\n",
      "[\n",
      "29\n",
      "]\n",
      "DeepSeek V2 properties\n",
      "[\n",
      "32\n",
      "]\n",
      ": Section 3.1.2, Appendix B\n",
      "[\n",
      "34\n",
      "]\n",
      "[\n",
      "35\n",
      "]\n",
      "Name\n",
      "Params\n",
      ".\n",
      "Active\n",
      "params\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "Context length\n",
      "n\n",
      "shared experts\n",
      "{\\displaystyle n_{\\text{shared experts}}}\n",
      "n\n",
      "routed experts\n",
      "{\\displaystyle n_{\\text{routed experts}}}\n",
      "V2-Lite\n",
      "15.7B\n",
      "2.4B\n",
      "27\n",
      "32K\n",
      "2\n",
      "64\n",
      "V2\n",
      "236B\n",
      "21B\n",
      "60\n",
      "128K\n",
      "2\n",
      "160\n",
      "The\n",
      "Financial Times\n",
      "reported that it was cheaper than its peers with a price of 2\n",
      "RMB\n",
      "for every million output tokens. The\n",
      "University of Waterloo\n",
      "Tiger Lab's leaderboard ranked DeepSeek-V2 seventh on its LLM ranking.\n",
      "[\n",
      "20\n",
      "]\n",
      "In June 2024, they released 4 models in the\n",
      "DeepSeek-Coder-V2\n",
      "series:\n",
      "V2-Base\n",
      ",\n",
      "V2-Lite-Base\n",
      ",\n",
      "V2-Instruct\n",
      ",\n",
      "V2-Lite-Instruct\n",
      ". They were trained as follows:\n",
      "[\n",
      "36\n",
      "]\n",
      "[\n",
      "note 2\n",
      "]\n",
      "The\n",
      "Base\n",
      "models were initialized from corresponding\n",
      "intermediate\n",
      "checkpoints after pretraining on 4.2T tokens (not the version at the end of pretraining), then pretrained further for 6T tokens, then context-extended to 128K context length. This produced the\n",
      "Base\n",
      "models.\n",
      "DeepSeek-Coder\n",
      "and\n",
      "DeepSeek-Math\n",
      "were used to generate 20K code-related and 30K math-related instruction data, then combined with an instruction dataset of 300M tokens. This was used for SFT.\n",
      "RL with GRPO. The reward for math problems was computed by comparing with the ground-truth label. The reward for code problems was generated by a reward model trained to predict whether a program would pass the unit tests.\n",
      "DeepSeek-V2.5\n",
      "was released in September and updated in December 2024. It was made by combining\n",
      "DeepSeek-V2-Chat\n",
      "and\n",
      "DeepSeek-Coder-V2-Instruct\n",
      ".\n",
      "[\n",
      "37\n",
      "]\n",
      "V3\n",
      "[\n",
      "edit\n",
      "]\n",
      "In December 2024, they released a base model\n",
      "DeepSeek-V3-Base\n",
      "and a chat model\n",
      "DeepSeek-V3\n",
      ". The model architecture is essentially the same as V2. They were trained as follows:\n",
      "[\n",
      "38\n",
      "]\n",
      "Pretraining on 14.8T tokens of a multilingual corpus, mostly English and Chinese. It contained a higher ratio of math and programming than the pretraining dataset of V2.\n",
      "Extend context length twice, from 4K to 32K and then to 128K, using YaRN.\n",
      "[\n",
      "33\n",
      "]\n",
      "This produced\n",
      "DeepSeek-V\n",
      "3-Base\n",
      ".\n",
      "SFT for 2 epochs on 1.5M samples of reasoning (math, programming, logic) and non-reasoning (creative writing, roleplay, simple question answering) data. Reasoning data was generated by \"expert models\". Non-reasoning data was generated by\n",
      "DeepSeek-V2.5\n",
      "and checked by humans.\n",
      "The \"expert models\" were trained by starting with an unspecified base model, then SFT on both\n",
      "<problem, original response>\n",
      "data, and synthetic\n",
      "<system prompt, problem, R1 response>\n",
      "data generated by an internal\n",
      "DeepSeek-R1\n",
      "model. The system prompt asked the\n",
      "R1\n",
      "to reflect and verify during thinking. Then the expert models were RL using an unspecified reward function.\n",
      "Each expert model was trained to generate just synthetic reasoning data in one specific domain (math, programming, logic).\n",
      "Expert models were used, instead of\n",
      "R1\n",
      "itself, since the output from\n",
      "R1\n",
      "itself suffered \"overthinking, poor formatting, and excessive length\".\n",
      "Model-based reward models were made by starting with a SFT checkpoint of\n",
      "V3\n",
      ", then finetuning on human preference data containing both final reward and chain-of-thought leading to the final reward. The reward model produced reward signals for both questions with objective but free-form answers, and questions without objective answers (such as creative writing).\n",
      "A SFT checkpoint of\n",
      "V3\n",
      "was trained by GRPO using both reward models and rule-based reward. The rule-based reward was computed for math problems with a final answer (put in a box), and for programming problems by unit tests. This produced\n",
      "DeepSeek-V3\n",
      ".\n",
      "DeepSeek V3 properties\n",
      "[\n",
      "38\n",
      "]\n",
      ": Section 4.2\n",
      "[\n",
      "39\n",
      "]\n",
      "Name\n",
      "Params\n",
      ".\n",
      "Active\n",
      "params\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "Context length\n",
      "n\n",
      "shared experts\n",
      "{\\displaystyle n_{\\text{shared experts}}}\n",
      "n\n",
      "routed experts\n",
      "{\\displaystyle n_{\\text{routed experts}}}\n",
      "V3\n",
      "671B\n",
      "37B\n",
      "61\n",
      "128K\n",
      "1\n",
      "256\n",
      "The DeepSeek team performed extensive low-level engineering to achieve efficiency. They used\n",
      "mixed-precision arithmetic\n",
      ". Much of the forward pass was performed in\n",
      "8-bit floating point numbers\n",
      "(5E2M: 5-bit exponent and 2-bit\n",
      "mantissa\n",
      ") rather than the standard\n",
      "32-bit\n",
      ", requiring special\n",
      "GEMM\n",
      "routines to accumulate accurately. They used a custom 12-bit float (E5M6) for\n",
      "only\n",
      "the inputs to the linear layers after the attention modules. Optimizer states were in 16-bit (\n",
      "BF16\n",
      "). They minimized the communication latency by overlapping extensively computation and communication, such as dedicating 20\n",
      "streaming multiprocessors\n",
      "out of 132 per H800 for only inter-GPU communication. They lowered communication by rearranging (every 10 minutes) the exact machine each expert was on in order to avoid certain machines being queried more often than the others, adding auxiliary load-balancing losses to the training loss function, and other load-balancing techniques.\n",
      "[\n",
      "38\n",
      "]\n",
      "After training, it was deployed on H800 clusters. The H800 cards within a cluster are connected by NVLink, and the clusters are connected by InfiniBand.\n",
      "[\n",
      "38\n",
      "]\n",
      "Total cost of training the DeepSeek-V3 model\n",
      "[\n",
      "38\n",
      "]\n",
      ": Table 1\n",
      "Stage\n",
      "Cost (in one thousand GPU hours)\n",
      "Cost (in one million USD$)\n",
      "Pre-training\n",
      "2,664\n",
      "5.328\n",
      "Context extension\n",
      "119\n",
      "0.24\n",
      "Fine-tuning\n",
      "5\n",
      "0.01\n",
      "Total\n",
      "2,788\n",
      "5.576\n",
      "Benchmark tests show that DeepSeek-V3 outperformed\n",
      "Llama\n",
      "3.1 and\n",
      "Qwen\n",
      "2.5 whilst matching\n",
      "GPT-4o\n",
      "and\n",
      "Claude\n",
      "3.5 Sonnet.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "40\n",
      "]\n",
      "[\n",
      "41\n",
      "]\n",
      "[\n",
      "42\n",
      "]\n",
      "R1\n",
      "[\n",
      "edit\n",
      "]\n",
      "On 20 November 2024,\n",
      "DeepSeek-R1-Lite-Preview\n",
      "became accessible via DeepSeek's API, as well as via a chat interface after logging in.\n",
      "[\n",
      "43\n",
      "]\n",
      "[\n",
      "44\n",
      "]\n",
      "[\n",
      "note 3\n",
      "]\n",
      "It was trained for logical inference, mathematical reasoning, and real-time problem-solving. DeepSeek claimed that it exceeded performance of\n",
      "OpenAI o1\n",
      "on benchmarks such as\n",
      "American Invitational Mathematics Examination\n",
      "(AIME) and MATH.\n",
      "[\n",
      "45\n",
      "]\n",
      "However,\n",
      "The Wall Street Journal\n",
      "stated when it used 15 problems from the 2024 edition of AIME, the o1 model reached a solution faster than\n",
      "DeepSeek-R1-Lite-Preview\n",
      ".\n",
      "[\n",
      "46\n",
      "]\n",
      "On 20 January 2025, DeepSeek released\n",
      "DeepSeek-R1\n",
      "and\n",
      "DeepSeek-R1-Zero\n",
      ".\n",
      "[\n",
      "47\n",
      "]\n",
      "Both were initialized from\n",
      "DeepSeek-V3-Base\n",
      ", and share its architecture. The company also released some \"\n",
      "DeepSeek-R1-Distill\n",
      "\" models, which are\n",
      "not\n",
      "initialized on\n",
      "V3-Base\n",
      ", but instead are initialized from other pretrained open-weight models, including\n",
      "LLaMA\n",
      "and\n",
      "Qwen\n",
      ", then fine-tuned on\n",
      "synthetic data\n",
      "generated by\n",
      "R1\n",
      ".\n",
      "[\n",
      "48\n",
      "]\n",
      "Template for\n",
      "DeepSeek-R1-Zero\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <prompt>. Assistant:\n",
      "– <prompt> is replaced with the specific reasoning question during training.\n",
      "DeepSeek-R1-Zero\n",
      "was trained exclusively using GRPO RL without SFT. Unlike previous versions, they used no model-based reward. All reward functions were rule-based, \"mainly\" of two types (other types were not specified): accuracy rewards and format rewards. Accuracy reward was checking whether a boxed answer is correct (for math) or whether a code passes tests (for programming). Format reward was checking whether the model puts its thinking trace within\n",
      "<think>...</think>\n",
      ".\n",
      "[\n",
      "48\n",
      "]\n",
      "As\n",
      "R1-Zero\n",
      "has issues with readability and mixing languages,\n",
      "R1\n",
      "was trained to address these issues and further improve reasoning:\n",
      "[\n",
      "48\n",
      "]\n",
      "SFT\n",
      "DeepSeek-V3-Base\n",
      "on \"thousands\" of \"cold-start\" data all with the standard format of\n",
      "|special_token|<reasoning_process>|special_token|summary>\n",
      ".\n",
      "Apply the same RL process as\n",
      "R1-Zero\n",
      ", but also with a \"language consistency reward\" to encourage it to respond monolingually. This produced an internal model not released.\n",
      "Synthesize 600K reasoning data from the internal model, with rejection sampling (i.e. if the generated reasoning had a wrong final answer, then it is removed). Synthesize 200K non-reasoning data (writing, factual QA, self-cognition, translation) using\n",
      "DeepSeek-V3\n",
      ".\n",
      "SFT\n",
      "DeepSeek-V3-Base\n",
      "on the 800K synthetic data for 2 epochs.\n",
      "GRPO RL with rule-based reward (for reasoning tasks) and model-based reward (for non-reasoning tasks, helpfulness, and harmlessness). This produced\n",
      "DeepSeek-R1\n",
      ".\n",
      "Distilled models were trained by SFT on 800K data synthesized from\n",
      "DeepSeek-R1\n",
      ", in a similar way as step 3 above. They were not trained with RL.\n",
      "[\n",
      "48\n",
      "]\n",
      "Assessment and reactions\n",
      "[\n",
      "edit\n",
      "]\n",
      "DeepSeek released its\n",
      "AI Assistant\n",
      ", which uses the V3 model as a\n",
      "chatbot app\n",
      "for\n",
      "Apple IOS\n",
      "and\n",
      "Android\n",
      ". By January 27, 2025, the app had surpassed ChatGPT as the highest-rated free app on the iOS App Store in the United States. Its chatbot reportedly answers questions, solves logic problems, and writes computer programs on par with other chatbots on the market, according to benchmark tests used by American AI companies.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek-V3 uses significantly fewer resources compared to its peers; for example, whereas the world's leading AI companies train their chatbots with\n",
      "supercomputers\n",
      "using as many as 16,000\n",
      "graphics processing units (GPUs)\n",
      ", if not more, DeepSeek claims to have needed only about 2,000 GPUs, namely the H800 series chip from\n",
      "Nvidia\n",
      ".\n",
      "[\n",
      "38\n",
      "]\n",
      "It was trained in around 55 days at a cost of US$5.58 million,\n",
      "[\n",
      "38\n",
      "]\n",
      "which is roughly one tenth of what United States tech giant\n",
      "Meta\n",
      "spent building its latest AI technology.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek's competitive performance at relatively minimal cost has been recognized as potentially challenging the global dominance of American AI models.\n",
      "[\n",
      "49\n",
      "]\n",
      "Various publications and news media, such as\n",
      "The Hill\n",
      "and\n",
      "The Guardian\n",
      ",\n",
      "described the release of its chatbot as a \"\n",
      "Sputnik moment\n",
      "\" for American AI.\n",
      "[\n",
      "50\n",
      "]\n",
      "[\n",
      "51\n",
      "]\n",
      "The performance of its\n",
      "R1\n",
      "model was reportedly \"on par with\" one of OpenAI's latest models when used for tasks such as mathematics, coding, and natural language reasoning;\n",
      "[\n",
      "52\n",
      "]\n",
      "echoing other commentators, American Silicon Valley venture capitalist\n",
      "Marc Andreessen\n",
      "likewise described\n",
      "R1\n",
      "as \"AI's Sputnik moment\".\n",
      "[\n",
      "52\n",
      "]\n",
      "DeepSeek's founder, Liang Wenfeng has been compared to Open AI CEO\n",
      "Sam Altman\n",
      ", with\n",
      "CNN\n",
      "calling him the Sam Altman of China and an evangelist for AI.\n",
      "[\n",
      "53\n",
      "]\n",
      "Chinese\n",
      "state media\n",
      "widely praised DeepSeek as a national asset.\n",
      "[\n",
      "54\n",
      "]\n",
      "[\n",
      "55\n",
      "]\n",
      "On 20 January 2025, China's Premier\n",
      "Li Qiang\n",
      "invited Liang Wenfeng to his symposium with experts and asked him to provide opinions and suggestions on a draft for comments of the annual 2024 government work report.\n",
      "[\n",
      "56\n",
      "]\n",
      "DeepSeek's optimization of limited resources has highlighted potential limits of United States sanctions on China's AI development, which include export restrictions on advanced AI chips to China.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "57\n",
      "]\n",
      "The success of the company's AI models consequently \"sparked market turmoil\"\n",
      "[\n",
      "58\n",
      "]\n",
      "and caused shares in major global technology companies to plunge on 27 January 2025: Nvidia's stock fell by as much as 17–18%,\n",
      "[\n",
      "59\n",
      "]\n",
      "as did the stock of rival\n",
      "Broadcom\n",
      ". Other tech firms also sank, including\n",
      "Microsoft\n",
      "(down 2.5%),\n",
      "Google\n",
      "'s owner\n",
      "Alphabet\n",
      "(down over 4%), and Dutch chip equipment maker\n",
      "ASML\n",
      "(down over 7%).\n",
      "[\n",
      "52\n",
      "]\n",
      "A global selloff of technology stocks on\n",
      "Nasdaq\n",
      ", prompted by the release of the\n",
      "R1\n",
      "model, had led to record losses of about $593 billion in the market capitalizations of AI and computer hardware companies;\n",
      "[\n",
      "60\n",
      "]\n",
      "by 28 January 2025, a total of $1 trillion of value was wiped off American stocks.\n",
      "[\n",
      "51\n",
      "]\n",
      "The login error DeepSeek gave on 28 Jan 2025 following a cyberattack\n",
      "Leading figures in the American AI sector had mixed reactions to DeepSeek's success and performance.\n",
      "[\n",
      "61\n",
      "]\n",
      "Microsoft CEO\n",
      "Satya Nadella\n",
      "and OpenAI CEO Sam Altman—whose companies are involved in the United States government-backed \"\n",
      "Stargate Project\n",
      "\" to develop American AI infrastructure—both called DeepSeek \"super impressive\".\n",
      "[\n",
      "62\n",
      "]\n",
      "[\n",
      "63\n",
      "]\n",
      "American President Donald Trump, who announced The Stargate Project, called DeepSeek a wake-up call\n",
      "[\n",
      "64\n",
      "]\n",
      "and a positive development.\n",
      "[\n",
      "65\n",
      "]\n",
      "[\n",
      "51\n",
      "]\n",
      "[\n",
      "52\n",
      "]\n",
      "[\n",
      "66\n",
      "]\n",
      "Other leaders in the field, including\n",
      "Scale AI\n",
      "CEO Alexandr Wang,\n",
      "Anthropic\n",
      "cofounder and CEO\n",
      "Dario Amodei\n",
      ", and\n",
      "Elon Musk\n",
      "expressed skepticism of the app's performance or of the sustainability of its success.\n",
      "[\n",
      "61\n",
      "]\n",
      "[\n",
      "67\n",
      "]\n",
      "[\n",
      "68\n",
      "]\n",
      "Various companies, including\n",
      "Amazon Web Services\n",
      ",\n",
      "Toyota\n",
      ", and\n",
      "Stripe\n",
      ", are seeking to use the model in their program.\n",
      "[\n",
      "69\n",
      "]\n",
      "On 27 January 2025, DeepSeek limited its new user registration to phone numbers from mainland China, email addresses, or Google account logins, following a \"large-scale\"\n",
      "cyberattack\n",
      "disrupted the proper functioning of its servers.\n",
      "[\n",
      "70\n",
      "]\n",
      "[\n",
      "71\n",
      "]\n",
      "Concerns\n",
      "[\n",
      "edit\n",
      "]\n",
      "Censorship\n",
      "[\n",
      "edit\n",
      "]\n",
      "See also:\n",
      "Chinese censorship abroad\n",
      "and\n",
      "Censorship in China\n",
      "DeepSeek responses when asked about\n",
      "Xi Jinping\n",
      "and\n",
      "Narendra Modi\n",
      "Some sources have observed that the official application programming interface (API) version of R1, which runs from servers located in China, uses\n",
      "censorship\n",
      "mechanisms for topics that are considered politically sensitive for the\n",
      "government of China\n",
      ". For example, the model refuses to answer questions about the\n",
      "1989 Tiananmen Square massacre\n",
      ",\n",
      "persecution of Uyghurs\n",
      ",\n",
      "comparisons between Xi Jinping and Winnie the Pooh\n",
      ", and\n",
      "human rights in China\n",
      ".\n",
      "[\n",
      "72\n",
      "]\n",
      "[\n",
      "73\n",
      "]\n",
      "[\n",
      "74\n",
      "]\n",
      "The AI may initially generate an answer, but then deletes it shortly afterwards and replaces it with a message such as: \"Sorry, that's beyond my current scope. Let's talk about something else.\"\n",
      "[\n",
      "73\n",
      "]\n",
      "The integrated censorship mechanisms and restrictions can only be removed to a limited extent in the open-source version of the R1 model. If the \"\n",
      "core socialist values\n",
      "\" defined by the\n",
      "Chinese Internet regulatory authorities\n",
      "are touched upon, or the\n",
      "political status of Taiwan\n",
      "is raised, discussions are terminated.\n",
      "[\n",
      "75\n",
      "]\n",
      "When tested by\n",
      "NBC News\n",
      ", DeepSeek's R1 described Taiwan as \"an inalienable part of China's territory,\" and stated: \"We firmly oppose any form of '\n",
      "Taiwan independence\n",
      "' separatist activities and are committed to achieving the complete reunification of the motherland through peaceful means.\"\n",
      "[\n",
      "76\n",
      "]\n",
      "In January 2025, Western researchers were able to trick DeepSeek into giving certain answers to some of these topics by requesting in its answer to swap certain letters for\n",
      "similar-looking numbers\n",
      ".\n",
      "[\n",
      "74\n",
      "]\n",
      "Security and privacy\n",
      "[\n",
      "edit\n",
      "]\n",
      "See also:\n",
      "Chinese information operations and information warfare\n",
      "Some experts fear that the government of China could use the AI system for foreign\n",
      "influence operations\n",
      ", spreading\n",
      "disinformation\n",
      ",\n",
      "surveillance\n",
      "and the development of\n",
      "cyberweapons\n",
      ".\n",
      "[\n",
      "77\n",
      "]\n",
      "[\n",
      "78\n",
      "]\n",
      "[\n",
      "79\n",
      "]\n",
      "DeepSeek's privacy terms and conditions say \"We store the information we collect in secure servers located in the People's Republic of China... We may collect your text or audio input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services\". According to a review by\n",
      "Wired\n",
      ", DeepSeek also sends data to\n",
      "Baidu\n",
      "'s web analytics service and collects data from\n",
      "ByteDance\n",
      ".\n",
      "[\n",
      "80\n",
      "]\n",
      "In response, the Italian data protection authority is seeking additional information on DeepSeek's collection and use of personal data, and the\n",
      "United States National Security Council\n",
      "announced that it had started a national security review.\n",
      "[\n",
      "81\n",
      "]\n",
      "[\n",
      "82\n",
      "]\n",
      "Taiwan's government banned the use of DeepSeek at government ministries on security grounds and South Korea's\n",
      "Personal Information Protection Commission\n",
      "opened an inquiry into DeepSeek's use of personal information.\n",
      "[\n",
      "83\n",
      "]\n",
      "See also\n",
      "[\n",
      "edit\n",
      "]\n",
      "Artificial intelligence industry in China\n",
      "Notes\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "The number of heads does not equal the number of KV heads, due to GQA.\n",
      "^\n",
      "Inexplicably, the model named\n",
      "DeepSeek-Coder-V2 Chat\n",
      "in the paper was released as\n",
      "DeepSeek-Coder-V2-Instruct\n",
      "in HuggingFace.\n",
      "^\n",
      "At that time, the\n",
      "R1-Lite-Preview\n",
      "required selecting \"Deep Think enabled\", and every user could use it only 50 times a day.\n",
      "References\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "Gibney, Elizabeth (23 January 2025).\n",
      "\"China's cheap, open AI model DeepSeek thrills scientists\"\n",
      ".\n",
      "Nature\n",
      ".\n",
      "doi\n",
      ":\n",
      "10.1038/d41586-025-00229-6\n",
      ".\n",
      "ISSN\n",
      "1476-4687\n",
      ".\n",
      "PMID\n",
      "39849139\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Vincent, James (28 January 2025).\n",
      "\"The DeepSeek panic reveals an AI world ready to blow\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "Metz, Cade; Tobin, Meaghan (23 January 2025).\n",
      "\"How Chinese A.I. Start-Up DeepSeek Is Competing With Silicon Valley Giants\"\n",
      ".\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Cosgrove, Emma (27 January 2025).\n",
      "\"DeepSeek's cheaper models and weaker chips call into question trillions in AI infrastructure spending\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Mallick, Subhrojit (16 January 2024).\n",
      "\"Biden admin's cap on GPU exports may hit India's AI ambitions\"\n",
      ".\n",
      "The Economic Times\n",
      ". Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Saran, Cliff (10 December 2024).\n",
      "\"Nvidia investigation signals widening of US and China chip war | Computer Weekly\"\n",
      ".\n",
      "Computer Weekly\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Sherman, Natalie (9 December 2024).\n",
      "\"Nvidia targeted by China in new chip war probe\"\n",
      ".\n",
      "BBC\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Metz, Cade (27 January 2025).\n",
      "\"What is DeepSeek? And How Is It Upending A.I.?\"\n",
      ".\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Field, Hayden (27 January 2025).\n",
      "\"China's DeepSeek AI dethrones ChatGPT on App Store: Here's what you should know\"\n",
      ".\n",
      "CNBC\n",
      ".\n",
      "^\n",
      "Picchi, Aimee (27 January 2025).\n",
      "\"What is DeepSeek, and why is it causing Nvidia and other stocks to slump?\"\n",
      ".\n",
      "CBS News\n",
      ".\n",
      "^\n",
      "Zahn, Max (27 January 2025).\n",
      "\"Nvidia, Microsoft shares tumble as China-based AI app DeepSeek hammers tech giants\"\n",
      ".\n",
      "ABC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Roose, Kevin (28 January 2025).\n",
      "\"Why DeepSeek Could Change What Silicon Valley Believe About A.I.\"\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Romero, Luis E. (28 January 2025).\n",
      "\"ChatGPT, DeepSeek, Or Llama? Meta's LeCun Says Open-Source Is The Key\"\n",
      ".\n",
      "Forbes\n",
      ".\n",
      "^\n",
      "Chen, Caiwei (24 January 2025).\n",
      "\"How a top Chinese AI model overcame US sanctions\"\n",
      ".\n",
      "MIT Technology Review\n",
      ".\n",
      "Archived\n",
      "from the original on 25 January 2025\n",
      ". Retrieved\n",
      "25 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "Ottinger, Lily (9 December 2024).\n",
      "\"Deepseek: From Hedge Fund to Frontier Model Maker\"\n",
      ".\n",
      "ChinaTalk\n",
      ".\n",
      "Archived\n",
      "from the original on 28 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Olcott, Eleanor; Wu, Zijing (24 January 2025).\n",
      "\"How small Chinese AI start-up DeepSeek shocked Silicon Valley\"\n",
      ".\n",
      "Financial Times\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Leswing, Kif (23 February 2023).\n",
      "\"Meet the $10,000 Nvidia chip powering the race for A.I.\"\n",
      "CNBC\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Yu, Xu (17 April 2023).\n",
      "\"[Exclusive] Chinese Quant Hedge Fund High-Flyer Won't Use AGI to Trade Stocks, MD Says\"\n",
      ".\n",
      "Yicai Global\n",
      ".\n",
      "Archived\n",
      "from the original on 31 December 2023\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "Jiang, Ben; Perezi, Bien (1 January 2025).\n",
      "\"Meet DeepSeek: the Chinese start-up that is changing how AI models are trained\"\n",
      ".\n",
      "South China Morning Post\n",
      ".\n",
      "Archived\n",
      "from the original on 22 January 2025\n",
      ". Retrieved\n",
      "1 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "McMorrow, Ryan; Olcott, Eleanor (9 June 2024).\n",
      "\"The Chinese quant fund-turned-AI pioneer\"\n",
      ".\n",
      "Financial Times\n",
      ".\n",
      "Archived\n",
      "from the original on 17 July 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Schneider, Jordan (27 November 2024).\n",
      "\"Deepseek: The Quiet Giant Leading China's AI Race\"\n",
      ".\n",
      "ChinaTalk\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"DeepSeek-Coder/LICENSE-MODEL at main · deepseek-ai/DeepSeek-Coder\"\n",
      ".\n",
      "GitHub\n",
      ".\n",
      "Archived\n",
      "from the original on 22 January 2025\n",
      ". Retrieved\n",
      "24 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Guo, Daya; Zhu, Qihao; Yang, Dejian; Xie, Zhenda; Dong, Kai; Zhang, Wentao; Chen, Guanting; Bi, Xiao; Wu, Y. (26 January 2024),\n",
      "DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.14196\n",
      "^\n",
      "\"DeepSeek Coder\"\n",
      ".\n",
      "deepseekcoder.github.io\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "deepseek-ai/DeepSeek-Coder\n",
      ", DeepSeek, 27 January 2025\n",
      ", retrieved\n",
      "27 January\n",
      "2025\n",
      "^\n",
      "\"deepseek-ai/deepseek-coder-5.7bmqa-base · Hugging Face\"\n",
      ".\n",
      "Hugging Face\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (5 January 2024),\n",
      "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.02954\n",
      "^\n",
      "deepseek-ai/DeepSeek-LLM\n",
      ", DeepSeek, 27 January 2025\n",
      ", retrieved\n",
      "27 January\n",
      "2025\n",
      "^\n",
      "a\n",
      "b\n",
      "Dai, Damai; Deng, Chengqi; Zhao, Chenggang; Xu, R. X.; Gao, Huazuo; Chen, Deli; Li, Jiashi; Zeng, Wangding; Yu, Xingkai (11 January 2024),\n",
      "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.06066\n",
      "^\n",
      "Shao, Zhihong; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Song, Junxiao; Bi, Xiao; Zhang, Haowei; Zhang, Mingchuan; Li, Y. K. (27 April 2024),\n",
      "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2402.03300\n",
      ".\n",
      "^\n",
      "Wang, Peiyi; Li, Lei; Shao, Zhihong; Xu, R. X.; Dai, Damai; Li, Yifei; Chen, Deli; Wu, Y.; Sui, Zhifang (19 February 2024),\n",
      "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2312.08935\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (19 June 2024),\n",
      "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2405.04434\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Peng, Bowen; Quesnelle, Jeffrey; Fan, Honglu; Shippole, Enrico (1 November 2023),\n",
      "YaRN: Efficient Context Window Extension of Large Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2309.00071\n",
      ".\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V2-Lite at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 15 May 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V2 at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 6 May 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "DeepSeek-AI; Zhu, Qihao; Guo, Daya; Shao, Zhihong; Yang, Dejian; Wang, Peiyi; Xu, Runxin; Wu, Y.; Li, Yukun (17 June 2024),\n",
      "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2406.11931\n",
      "^\n",
      "\"deepseek-ai/DeepSeek-V2.5 · Hugging Face\"\n",
      ".\n",
      "Hugging Face\n",
      ". 3 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "DeepSeek-AI; Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi (27 December 2024),\n",
      "DeepSeek-V3 Technical Report\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2412.19437\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V3 at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 26 December 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Jiang, Ben (27 December 2024).\n",
      "\"Chinese start-up DeepSeek's new AI model outperforms Meta, OpenAI products\"\n",
      ".\n",
      "South China Morning Post\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Sharma, Shubham (26 December 2024).\n",
      "\"DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch\"\n",
      ".\n",
      "VentureBeat\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Wiggers, Kyle (26 December 2024).\n",
      "\"DeepSeek's new AI model appears to be one of the best 'open' challengers yet\"\n",
      ".\n",
      "TechCrunch\n",
      ".\n",
      "Archived\n",
      "from the original on 2 January 2025\n",
      ". Retrieved\n",
      "31 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"Deepseek Log in page\"\n",
      ".\n",
      "DeepSeek\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"News | DeepSeek-R1-Lite Release 2024/11/20: 🚀 DeepSeek-R1-Lite-Preview is now live: unleashing supercharged reasoning power!\"\n",
      ".\n",
      "DeepSeek API Docs\n",
      ". Archived from\n",
      "the original\n",
      "on 20 November 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Franzen, Carl (20 November 2024).\n",
      "\"DeepSeek's first reasoning model R1-Lite-Preview turns heads, beating OpenAI o1 performance\"\n",
      ".\n",
      "VentureBeat\n",
      ".\n",
      "Archived\n",
      "from the original on 22 November 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Huang, Raffaele (24 December 2024).\n",
      "\"Don't Look Now, but China's AI Is Catching Up Fast\"\n",
      ".\n",
      "The Wall Street Journal\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"Release DeepSeek-R1 · deepseek-ai/DeepSeek-R1@23807ce\"\n",
      ".\n",
      "GitHub\n",
      ".\n",
      "Archived\n",
      "from the original on 21 January 2025\n",
      ". Retrieved\n",
      "21 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (22 January 2025),\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2501.12948\n",
      "^\n",
      "\"Chinese AI startup DeepSeek overtakes ChatGPT on Apple App Store\"\n",
      ".\n",
      "Reuters\n",
      ". 27 January 2025\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Wade, David (6 December 2024).\n",
      "\"American AI has reached its Sputnik moment\"\n",
      ".\n",
      "The Hill\n",
      ".\n",
      "Archived\n",
      "from the original on 8 December 2024\n",
      ". Retrieved\n",
      "25 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Milmo, Dan; Hawkins, Amy; Booth, Robert; Kollewe, Julia (28 January 2025).\n",
      "\"\n",
      "'Sputnik moment': $1tn wiped off US stocks after Chinese firm unveils AI chatbot\"\n",
      "– via The Guardian.\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "Hoskins, Peter; Rahman-Jones, Imran (27 January 2025).\n",
      "\"Nvidia shares sink as Chinese AI app spooks markets\"\n",
      ".\n",
      "BBC\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Goldman, David (27 January 2025).\n",
      "\"What is DeepSeek, the Chinese AI startup that shook the tech world? | CNN Business\"\n",
      ".\n",
      "CNN\n",
      ". Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"DeepSeek poses a challenge to Beijing as much as to Silicon Valley\"\n",
      ".\n",
      "The Economist\n",
      ". 29 January 2025.\n",
      "ISSN\n",
      "0013-0613\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Paul, Katie; Nellis, Stephen (30 January 2025).\n",
      "\"Chinese state-linked accounts hyped DeepSeek AI launch ahead of US stock rout, Graphika says\"\n",
      ".\n",
      "Reuters\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "澎湃新闻 (22 January 2025).\n",
      "\"量化巨头幻方创始人梁文锋参加总理座谈会并发言，他还创办了\"AI界拼多多\"\n",
      "\"\n",
      ".\n",
      "finance.sina.com.cn\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Shilov, Anton (27 December 2024).\n",
      "\"Chinese AI company's AI model breakthrough highlights limits of US sanctions\"\n",
      ".\n",
      "Tom's Hardware\n",
      ".\n",
      "Archived\n",
      "from the original on 28 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"DeepSeek updates – Chinese AI chatbot sparks US market turmoil, wiping $500bn off Nvidia\"\n",
      ".\n",
      "BBC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Nazareth, Rita (26 January 2025).\n",
      "\"Stock Rout Gets Ugly as Nvidia Extends Loss to 17%: Markets Wrap\"\n",
      ".\n",
      "Bloomberg\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Carew, Sinéad; Cooper, Amanda; Banerjee, Ankur (27 January 2025).\n",
      "\"DeepSeek sparks global AI selloff, Nvidia losses about $593 billion of value\"\n",
      ". Reuters.\n",
      "^\n",
      "a\n",
      "b\n",
      "Sherry, Ben (28 January 2025).\n",
      "\"DeepSeek, Calling It 'Impressive' but Staying Skeptical\"\n",
      ".\n",
      "Inc.\n",
      "Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Okemwa, Kevin (28 January 2025).\n",
      "\"Microsoft CEO Satya Nadella touts DeepSeek's open-source AI as \"super impressive\": \"We should take the developments out of China very, very seriously\"\n",
      "\"\n",
      ".\n",
      "Windows Central\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Nazzaro, Miranda (28 January 2025).\n",
      "\"OpenAI's Sam Altman calls DeepSeek model 'impressive'\n",
      "\"\n",
      ".\n",
      "The Hill\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Dou, Eva; Gregg, Aaron; Zakrzewski, Cat; Tiku, Nitasha; Najmabadi, Shannon (28 January 2025).\n",
      "\"Trump calls China's DeepSeek AI app a 'wake-up call' after tech stocks slide\"\n",
      ".\n",
      "The Washington Post\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Habeshian, Sareen (28 January 2025).\n",
      "\"Johnson bashes China on AI, Trump calls DeepSeek development \"positive\"\n",
      "\"\n",
      ".\n",
      "Axios\n",
      ".\n",
      "^\n",
      "Karaian, Jason; Rennison, Joe (27 January 2025).\n",
      "\"China's A.I. Advances Spook Big Tech Investors on Wall Street\"\n",
      "– via NYTimes.com.\n",
      "^\n",
      "Sharma, Manoj (6 January 2025).\n",
      "\"Musk dismisses, Altman applauds: What leaders say on DeepSeek's disruption\"\n",
      ".\n",
      "Fortune India\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Elon Musk 'questions' DeepSeek's claims, suggests massive Nvidia GPU infrastructure\"\n",
      ".\n",
      "Financialexpress\n",
      ". 28 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Kim, Eugene.\n",
      "\"Big AWS customers, including Stripe and Toyota, are hounding the cloud giant for access to DeepSeek AI models\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Kerr, Dara (27 January 2025).\n",
      "\"DeepSeek hit with 'large-scale' cyber-attack after AI chatbot tops app stores\"\n",
      ".\n",
      "The Guardian\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Tweedie, Steven; Altchek, Ana.\n",
      "\"DeepSeek temporarily limited new sign-ups, citing 'large-scale malicious attacks'\n",
      "\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Field, Matthew; Titcomb, James (27 January 2025).\n",
      "\"Chinese AI has sparked a $1 trillion panic – and it doesn't care about free speech\"\n",
      ".\n",
      "The Daily Telegraph\n",
      ".\n",
      "ISSN\n",
      "0307-1235\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Steinschaden, Jakob (27 January 2025).\n",
      "\"DeepSeek: This is what live censorship looks like in the Chinese AI chatbot\"\n",
      ".\n",
      "Trending Topics\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Lu, Donna (28 January 2025).\n",
      "\"We tried out DeepSeek. It worked well, until we asked it about Tiananmen Square and Taiwan\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"The Guardian view on a global AI race: geopolitics, innovation and the rise of chaos\"\n",
      ".\n",
      "The Guardian\n",
      ". 26 January 2025.\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Yang, Angela; Cui, Jasmine (27 January 2025).\n",
      "\"Chinese AI DeepSeek jolts Silicon Valley, giving the AI race its 'Sputnik moment'\n",
      "\"\n",
      ".\n",
      "NBC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Kimery, Anthony (26 January 2025).\n",
      "\"China's DeepSeek AI poses formidable cyber, data privacy threats\"\n",
      ".\n",
      "Biometric Update\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Booth, Robert; Milmo, Dan (28 January 2025).\n",
      "\"Experts urge caution over use of Chinese AI DeepSeek\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Hornby, Rael (28 January 2025).\n",
      "\"DeepSeek's success has painted a huge TikTok-shaped target on its back\"\n",
      ".\n",
      "LaptopMag\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Burgess, Matt; Newman, Lily Hay (27 January 2025).\n",
      "\"DeepSeek's Popular AI App Is Explicitly Sending US Data to China\"\n",
      ".\n",
      "Wired\n",
      ".\n",
      "ISSN\n",
      "1059-1028\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Italy regulator seeks information from DeepSeek on data protection\"\n",
      ".\n",
      "Reuters\n",
      ". 28 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Shalal, Andrea; Shepardson, David (28 January 2025).\n",
      "\"White House evaluates effect of China AI app DeepSeek on national security, official says\"\n",
      ".\n",
      "Reuters\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Taiwan says government departments should not use DeepSeek, citing security concerns\"\n",
      ".\n",
      "Reuters\n",
      ". 31 January 2025\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "External links\n",
      "[\n",
      "edit\n",
      "]\n",
      "Free and open-source software portal\n",
      "Wikimedia Commons has media related to\n",
      "DeepSeek\n",
      ".\n",
      "Official website\n",
      "DeepSeek\n",
      "on\n",
      "GitHub\n",
      "DeepSeek\n",
      "on\n",
      "Hugging Face\n",
      "Official API documentation\n",
      "Anthology of DeepSeek papers\n",
      "v\n",
      "t\n",
      "e\n",
      "Generative AI\n",
      "chatbots\n",
      "United States\n",
      "ChatGPT\n",
      "Claude\n",
      "Copilot\n",
      "Gemini\n",
      "Grok\n",
      "Poe\n",
      "Replika\n",
      "You.com\n",
      "Russia\n",
      "YandexGPT\n",
      "China\n",
      "DeepSeek\n",
      "Qwen\n",
      "Europe\n",
      "Mistral\n",
      "(France)\n",
      "Korea\n",
      "Galaxy AI\n",
      "Defunct\n",
      "Bard\n",
      "Related\n",
      "Large language models\n",
      "Category\n",
      "v\n",
      "t\n",
      "e\n",
      "Generative AI\n",
      "Concepts\n",
      "Autoencoder\n",
      "Deep learning\n",
      "Generative adversarial network\n",
      "Generative pre-trained transformer\n",
      "Large language model\n",
      "Neural network\n",
      "Prompt engineering\n",
      "Retrieval-augmented generation\n",
      "Reinforcement learning from human feedback\n",
      "Self-supervised learning\n",
      "Transformer\n",
      "Variational autoencoder\n",
      "Vision transformer\n",
      "Word embedding\n",
      "Models\n",
      "Text\n",
      "Claude\n",
      "DBRX\n",
      "DeepSeek\n",
      "Gemini\n",
      "GPT\n",
      "1\n",
      "2\n",
      "3\n",
      "J\n",
      "ChatGPT\n",
      "4\n",
      "4o\n",
      "o1\n",
      "o3\n",
      "Grok\n",
      "Granite\n",
      "Llama\n",
      "Mistral Large\n",
      "PanGu-Σ\n",
      "Qwen\n",
      "Image\n",
      "Aurora\n",
      "DALL-E\n",
      "Firefly\n",
      "Flux\n",
      "Ideogram\n",
      "Midjourney\n",
      "Stable Diffusion\n",
      "Video\n",
      "Dream Machine\n",
      "Gen-3 Alpha\n",
      "Hailuo AI\n",
      "Kling\n",
      "Sora\n",
      "Veo\n",
      "VideoPoet\n",
      "Music\n",
      "Udio\n",
      "Suno AI\n",
      "Companies\n",
      "List of artificial intelligence companies\n",
      "Category\n",
      "Commons\n",
      "Retrieved from \"\n",
      "https://en.wikipedia.org/w/index.php?title=DeepSeek&oldid=1273198562\n",
      "\"\n",
      "Categories\n",
      ":\n",
      "2023 establishments in China\n",
      "Artificial intelligence companies\n",
      "Artificial intelligence laboratories\n",
      "Companies based in Hangzhou\n",
      "Technology companies established in 2023\n",
      "Hidden categories:\n",
      "Articles with short description\n",
      "Short description matches Wikidata\n",
      "Use dmy dates from January 2025\n",
      "Articles containing Chinese-language text\n",
      "Articles containing simplified Chinese-language text\n",
      "Wikipedia articles that are too technical from January 2025\n",
      "All articles that are too technical\n",
      "Commons category link from Wikidata\n",
      "This page was last edited on 1 February 2025, at 06:51\n",
      "(UTC)\n",
      ".\n",
      "Text is available under the\n",
      "Creative Commons Attribution-ShareAlike 4.0 License\n",
      ";\n",
      "additional terms may apply. By using this site, you agree to the\n",
      "Terms of Use\n",
      "and\n",
      "Privacy Policy\n",
      ". Wikipedia® is a registered trademark of the\n",
      "Wikimedia Foundation, Inc.\n",
      ", a non-profit organization.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Code of Conduct\n",
      "Developers\n",
      "Statistics\n",
      "Cookie statement\n",
      "Mobile view\n",
      "Search\n",
      "Search\n",
      "Toggle the table of contents\n",
      "DeepSeek\n",
      "55 languages\n",
      "Add topic\n"
     ]
    }
   ],
   "source": [
    "# Lets try to parse one website\n",
    "\n",
    "web = Website(\"https://en.wikipedia.org/wiki/DeepSeek\")\n",
    "print(web.title)\n",
    "print(web.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Prompts:\n",
    "1. System prompt : this tells the model what task to be performed and what tone should be used\n",
    "2. User prompt : the conversation starter that the model should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our System prompt:\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the content of a website \\\n",
    "    and provides a short summary, ignoring text that might be navigation related \\\n",
    "    or any other irrelevant advirtisements. Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our User Prompt considering input as website title\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    if not website.text:  # Handle empty content\n",
    "        return \"The webpage could not be parsed. No content available.\"\n",
    "    \n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\n The contents of this website is as follows: \\\n",
    "        please provide a short summary of this website in markdown. \\\n",
    "        If it includes news or announcements, then summarize these too. \\n\\n\"\n",
    "    \n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an assistant that analyzes the content of a website     and provides a short summary, ignoring text that might be navigation related     or any other irrelevant advirtisements. Respond in markdown.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are looking at a website titled DeepSeek - Wikipedia\n",
      " The contents of this website is as follows:         please provide a short summary of this website in markdown.         If it includes news or announcements, then summarize these too. \n",
      "\n",
      "Jump to content\n",
      "Main menu\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "Navigation\n",
      "Main page\n",
      "Contents\n",
      "Current events\n",
      "Random article\n",
      "About Wikipedia\n",
      "Contact us\n",
      "Contribute\n",
      "Help\n",
      "Learn to edit\n",
      "Community portal\n",
      "Recent changes\n",
      "Upload file\n",
      "Search\n",
      "Search\n",
      "Appearance\n",
      "Donate\n",
      "Create account\n",
      "Log in\n",
      "Personal tools\n",
      "Donate\n",
      "Create account\n",
      "Log in\n",
      "Pages for logged out editors\n",
      "learn more\n",
      "Contributions\n",
      "Talk\n",
      "Contents\n",
      "move to sidebar\n",
      "hide\n",
      "(Top)\n",
      "1\n",
      "Background\n",
      "2\n",
      "Development and release history\n",
      "Toggle Development and release history subsection\n",
      "2.1\n",
      "DeepSeek LLM\n",
      "2.2\n",
      "V2\n",
      "2.3\n",
      "V3\n",
      "2.4\n",
      "R1\n",
      "3\n",
      "Assessment and reactions\n",
      "4\n",
      "Concerns\n",
      "Toggle Concerns subsection\n",
      "4.1\n",
      "Censorship\n",
      "4.2\n",
      "Security and privacy\n",
      "5\n",
      "See also\n",
      "6\n",
      "Notes\n",
      "7\n",
      "References\n",
      "8\n",
      "External links\n",
      "Toggle the table of contents\n",
      "DeepSeek\n",
      "55 languages\n",
      "Afrikaans\n",
      "العربية\n",
      "Aragonés\n",
      "অসমীয়া\n",
      "Azərbaycanca\n",
      "বাংলা\n",
      "Български\n",
      "Català\n",
      "Čeština\n",
      "Dansk\n",
      "الدارجة\n",
      "Deutsch\n",
      "Ελληνικά\n",
      "Español\n",
      "Esperanto\n",
      "Euskara\n",
      "فارسی\n",
      "Français\n",
      "Frysk\n",
      "Fulfulde\n",
      "Gaeilge\n",
      "Galego\n",
      "한국어\n",
      "Bahasa Indonesia\n",
      "Italiano\n",
      "עברית\n",
      "Kiswahili\n",
      "Magyar\n",
      "Македонски\n",
      "മലയാളം\n",
      "Nederlands\n",
      "नेपाली\n",
      "日本語\n",
      "Oʻzbekcha / ўзбекча\n",
      "Polski\n",
      "Português\n",
      "Qaraqalpaqsha\n",
      "Română\n",
      "Русский\n",
      "Simple English\n",
      "Српски / srpski\n",
      "Suomi\n",
      "Svenska\n",
      "Tagalog\n",
      "தமிழ்\n",
      "ၽႃႇသႃႇတႆး\n",
      "ไทย\n",
      "Türkçe\n",
      "Українська\n",
      "اردو\n",
      "ئۇيغۇرچە / Uyghurche\n",
      "Tiếng Việt\n",
      "吴语\n",
      "粵語\n",
      "中文\n",
      "Edit links\n",
      "Article\n",
      "Talk\n",
      "English\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "Tools\n",
      "Tools\n",
      "move to sidebar\n",
      "hide\n",
      "Actions\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "General\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Get shortened URL\n",
      "Download QR code\n",
      "Print/export\n",
      "Download as PDF\n",
      "Printable version\n",
      "In other projects\n",
      "Wikimedia Commons\n",
      "Wikidata item\n",
      "Appearance\n",
      "move to sidebar\n",
      "hide\n",
      "From Wikipedia, the free encyclopedia\n",
      "Chinese artificial intelligence company\n",
      "Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\n",
      "Native name\n",
      "杭州深度求索人工智能基础技术研究有限公司\n",
      "Company type\n",
      "Private\n",
      "Industry\n",
      "Information technology\n",
      "Artificial intelligence\n",
      "Founded\n",
      "May 16, 2023\n",
      "; 20 months ago\n",
      "(\n",
      "2023-05-16\n",
      ")\n",
      "Founder\n",
      "Liang Wenfeng\n",
      "Headquarters\n",
      "Hangzhou\n",
      ",\n",
      "Zhejiang\n",
      ", China\n",
      "Key people\n",
      "Liang Wenfeng (CEO)\n",
      "Owner\n",
      "High-Flyer\n",
      "Number of employees\n",
      "Under 200\n",
      "Website\n",
      "www\n",
      ".deepseek\n",
      ".com\n",
      "Hangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\n",
      ", commonly referred to as\n",
      "DeepSeek\n",
      ", (\n",
      "Chinese\n",
      ":\n",
      "深度求索\n",
      ";\n",
      "pinyin\n",
      ":\n",
      "Shēndù Qiúsuǒ\n",
      ") is a Chinese\n",
      "artificial intelligence\n",
      "company that develops\n",
      "open-source\n",
      "large language models\n",
      "(LLMs). Based in\n",
      "Hangzhou, Zhejiang\n",
      ", it is owned and funded by Chinese hedge fund\n",
      "High-Flyer\n",
      ", whose co-founder,\n",
      "Liang Wenfeng\n",
      ", established the company in 2023 and serves as its\n",
      "CEO\n",
      ".\n",
      "The DeepSeek-R1 model provides responses comparable to other contemporary\n",
      "large language models\n",
      ", such as\n",
      "OpenAI\n",
      "'s\n",
      "GPT-4o\n",
      "and\n",
      "o1\n",
      ".\n",
      "[\n",
      "1\n",
      "]\n",
      "It is\n",
      "trained\n",
      "at a significantly lower cost—stated at\n",
      "US$\n",
      "6 million compared to $100 million for OpenAI's\n",
      "GPT-4\n",
      "in 2023\n",
      "[\n",
      "2\n",
      "]\n",
      "—and requires a tenth of the computing power of a comparable LLM.\n",
      "[\n",
      "2\n",
      "]\n",
      "[\n",
      "3\n",
      "]\n",
      "[\n",
      "4\n",
      "]\n",
      "DeepSeek's AI models were developed amid United States sanctions on China for\n",
      "Nvidia\n",
      "chips,\n",
      "[\n",
      "5\n",
      "]\n",
      "which were intended to restrict the ability of the country to develop advanced AI systems.\n",
      "[\n",
      "6\n",
      "]\n",
      "[\n",
      "7\n",
      "]\n",
      "On 10 January 2025, DeepSeek released its first free\n",
      "chatbot app\n",
      ", based on the DeepSeek-R1 model, for\n",
      "iOS\n",
      "and\n",
      "Android\n",
      "; by 27 January, DeepSeek-R1 had surpassed\n",
      "ChatGPT\n",
      "as the most-downloaded free app on the\n",
      "iOS App Store\n",
      "in the United States,\n",
      "[\n",
      "8\n",
      "]\n",
      "causing Nvidia's share price to drop by 18%.\n",
      "[\n",
      "9\n",
      "]\n",
      "[\n",
      "10\n",
      "]\n",
      "DeepSeek's success against larger and more established rivals has been described as \"upending AI\",\n",
      "[\n",
      "8\n",
      "]\n",
      "constituting \"the first shot at what is emerging as a global AI space race\",\n",
      "[\n",
      "11\n",
      "]\n",
      "and ushering in \"a new era of AI\n",
      "brinkmanship\n",
      "\".\n",
      "[\n",
      "12\n",
      "]\n",
      "DeepSeek makes its\n",
      "generative artificial intelligence\n",
      "algorithms, models, and training details open-source, allowing its code to be freely available for use, modification, viewing, and designing documents for building purposes.\n",
      "[\n",
      "13\n",
      "]\n",
      "The company reportedly vigorously recruits young AI researchers from top Chinese universities,\n",
      "[\n",
      "8\n",
      "]\n",
      "and hires from outside the\n",
      "computer science\n",
      "field to diversify its models' knowledge and abilities.\n",
      "[\n",
      "3\n",
      "]\n",
      "Background\n",
      "[\n",
      "edit\n",
      "]\n",
      "In February 2016, High-Flyer was co-founded by AI enthusiast Liang Wenfeng, who had been trading since the\n",
      "2007–2008 financial crisis\n",
      "while attending\n",
      "Zhejiang University\n",
      ".\n",
      "[\n",
      "14\n",
      "]\n",
      "By 2019, he established High-Flyer as a hedge fund focused on developing and using AI trading algorithms. By 2021, High-Flyer exclusively used AI in trading\n",
      "[\n",
      "15\n",
      "]\n",
      ", often using Nvidia chips.\n",
      "[\n",
      "16\n",
      "]\n",
      "DeepSeek has made its\n",
      "generative artificial intelligence\n",
      "chatbot\n",
      "open source\n",
      ", meaning its code is freely available for use, modification, and viewing. This includes permission to access and use the source code, as well as design documents, for building purposes.\n",
      "[\n",
      "13\n",
      "]\n",
      "In 2021, while running High-Flyer, Liang began stockpiling Nvidia GPUs for an AI project.\n",
      "[\n",
      "16\n",
      "]\n",
      "According to\n",
      "36Kr\n",
      ", Liang had built up a store of 10,000\n",
      "Nvidia A100\n",
      "GPUs, which are used to train AI,\n",
      "[\n",
      "17\n",
      "]\n",
      "before the United States federal government imposed AI chip restrictions on China.\n",
      "[\n",
      "15\n",
      "]\n",
      "In April 2023, High-Flyer started an\n",
      "artificial general intelligence\n",
      "lab dedicated to research developing AI tools separate from High-Flyer's financial business.\n",
      "[\n",
      "18\n",
      "]\n",
      "[\n",
      "19\n",
      "]\n",
      "In May 2023, with High-Flyer as one of the investors, the lab became its own company, DeepSeek.\n",
      "[\n",
      "15\n",
      "]\n",
      "[\n",
      "20\n",
      "]\n",
      "[\n",
      "19\n",
      "]\n",
      "Venture capital\n",
      "firms were reluctant in providing funding as it was unlikely that it would be able to generate an\n",
      "exit\n",
      "in a short period of time.\n",
      "[\n",
      "15\n",
      "]\n",
      "After releasing DeepSeek-V2 in May 2024, which offered strong performance for a low price, DeepSeek became known as the catalyst for China's AI model\n",
      "price war\n",
      ". It was quickly dubbed the \"\n",
      "Pinduoduo\n",
      "of AI\", and other major tech giants such as\n",
      "ByteDance\n",
      ",\n",
      "Tencent\n",
      ",\n",
      "Baidu\n",
      ", and\n",
      "Alibaba\n",
      "began to cut the price of their AI models to compete with the company. Despite the low price charged by DeepSeek, it was profitable compared to its rivals that were losing money.\n",
      "[\n",
      "21\n",
      "]\n",
      "DeepSeek is focused on research and has no detailed plans for commercialization;\n",
      "[\n",
      "21\n",
      "]\n",
      "this also allows its technology to avoid the most stringent provisions of China's AI regulations, such as requiring consumer-facing technology to comply with the government’s controls on information.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek's hiring preferences target technical abilities rather than work experience, resulting in most new hires being either recent university graduates or developers whose AI careers are less established.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "3\n",
      "]\n",
      "Likewise, the company recruits individuals without any computer science background to help its technology understand other topics and knowledge areas, including being able to generate poetry and perform well on the notoriously difficult\n",
      "Chinese college admissions exams (Gaokao)\n",
      ".\n",
      "[\n",
      "3\n",
      "]\n",
      "Development and release history\n",
      "[\n",
      "edit\n",
      "]\n",
      "This section\n",
      "may be too technical for most readers to understand\n",
      ".\n",
      "Please\n",
      "help improve it\n",
      "to\n",
      "make it understandable to non-experts\n",
      ", without removing the technical details.\n",
      "(\n",
      "January 2025\n",
      ")\n",
      "(\n",
      "Learn how and when to remove this message\n",
      ")\n",
      "DeepSeek LLM\n",
      "[\n",
      "edit\n",
      "]\n",
      "On 2 November 2023, DeepSeek released its first series of model,\n",
      "DeepSeek-Coder\n",
      ", which is available for free to both researchers and commercial users. The code for the model was made open-source under the\n",
      "MIT license\n",
      ", with an additional license agreement (\"DeepSeek license\") regarding \"open and responsible downstream usage\" for the model itself.\n",
      "[\n",
      "22\n",
      "]\n",
      "They are of the same architecture as DeepSeek LLM detailed below. The series includes 8 models, 4 pretrained (\n",
      "Base\n",
      ") and 4 instruction-finetuned (\n",
      "Instruct\n",
      "). They all have 16K context lengths. The\n",
      "training\n",
      "was as follows:\n",
      "[\n",
      "23\n",
      "]\n",
      "[\n",
      "24\n",
      "]\n",
      "[\n",
      "25\n",
      "]\n",
      "Pretraining: 1.8T tokens (87% source code, 10% code-related English (GitHub markdown and\n",
      "Stack Exchange\n",
      "), and 3% code-unrelated Chinese).\n",
      "Long-context pretraining: 200B tokens. This extends the context length from 4K to 16K. This produced the\n",
      "Base\n",
      "models.\n",
      "Supervised\n",
      "finetuning\n",
      "(SFT): 2B tokens of instruction data. This produced the\n",
      "Instruct\n",
      "models.\n",
      "They were trained on clusters of A100 and\n",
      "H800\n",
      "Nvidia GPUs, connected by\n",
      "InfiniBand\n",
      ",\n",
      "NVLink\n",
      ",\n",
      "NVSwitch\n",
      ".\n",
      "[\n",
      "23\n",
      "]\n",
      "DeepSeek Coder properties\n",
      "[\n",
      "23\n",
      "]\n",
      ": Table 2\n",
      "[\n",
      "26\n",
      "]\n",
      "Params\n",
      ".\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "d\n",
      "model\n",
      "{\\displaystyle d_{\\text{model}}}\n",
      "d\n",
      "intermediate\n",
      "{\\displaystyle d_{\\text{intermediate}}}\n",
      "n\n",
      "heads\n",
      "{\\displaystyle n_{\\text{heads}}}\n",
      "n\n",
      "kv-heads\n",
      "{\\displaystyle n_{\\text{kv-heads}}}\n",
      "1.3B\n",
      "24\n",
      "2048\n",
      "5504\n",
      "16\n",
      "16\n",
      "5.7B\n",
      "32\n",
      "4096\n",
      "11008\n",
      "32\n",
      "1\n",
      "[\n",
      "note 1\n",
      "]\n",
      "6.7B\n",
      "32\n",
      "4096\n",
      "11008\n",
      "32\n",
      "32\n",
      "33B\n",
      "62\n",
      "7168\n",
      "19200\n",
      "56\n",
      "7\n",
      "[\n",
      "note 1\n",
      "]\n",
      "On 29 November 2023, DeepSeek released the\n",
      "DeepSeek-LLM\n",
      "series of models, with 7B and 67B parameters in both\n",
      "Base\n",
      "and\n",
      "Chat\n",
      "forms (no\n",
      "Instruct\n",
      "was released). It was developed to compete with other LLMs available at the time. The paper claimed benchmark results higher than most open source LLMs at the time, especially Llama 2.\n",
      "[\n",
      "27\n",
      "]\n",
      ": section 5\n",
      "Like DeepSeek Coder, the code for the model was under MIT license, with DeepSeek license for the model itself.\n",
      "[\n",
      "28\n",
      "]\n",
      "The architecture was essentially the same as those of the\n",
      "Llama\n",
      "series. They used the\n",
      "pre-norm\n",
      "decoder-only Transformer\n",
      "with\n",
      "RMSNorm\n",
      "as the normalization,\n",
      "SwiGLU\n",
      "in the feedforward layers,\n",
      "rotary positional embedding\n",
      "(RoPE), and\n",
      "grouped-query attention\n",
      "(GQA). Both had vocabulary size 102,400 (\n",
      "byte-level BPE\n",
      ") and context length of 4096. They trained on 2 trillion tokens of English and Chinese text obtained by deduplicating the\n",
      "Common Crawl\n",
      ".\n",
      "[\n",
      "27\n",
      "]\n",
      "DeepSeek LLM properties\n",
      "[\n",
      "27\n",
      "]\n",
      ": Table 2\n",
      "Params\n",
      ".\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "d\n",
      "model\n",
      "{\\displaystyle d_{\\text{model}}}\n",
      "d\n",
      "intermediate\n",
      "{\\displaystyle d_{\\text{intermediate}}}\n",
      "n\n",
      "heads\n",
      "{\\displaystyle n_{\\text{heads}}}\n",
      "n\n",
      "kv-heads\n",
      "{\\displaystyle n_{\\text{kv-heads}}}\n",
      "7B\n",
      "30\n",
      "4096\n",
      "11008\n",
      "32\n",
      "32\n",
      "67B\n",
      "95\n",
      "8192\n",
      "22016\n",
      "64\n",
      "8\n",
      "[\n",
      "note 1\n",
      "]\n",
      "The\n",
      "Chat\n",
      "versions of the two\n",
      "Base\n",
      "models was also released concurrently, obtained by training\n",
      "Base\n",
      "by\n",
      "supervised finetuning (SFT) followed by direct policy optimization (DPO)\n",
      ".\n",
      "[\n",
      "27\n",
      "]\n",
      "On 9 January 2024, they released 2\n",
      "DeepSeek-MoE\n",
      "models (\n",
      "Base\n",
      ",\n",
      "Chat\n",
      "), each of 16B parameters (2.7B activated per token, 4K context length). The training was essentially the same as\n",
      "DeepSeek-LLM 7B\n",
      ", and was trained on a part of its training dataset. They claimed comparable performance with a 16B MoE as a 7B non-MoE. In architecture, it is a variant of the standard\n",
      "sparsely-gated MoE\n",
      ", with \"shared experts\" that are always queried, and \"routed experts\" that might not be. They found this to help with expert balancing. In standard MoE, some experts can become overly relied on, while other experts might be rarely used, wasting parameters. Attempting to balance the experts so that they are equally used then causes experts to replicate the same capacity. They proposed the shared experts to learn core capacities that are often used, and let the routed experts to learn the peripheral capacities that are rarely used.\n",
      "[\n",
      "29\n",
      "]\n",
      "In April 2024, they released 3\n",
      "DeepSeek-Math\n",
      "models specialized for doing math:\n",
      "Base\n",
      ",\n",
      "Instruct\n",
      ",\n",
      "RL\n",
      ". It was trained as follows:\n",
      "[\n",
      "30\n",
      "]\n",
      "Initialize with a previously pretrained\n",
      "DeepSeek-Coder-Base-v1.5 7B\n",
      ".\n",
      "Further pretrain with 500B tokens (6% DeepSeekMath Corpus, 4% AlgebraicStack, 10% arXiv, 20% GitHub code, 10% Common Crawl). This produced the\n",
      "Base\n",
      "model.\n",
      "Train an instruction-following model by SFT\n",
      "Base\n",
      "with 776K math problems and their tool-use-integrated step-by-step solutions. This produced the\n",
      "Instruct\n",
      "model.\n",
      "Reinforcement learning\n",
      "(RL): The reward model was a\n",
      "process reward model\n",
      "(PRM) trained from\n",
      "Base\n",
      "according to the Math-Shepherd method.\n",
      "[\n",
      "31\n",
      "]\n",
      "This reward model was then used to train\n",
      "Instruct\n",
      "using\n",
      "group relative policy optimization\n",
      "(GRPO) on a dataset of 144K math questions \"related to GSM8K and MATH\". The reward model was continuously updated during training to avoid reward hacking. This resulted in the\n",
      "RL\n",
      "model.\n",
      "V2\n",
      "[\n",
      "edit\n",
      "]\n",
      "In May 2024, they released the\n",
      "DeepSeek-V2\n",
      "series. The series includes 4 models, 2 base models (\n",
      "DeepSeek-V2\n",
      ",\n",
      "DeepSeek-V2-Lite\n",
      ") and 2 chatbots (\n",
      "-Chat\n",
      "). The two larger models were trained as follows:\n",
      "[\n",
      "32\n",
      "]\n",
      "Pretrain on a dataset of 8.1T tokens, where Chinese tokens are 12% more than English ones.\n",
      "Extend context length from 4K to 128K using YaRN.\n",
      "[\n",
      "33\n",
      "]\n",
      "This resulted in\n",
      "DeepSeek-V2\n",
      ".\n",
      "SFT with 1.2M instances for helpfulness and 0.3M for safety. This resulted in\n",
      "DeepSeek-V2-Chat (SFT)\n",
      "which was not released.\n",
      "RL using GRPO in two stages. The first stage was trained to solve math and coding problems. This stage used 1 reward model, trained on compiler feedback (for coding) and ground-truth labels (for math). The second stage was trained to be helpful, safe, and follow rules. This stage used 3 reward models. The helpfulness and safety reward models were trained on human preference data. The rule-based reward model was manually programmed. All trained reward models were initialized from\n",
      "DeepSeek-V2-Chat (SFT)\n",
      ". This resulted in the released version of\n",
      "DeepSeek-V2-Chat\n",
      ".\n",
      "They opted for 2-staged RL, because they found that RL on reasoning data had \"unique characteristics\" different from RL on general data. For example, RL on reasoning could improve over more training steps.\n",
      "[\n",
      "32\n",
      "]\n",
      "The two\n",
      "V2-Lite\n",
      "models were smaller, and trained similarly, though\n",
      "DeepSeek-V2-Lite-Chat\n",
      "only underwent SFT, not RL. They trained the Lite version to help \"further research and development on MLA and DeepSeekMoE\".\n",
      "[\n",
      "32\n",
      "]\n",
      "Architecturally, the V2 models were significantly modified from the DeepSeek LLM series. They changed the standard attention mechanism by a\n",
      "low-rank approximation\n",
      "called\n",
      "multi-head latent attention\n",
      "(MLA), and used the\n",
      "mixture of experts\n",
      "(MoE) variant previously published in January.\n",
      "[\n",
      "29\n",
      "]\n",
      "DeepSeek V2 properties\n",
      "[\n",
      "32\n",
      "]\n",
      ": Section 3.1.2, Appendix B\n",
      "[\n",
      "34\n",
      "]\n",
      "[\n",
      "35\n",
      "]\n",
      "Name\n",
      "Params\n",
      ".\n",
      "Active\n",
      "params\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "Context length\n",
      "n\n",
      "shared experts\n",
      "{\\displaystyle n_{\\text{shared experts}}}\n",
      "n\n",
      "routed experts\n",
      "{\\displaystyle n_{\\text{routed experts}}}\n",
      "V2-Lite\n",
      "15.7B\n",
      "2.4B\n",
      "27\n",
      "32K\n",
      "2\n",
      "64\n",
      "V2\n",
      "236B\n",
      "21B\n",
      "60\n",
      "128K\n",
      "2\n",
      "160\n",
      "The\n",
      "Financial Times\n",
      "reported that it was cheaper than its peers with a price of 2\n",
      "RMB\n",
      "for every million output tokens. The\n",
      "University of Waterloo\n",
      "Tiger Lab's leaderboard ranked DeepSeek-V2 seventh on its LLM ranking.\n",
      "[\n",
      "20\n",
      "]\n",
      "In June 2024, they released 4 models in the\n",
      "DeepSeek-Coder-V2\n",
      "series:\n",
      "V2-Base\n",
      ",\n",
      "V2-Lite-Base\n",
      ",\n",
      "V2-Instruct\n",
      ",\n",
      "V2-Lite-Instruct\n",
      ". They were trained as follows:\n",
      "[\n",
      "36\n",
      "]\n",
      "[\n",
      "note 2\n",
      "]\n",
      "The\n",
      "Base\n",
      "models were initialized from corresponding\n",
      "intermediate\n",
      "checkpoints after pretraining on 4.2T tokens (not the version at the end of pretraining), then pretrained further for 6T tokens, then context-extended to 128K context length. This produced the\n",
      "Base\n",
      "models.\n",
      "DeepSeek-Coder\n",
      "and\n",
      "DeepSeek-Math\n",
      "were used to generate 20K code-related and 30K math-related instruction data, then combined with an instruction dataset of 300M tokens. This was used for SFT.\n",
      "RL with GRPO. The reward for math problems was computed by comparing with the ground-truth label. The reward for code problems was generated by a reward model trained to predict whether a program would pass the unit tests.\n",
      "DeepSeek-V2.5\n",
      "was released in September and updated in December 2024. It was made by combining\n",
      "DeepSeek-V2-Chat\n",
      "and\n",
      "DeepSeek-Coder-V2-Instruct\n",
      ".\n",
      "[\n",
      "37\n",
      "]\n",
      "V3\n",
      "[\n",
      "edit\n",
      "]\n",
      "In December 2024, they released a base model\n",
      "DeepSeek-V3-Base\n",
      "and a chat model\n",
      "DeepSeek-V3\n",
      ". The model architecture is essentially the same as V2. They were trained as follows:\n",
      "[\n",
      "38\n",
      "]\n",
      "Pretraining on 14.8T tokens of a multilingual corpus, mostly English and Chinese. It contained a higher ratio of math and programming than the pretraining dataset of V2.\n",
      "Extend context length twice, from 4K to 32K and then to 128K, using YaRN.\n",
      "[\n",
      "33\n",
      "]\n",
      "This produced\n",
      "DeepSeek-V\n",
      "3-Base\n",
      ".\n",
      "SFT for 2 epochs on 1.5M samples of reasoning (math, programming, logic) and non-reasoning (creative writing, roleplay, simple question answering) data. Reasoning data was generated by \"expert models\". Non-reasoning data was generated by\n",
      "DeepSeek-V2.5\n",
      "and checked by humans.\n",
      "The \"expert models\" were trained by starting with an unspecified base model, then SFT on both\n",
      "<problem, original response>\n",
      "data, and synthetic\n",
      "<system prompt, problem, R1 response>\n",
      "data generated by an internal\n",
      "DeepSeek-R1\n",
      "model. The system prompt asked the\n",
      "R1\n",
      "to reflect and verify during thinking. Then the expert models were RL using an unspecified reward function.\n",
      "Each expert model was trained to generate just synthetic reasoning data in one specific domain (math, programming, logic).\n",
      "Expert models were used, instead of\n",
      "R1\n",
      "itself, since the output from\n",
      "R1\n",
      "itself suffered \"overthinking, poor formatting, and excessive length\".\n",
      "Model-based reward models were made by starting with a SFT checkpoint of\n",
      "V3\n",
      ", then finetuning on human preference data containing both final reward and chain-of-thought leading to the final reward. The reward model produced reward signals for both questions with objective but free-form answers, and questions without objective answers (such as creative writing).\n",
      "A SFT checkpoint of\n",
      "V3\n",
      "was trained by GRPO using both reward models and rule-based reward. The rule-based reward was computed for math problems with a final answer (put in a box), and for programming problems by unit tests. This produced\n",
      "DeepSeek-V3\n",
      ".\n",
      "DeepSeek V3 properties\n",
      "[\n",
      "38\n",
      "]\n",
      ": Section 4.2\n",
      "[\n",
      "39\n",
      "]\n",
      "Name\n",
      "Params\n",
      ".\n",
      "Active\n",
      "params\n",
      "n\n",
      "layers\n",
      "{\\displaystyle n_{\\text{layers}}}\n",
      "Context length\n",
      "n\n",
      "shared experts\n",
      "{\\displaystyle n_{\\text{shared experts}}}\n",
      "n\n",
      "routed experts\n",
      "{\\displaystyle n_{\\text{routed experts}}}\n",
      "V3\n",
      "671B\n",
      "37B\n",
      "61\n",
      "128K\n",
      "1\n",
      "256\n",
      "The DeepSeek team performed extensive low-level engineering to achieve efficiency. They used\n",
      "mixed-precision arithmetic\n",
      ". Much of the forward pass was performed in\n",
      "8-bit floating point numbers\n",
      "(5E2M: 5-bit exponent and 2-bit\n",
      "mantissa\n",
      ") rather than the standard\n",
      "32-bit\n",
      ", requiring special\n",
      "GEMM\n",
      "routines to accumulate accurately. They used a custom 12-bit float (E5M6) for\n",
      "only\n",
      "the inputs to the linear layers after the attention modules. Optimizer states were in 16-bit (\n",
      "BF16\n",
      "). They minimized the communication latency by overlapping extensively computation and communication, such as dedicating 20\n",
      "streaming multiprocessors\n",
      "out of 132 per H800 for only inter-GPU communication. They lowered communication by rearranging (every 10 minutes) the exact machine each expert was on in order to avoid certain machines being queried more often than the others, adding auxiliary load-balancing losses to the training loss function, and other load-balancing techniques.\n",
      "[\n",
      "38\n",
      "]\n",
      "After training, it was deployed on H800 clusters. The H800 cards within a cluster are connected by NVLink, and the clusters are connected by InfiniBand.\n",
      "[\n",
      "38\n",
      "]\n",
      "Total cost of training the DeepSeek-V3 model\n",
      "[\n",
      "38\n",
      "]\n",
      ": Table 1\n",
      "Stage\n",
      "Cost (in one thousand GPU hours)\n",
      "Cost (in one million USD$)\n",
      "Pre-training\n",
      "2,664\n",
      "5.328\n",
      "Context extension\n",
      "119\n",
      "0.24\n",
      "Fine-tuning\n",
      "5\n",
      "0.01\n",
      "Total\n",
      "2,788\n",
      "5.576\n",
      "Benchmark tests show that DeepSeek-V3 outperformed\n",
      "Llama\n",
      "3.1 and\n",
      "Qwen\n",
      "2.5 whilst matching\n",
      "GPT-4o\n",
      "and\n",
      "Claude\n",
      "3.5 Sonnet.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "40\n",
      "]\n",
      "[\n",
      "41\n",
      "]\n",
      "[\n",
      "42\n",
      "]\n",
      "R1\n",
      "[\n",
      "edit\n",
      "]\n",
      "On 20 November 2024,\n",
      "DeepSeek-R1-Lite-Preview\n",
      "became accessible via DeepSeek's API, as well as via a chat interface after logging in.\n",
      "[\n",
      "43\n",
      "]\n",
      "[\n",
      "44\n",
      "]\n",
      "[\n",
      "note 3\n",
      "]\n",
      "It was trained for logical inference, mathematical reasoning, and real-time problem-solving. DeepSeek claimed that it exceeded performance of\n",
      "OpenAI o1\n",
      "on benchmarks such as\n",
      "American Invitational Mathematics Examination\n",
      "(AIME) and MATH.\n",
      "[\n",
      "45\n",
      "]\n",
      "However,\n",
      "The Wall Street Journal\n",
      "stated when it used 15 problems from the 2024 edition of AIME, the o1 model reached a solution faster than\n",
      "DeepSeek-R1-Lite-Preview\n",
      ".\n",
      "[\n",
      "46\n",
      "]\n",
      "On 20 January 2025, DeepSeek released\n",
      "DeepSeek-R1\n",
      "and\n",
      "DeepSeek-R1-Zero\n",
      ".\n",
      "[\n",
      "47\n",
      "]\n",
      "Both were initialized from\n",
      "DeepSeek-V3-Base\n",
      ", and share its architecture. The company also released some \"\n",
      "DeepSeek-R1-Distill\n",
      "\" models, which are\n",
      "not\n",
      "initialized on\n",
      "V3-Base\n",
      ", but instead are initialized from other pretrained open-weight models, including\n",
      "LLaMA\n",
      "and\n",
      "Qwen\n",
      ", then fine-tuned on\n",
      "synthetic data\n",
      "generated by\n",
      "R1\n",
      ".\n",
      "[\n",
      "48\n",
      "]\n",
      "Template for\n",
      "DeepSeek-R1-Zero\n",
      "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <prompt>. Assistant:\n",
      "– <prompt> is replaced with the specific reasoning question during training.\n",
      "DeepSeek-R1-Zero\n",
      "was trained exclusively using GRPO RL without SFT. Unlike previous versions, they used no model-based reward. All reward functions were rule-based, \"mainly\" of two types (other types were not specified): accuracy rewards and format rewards. Accuracy reward was checking whether a boxed answer is correct (for math) or whether a code passes tests (for programming). Format reward was checking whether the model puts its thinking trace within\n",
      "<think>...</think>\n",
      ".\n",
      "[\n",
      "48\n",
      "]\n",
      "As\n",
      "R1-Zero\n",
      "has issues with readability and mixing languages,\n",
      "R1\n",
      "was trained to address these issues and further improve reasoning:\n",
      "[\n",
      "48\n",
      "]\n",
      "SFT\n",
      "DeepSeek-V3-Base\n",
      "on \"thousands\" of \"cold-start\" data all with the standard format of\n",
      "|special_token|<reasoning_process>|special_token|summary>\n",
      ".\n",
      "Apply the same RL process as\n",
      "R1-Zero\n",
      ", but also with a \"language consistency reward\" to encourage it to respond monolingually. This produced an internal model not released.\n",
      "Synthesize 600K reasoning data from the internal model, with rejection sampling (i.e. if the generated reasoning had a wrong final answer, then it is removed). Synthesize 200K non-reasoning data (writing, factual QA, self-cognition, translation) using\n",
      "DeepSeek-V3\n",
      ".\n",
      "SFT\n",
      "DeepSeek-V3-Base\n",
      "on the 800K synthetic data for 2 epochs.\n",
      "GRPO RL with rule-based reward (for reasoning tasks) and model-based reward (for non-reasoning tasks, helpfulness, and harmlessness). This produced\n",
      "DeepSeek-R1\n",
      ".\n",
      "Distilled models were trained by SFT on 800K data synthesized from\n",
      "DeepSeek-R1\n",
      ", in a similar way as step 3 above. They were not trained with RL.\n",
      "[\n",
      "48\n",
      "]\n",
      "Assessment and reactions\n",
      "[\n",
      "edit\n",
      "]\n",
      "DeepSeek released its\n",
      "AI Assistant\n",
      ", which uses the V3 model as a\n",
      "chatbot app\n",
      "for\n",
      "Apple IOS\n",
      "and\n",
      "Android\n",
      ". By January 27, 2025, the app had surpassed ChatGPT as the highest-rated free app on the iOS App Store in the United States. Its chatbot reportedly answers questions, solves logic problems, and writes computer programs on par with other chatbots on the market, according to benchmark tests used by American AI companies.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek-V3 uses significantly fewer resources compared to its peers; for example, whereas the world's leading AI companies train their chatbots with\n",
      "supercomputers\n",
      "using as many as 16,000\n",
      "graphics processing units (GPUs)\n",
      ", if not more, DeepSeek claims to have needed only about 2,000 GPUs, namely the H800 series chip from\n",
      "Nvidia\n",
      ".\n",
      "[\n",
      "38\n",
      "]\n",
      "It was trained in around 55 days at a cost of US$5.58 million,\n",
      "[\n",
      "38\n",
      "]\n",
      "which is roughly one tenth of what United States tech giant\n",
      "Meta\n",
      "spent building its latest AI technology.\n",
      "[\n",
      "3\n",
      "]\n",
      "DeepSeek's competitive performance at relatively minimal cost has been recognized as potentially challenging the global dominance of American AI models.\n",
      "[\n",
      "49\n",
      "]\n",
      "Various publications and news media, such as\n",
      "The Hill\n",
      "and\n",
      "The Guardian\n",
      ",\n",
      "described the release of its chatbot as a \"\n",
      "Sputnik moment\n",
      "\" for American AI.\n",
      "[\n",
      "50\n",
      "]\n",
      "[\n",
      "51\n",
      "]\n",
      "The performance of its\n",
      "R1\n",
      "model was reportedly \"on par with\" one of OpenAI's latest models when used for tasks such as mathematics, coding, and natural language reasoning;\n",
      "[\n",
      "52\n",
      "]\n",
      "echoing other commentators, American Silicon Valley venture capitalist\n",
      "Marc Andreessen\n",
      "likewise described\n",
      "R1\n",
      "as \"AI's Sputnik moment\".\n",
      "[\n",
      "52\n",
      "]\n",
      "DeepSeek's founder, Liang Wenfeng has been compared to Open AI CEO\n",
      "Sam Altman\n",
      ", with\n",
      "CNN\n",
      "calling him the Sam Altman of China and an evangelist for AI.\n",
      "[\n",
      "53\n",
      "]\n",
      "Chinese\n",
      "state media\n",
      "widely praised DeepSeek as a national asset.\n",
      "[\n",
      "54\n",
      "]\n",
      "[\n",
      "55\n",
      "]\n",
      "On 20 January 2025, China's Premier\n",
      "Li Qiang\n",
      "invited Liang Wenfeng to his symposium with experts and asked him to provide opinions and suggestions on a draft for comments of the annual 2024 government work report.\n",
      "[\n",
      "56\n",
      "]\n",
      "DeepSeek's optimization of limited resources has highlighted potential limits of United States sanctions on China's AI development, which include export restrictions on advanced AI chips to China.\n",
      "[\n",
      "19\n",
      "]\n",
      "[\n",
      "57\n",
      "]\n",
      "The success of the company's AI models consequently \"sparked market turmoil\"\n",
      "[\n",
      "58\n",
      "]\n",
      "and caused shares in major global technology companies to plunge on 27 January 2025: Nvidia's stock fell by as much as 17–18%,\n",
      "[\n",
      "59\n",
      "]\n",
      "as did the stock of rival\n",
      "Broadcom\n",
      ". Other tech firms also sank, including\n",
      "Microsoft\n",
      "(down 2.5%),\n",
      "Google\n",
      "'s owner\n",
      "Alphabet\n",
      "(down over 4%), and Dutch chip equipment maker\n",
      "ASML\n",
      "(down over 7%).\n",
      "[\n",
      "52\n",
      "]\n",
      "A global selloff of technology stocks on\n",
      "Nasdaq\n",
      ", prompted by the release of the\n",
      "R1\n",
      "model, had led to record losses of about $593 billion in the market capitalizations of AI and computer hardware companies;\n",
      "[\n",
      "60\n",
      "]\n",
      "by 28 January 2025, a total of $1 trillion of value was wiped off American stocks.\n",
      "[\n",
      "51\n",
      "]\n",
      "The login error DeepSeek gave on 28 Jan 2025 following a cyberattack\n",
      "Leading figures in the American AI sector had mixed reactions to DeepSeek's success and performance.\n",
      "[\n",
      "61\n",
      "]\n",
      "Microsoft CEO\n",
      "Satya Nadella\n",
      "and OpenAI CEO Sam Altman—whose companies are involved in the United States government-backed \"\n",
      "Stargate Project\n",
      "\" to develop American AI infrastructure—both called DeepSeek \"super impressive\".\n",
      "[\n",
      "62\n",
      "]\n",
      "[\n",
      "63\n",
      "]\n",
      "American President Donald Trump, who announced The Stargate Project, called DeepSeek a wake-up call\n",
      "[\n",
      "64\n",
      "]\n",
      "and a positive development.\n",
      "[\n",
      "65\n",
      "]\n",
      "[\n",
      "51\n",
      "]\n",
      "[\n",
      "52\n",
      "]\n",
      "[\n",
      "66\n",
      "]\n",
      "Other leaders in the field, including\n",
      "Scale AI\n",
      "CEO Alexandr Wang,\n",
      "Anthropic\n",
      "cofounder and CEO\n",
      "Dario Amodei\n",
      ", and\n",
      "Elon Musk\n",
      "expressed skepticism of the app's performance or of the sustainability of its success.\n",
      "[\n",
      "61\n",
      "]\n",
      "[\n",
      "67\n",
      "]\n",
      "[\n",
      "68\n",
      "]\n",
      "Various companies, including\n",
      "Amazon Web Services\n",
      ",\n",
      "Toyota\n",
      ", and\n",
      "Stripe\n",
      ", are seeking to use the model in their program.\n",
      "[\n",
      "69\n",
      "]\n",
      "On 27 January 2025, DeepSeek limited its new user registration to phone numbers from mainland China, email addresses, or Google account logins, following a \"large-scale\"\n",
      "cyberattack\n",
      "disrupted the proper functioning of its servers.\n",
      "[\n",
      "70\n",
      "]\n",
      "[\n",
      "71\n",
      "]\n",
      "Concerns\n",
      "[\n",
      "edit\n",
      "]\n",
      "Censorship\n",
      "[\n",
      "edit\n",
      "]\n",
      "See also:\n",
      "Chinese censorship abroad\n",
      "and\n",
      "Censorship in China\n",
      "DeepSeek responses when asked about\n",
      "Xi Jinping\n",
      "and\n",
      "Narendra Modi\n",
      "Some sources have observed that the official application programming interface (API) version of R1, which runs from servers located in China, uses\n",
      "censorship\n",
      "mechanisms for topics that are considered politically sensitive for the\n",
      "government of China\n",
      ". For example, the model refuses to answer questions about the\n",
      "1989 Tiananmen Square massacre\n",
      ",\n",
      "persecution of Uyghurs\n",
      ",\n",
      "comparisons between Xi Jinping and Winnie the Pooh\n",
      ", and\n",
      "human rights in China\n",
      ".\n",
      "[\n",
      "72\n",
      "]\n",
      "[\n",
      "73\n",
      "]\n",
      "[\n",
      "74\n",
      "]\n",
      "The AI may initially generate an answer, but then deletes it shortly afterwards and replaces it with a message such as: \"Sorry, that's beyond my current scope. Let's talk about something else.\"\n",
      "[\n",
      "73\n",
      "]\n",
      "The integrated censorship mechanisms and restrictions can only be removed to a limited extent in the open-source version of the R1 model. If the \"\n",
      "core socialist values\n",
      "\" defined by the\n",
      "Chinese Internet regulatory authorities\n",
      "are touched upon, or the\n",
      "political status of Taiwan\n",
      "is raised, discussions are terminated.\n",
      "[\n",
      "75\n",
      "]\n",
      "When tested by\n",
      "NBC News\n",
      ", DeepSeek's R1 described Taiwan as \"an inalienable part of China's territory,\" and stated: \"We firmly oppose any form of '\n",
      "Taiwan independence\n",
      "' separatist activities and are committed to achieving the complete reunification of the motherland through peaceful means.\"\n",
      "[\n",
      "76\n",
      "]\n",
      "In January 2025, Western researchers were able to trick DeepSeek into giving certain answers to some of these topics by requesting in its answer to swap certain letters for\n",
      "similar-looking numbers\n",
      ".\n",
      "[\n",
      "74\n",
      "]\n",
      "Security and privacy\n",
      "[\n",
      "edit\n",
      "]\n",
      "See also:\n",
      "Chinese information operations and information warfare\n",
      "Some experts fear that the government of China could use the AI system for foreign\n",
      "influence operations\n",
      ", spreading\n",
      "disinformation\n",
      ",\n",
      "surveillance\n",
      "and the development of\n",
      "cyberweapons\n",
      ".\n",
      "[\n",
      "77\n",
      "]\n",
      "[\n",
      "78\n",
      "]\n",
      "[\n",
      "79\n",
      "]\n",
      "DeepSeek's privacy terms and conditions say \"We store the information we collect in secure servers located in the People's Republic of China... We may collect your text or audio input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services\". According to a review by\n",
      "Wired\n",
      ", DeepSeek also sends data to\n",
      "Baidu\n",
      "'s web analytics service and collects data from\n",
      "ByteDance\n",
      ".\n",
      "[\n",
      "80\n",
      "]\n",
      "In response, the Italian data protection authority is seeking additional information on DeepSeek's collection and use of personal data, and the\n",
      "United States National Security Council\n",
      "announced that it had started a national security review.\n",
      "[\n",
      "81\n",
      "]\n",
      "[\n",
      "82\n",
      "]\n",
      "Taiwan's government banned the use of DeepSeek at government ministries on security grounds and South Korea's\n",
      "Personal Information Protection Commission\n",
      "opened an inquiry into DeepSeek's use of personal information.\n",
      "[\n",
      "83\n",
      "]\n",
      "See also\n",
      "[\n",
      "edit\n",
      "]\n",
      "Artificial intelligence industry in China\n",
      "Notes\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "The number of heads does not equal the number of KV heads, due to GQA.\n",
      "^\n",
      "Inexplicably, the model named\n",
      "DeepSeek-Coder-V2 Chat\n",
      "in the paper was released as\n",
      "DeepSeek-Coder-V2-Instruct\n",
      "in HuggingFace.\n",
      "^\n",
      "At that time, the\n",
      "R1-Lite-Preview\n",
      "required selecting \"Deep Think enabled\", and every user could use it only 50 times a day.\n",
      "References\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "Gibney, Elizabeth (23 January 2025).\n",
      "\"China's cheap, open AI model DeepSeek thrills scientists\"\n",
      ".\n",
      "Nature\n",
      ".\n",
      "doi\n",
      ":\n",
      "10.1038/d41586-025-00229-6\n",
      ".\n",
      "ISSN\n",
      "1476-4687\n",
      ".\n",
      "PMID\n",
      "39849139\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Vincent, James (28 January 2025).\n",
      "\"The DeepSeek panic reveals an AI world ready to blow\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "Metz, Cade; Tobin, Meaghan (23 January 2025).\n",
      "\"How Chinese A.I. Start-Up DeepSeek Is Competing With Silicon Valley Giants\"\n",
      ".\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Cosgrove, Emma (27 January 2025).\n",
      "\"DeepSeek's cheaper models and weaker chips call into question trillions in AI infrastructure spending\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Mallick, Subhrojit (16 January 2024).\n",
      "\"Biden admin's cap on GPU exports may hit India's AI ambitions\"\n",
      ".\n",
      "The Economic Times\n",
      ". Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Saran, Cliff (10 December 2024).\n",
      "\"Nvidia investigation signals widening of US and China chip war | Computer Weekly\"\n",
      ".\n",
      "Computer Weekly\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Sherman, Natalie (9 December 2024).\n",
      "\"Nvidia targeted by China in new chip war probe\"\n",
      ".\n",
      "BBC\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Metz, Cade (27 January 2025).\n",
      "\"What is DeepSeek? And How Is It Upending A.I.?\"\n",
      ".\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Field, Hayden (27 January 2025).\n",
      "\"China's DeepSeek AI dethrones ChatGPT on App Store: Here's what you should know\"\n",
      ".\n",
      "CNBC\n",
      ".\n",
      "^\n",
      "Picchi, Aimee (27 January 2025).\n",
      "\"What is DeepSeek, and why is it causing Nvidia and other stocks to slump?\"\n",
      ".\n",
      "CBS News\n",
      ".\n",
      "^\n",
      "Zahn, Max (27 January 2025).\n",
      "\"Nvidia, Microsoft shares tumble as China-based AI app DeepSeek hammers tech giants\"\n",
      ".\n",
      "ABC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Roose, Kevin (28 January 2025).\n",
      "\"Why DeepSeek Could Change What Silicon Valley Believe About A.I.\"\n",
      "The New York Times\n",
      ".\n",
      "ISSN\n",
      "0362-4331\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Romero, Luis E. (28 January 2025).\n",
      "\"ChatGPT, DeepSeek, Or Llama? Meta's LeCun Says Open-Source Is The Key\"\n",
      ".\n",
      "Forbes\n",
      ".\n",
      "^\n",
      "Chen, Caiwei (24 January 2025).\n",
      "\"How a top Chinese AI model overcame US sanctions\"\n",
      ".\n",
      "MIT Technology Review\n",
      ".\n",
      "Archived\n",
      "from the original on 25 January 2025\n",
      ". Retrieved\n",
      "25 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "Ottinger, Lily (9 December 2024).\n",
      "\"Deepseek: From Hedge Fund to Frontier Model Maker\"\n",
      ".\n",
      "ChinaTalk\n",
      ".\n",
      "Archived\n",
      "from the original on 28 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Olcott, Eleanor; Wu, Zijing (24 January 2025).\n",
      "\"How small Chinese AI start-up DeepSeek shocked Silicon Valley\"\n",
      ".\n",
      "Financial Times\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Leswing, Kif (23 February 2023).\n",
      "\"Meet the $10,000 Nvidia chip powering the race for A.I.\"\n",
      "CNBC\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Yu, Xu (17 April 2023).\n",
      "\"[Exclusive] Chinese Quant Hedge Fund High-Flyer Won't Use AGI to Trade Stocks, MD Says\"\n",
      ".\n",
      "Yicai Global\n",
      ".\n",
      "Archived\n",
      "from the original on 31 December 2023\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "Jiang, Ben; Perezi, Bien (1 January 2025).\n",
      "\"Meet DeepSeek: the Chinese start-up that is changing how AI models are trained\"\n",
      ".\n",
      "South China Morning Post\n",
      ".\n",
      "Archived\n",
      "from the original on 22 January 2025\n",
      ". Retrieved\n",
      "1 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "McMorrow, Ryan; Olcott, Eleanor (9 June 2024).\n",
      "\"The Chinese quant fund-turned-AI pioneer\"\n",
      ".\n",
      "Financial Times\n",
      ".\n",
      "Archived\n",
      "from the original on 17 July 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Schneider, Jordan (27 November 2024).\n",
      "\"Deepseek: The Quiet Giant Leading China's AI Race\"\n",
      ".\n",
      "ChinaTalk\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"DeepSeek-Coder/LICENSE-MODEL at main · deepseek-ai/DeepSeek-Coder\"\n",
      ".\n",
      "GitHub\n",
      ".\n",
      "Archived\n",
      "from the original on 22 January 2025\n",
      ". Retrieved\n",
      "24 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Guo, Daya; Zhu, Qihao; Yang, Dejian; Xie, Zhenda; Dong, Kai; Zhang, Wentao; Chen, Guanting; Bi, Xiao; Wu, Y. (26 January 2024),\n",
      "DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.14196\n",
      "^\n",
      "\"DeepSeek Coder\"\n",
      ".\n",
      "deepseekcoder.github.io\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "deepseek-ai/DeepSeek-Coder\n",
      ", DeepSeek, 27 January 2025\n",
      ", retrieved\n",
      "27 January\n",
      "2025\n",
      "^\n",
      "\"deepseek-ai/deepseek-coder-5.7bmqa-base · Hugging Face\"\n",
      ".\n",
      "Hugging Face\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (5 January 2024),\n",
      "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.02954\n",
      "^\n",
      "deepseek-ai/DeepSeek-LLM\n",
      ", DeepSeek, 27 January 2025\n",
      ", retrieved\n",
      "27 January\n",
      "2025\n",
      "^\n",
      "a\n",
      "b\n",
      "Dai, Damai; Deng, Chengqi; Zhao, Chenggang; Xu, R. X.; Gao, Huazuo; Chen, Deli; Li, Jiashi; Zeng, Wangding; Yu, Xingkai (11 January 2024),\n",
      "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2401.06066\n",
      "^\n",
      "Shao, Zhihong; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Song, Junxiao; Bi, Xiao; Zhang, Haowei; Zhang, Mingchuan; Li, Y. K. (27 April 2024),\n",
      "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2402.03300\n",
      ".\n",
      "^\n",
      "Wang, Peiyi; Li, Lei; Shao, Zhihong; Xu, R. X.; Dai, Damai; Li, Yifei; Chen, Deli; Wu, Y.; Sui, Zhifang (19 February 2024),\n",
      "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2312.08935\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (19 June 2024),\n",
      "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2405.04434\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Peng, Bowen; Quesnelle, Jeffrey; Fan, Honglu; Shippole, Enrico (1 November 2023),\n",
      "YaRN: Efficient Context Window Extension of Large Language Models\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2309.00071\n",
      ".\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V2-Lite at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 15 May 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V2 at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 6 May 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "DeepSeek-AI; Zhu, Qihao; Guo, Daya; Shao, Zhihong; Yang, Dejian; Wang, Peiyi; Xu, Runxin; Wu, Y.; Li, Yukun (17 June 2024),\n",
      "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2406.11931\n",
      "^\n",
      "\"deepseek-ai/DeepSeek-V2.5 · Hugging Face\"\n",
      ".\n",
      "Hugging Face\n",
      ". 3 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "DeepSeek-AI; Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi (27 December 2024),\n",
      "DeepSeek-V3 Technical Report\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2412.19437\n",
      "^\n",
      "\"config.json · deepseek-ai/DeepSeek-V3 at main\"\n",
      ".\n",
      "Hugging Face\n",
      ". 26 December 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Jiang, Ben (27 December 2024).\n",
      "\"Chinese start-up DeepSeek's new AI model outperforms Meta, OpenAI products\"\n",
      ".\n",
      "South China Morning Post\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Sharma, Shubham (26 December 2024).\n",
      "\"DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch\"\n",
      ".\n",
      "VentureBeat\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Wiggers, Kyle (26 December 2024).\n",
      "\"DeepSeek's new AI model appears to be one of the best 'open' challengers yet\"\n",
      ".\n",
      "TechCrunch\n",
      ".\n",
      "Archived\n",
      "from the original on 2 January 2025\n",
      ". Retrieved\n",
      "31 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"Deepseek Log in page\"\n",
      ".\n",
      "DeepSeek\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"News | DeepSeek-R1-Lite Release 2024/11/20: 🚀 DeepSeek-R1-Lite-Preview is now live: unleashing supercharged reasoning power!\"\n",
      ".\n",
      "DeepSeek API Docs\n",
      ". Archived from\n",
      "the original\n",
      "on 20 November 2024\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Franzen, Carl (20 November 2024).\n",
      "\"DeepSeek's first reasoning model R1-Lite-Preview turns heads, beating OpenAI o1 performance\"\n",
      ".\n",
      "VentureBeat\n",
      ".\n",
      "Archived\n",
      "from the original on 22 November 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "Huang, Raffaele (24 December 2024).\n",
      "\"Don't Look Now, but China's AI Is Catching Up Fast\"\n",
      ".\n",
      "The Wall Street Journal\n",
      ".\n",
      "Archived\n",
      "from the original on 27 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"Release DeepSeek-R1 · deepseek-ai/DeepSeek-R1@23807ce\"\n",
      ".\n",
      "GitHub\n",
      ".\n",
      "Archived\n",
      "from the original on 21 January 2025\n",
      ". Retrieved\n",
      "21 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "DeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (22 January 2025),\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
      ",\n",
      "arXiv\n",
      ":\n",
      "2501.12948\n",
      "^\n",
      "\"Chinese AI startup DeepSeek overtakes ChatGPT on Apple App Store\"\n",
      ".\n",
      "Reuters\n",
      ". 27 January 2025\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Wade, David (6 December 2024).\n",
      "\"American AI has reached its Sputnik moment\"\n",
      ".\n",
      "The Hill\n",
      ".\n",
      "Archived\n",
      "from the original on 8 December 2024\n",
      ". Retrieved\n",
      "25 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "Milmo, Dan; Hawkins, Amy; Booth, Robert; Kollewe, Julia (28 January 2025).\n",
      "\"\n",
      "'Sputnik moment': $1tn wiped off US stocks after Chinese firm unveils AI chatbot\"\n",
      "– via The Guardian.\n",
      "^\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "Hoskins, Peter; Rahman-Jones, Imran (27 January 2025).\n",
      "\"Nvidia shares sink as Chinese AI app spooks markets\"\n",
      ".\n",
      "BBC\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Goldman, David (27 January 2025).\n",
      "\"What is DeepSeek, the Chinese AI startup that shook the tech world? | CNN Business\"\n",
      ".\n",
      "CNN\n",
      ". Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"DeepSeek poses a challenge to Beijing as much as to Silicon Valley\"\n",
      ".\n",
      "The Economist\n",
      ". 29 January 2025.\n",
      "ISSN\n",
      "0013-0613\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Paul, Katie; Nellis, Stephen (30 January 2025).\n",
      "\"Chinese state-linked accounts hyped DeepSeek AI launch ahead of US stock rout, Graphika says\"\n",
      ".\n",
      "Reuters\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "澎湃新闻 (22 January 2025).\n",
      "\"量化巨头幻方创始人梁文锋参加总理座谈会并发言，他还创办了\"AI界拼多多\"\n",
      "\"\n",
      ".\n",
      "finance.sina.com.cn\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Shilov, Anton (27 December 2024).\n",
      "\"Chinese AI company's AI model breakthrough highlights limits of US sanctions\"\n",
      ".\n",
      "Tom's Hardware\n",
      ".\n",
      "Archived\n",
      "from the original on 28 December 2024\n",
      ". Retrieved\n",
      "28 December\n",
      "2024\n",
      ".\n",
      "^\n",
      "\"DeepSeek updates – Chinese AI chatbot sparks US market turmoil, wiping $500bn off Nvidia\"\n",
      ".\n",
      "BBC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Nazareth, Rita (26 January 2025).\n",
      "\"Stock Rout Gets Ugly as Nvidia Extends Loss to 17%: Markets Wrap\"\n",
      ".\n",
      "Bloomberg\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Carew, Sinéad; Cooper, Amanda; Banerjee, Ankur (27 January 2025).\n",
      "\"DeepSeek sparks global AI selloff, Nvidia losses about $593 billion of value\"\n",
      ". Reuters.\n",
      "^\n",
      "a\n",
      "b\n",
      "Sherry, Ben (28 January 2025).\n",
      "\"DeepSeek, Calling It 'Impressive' but Staying Skeptical\"\n",
      ".\n",
      "Inc.\n",
      "Retrieved\n",
      "29 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Okemwa, Kevin (28 January 2025).\n",
      "\"Microsoft CEO Satya Nadella touts DeepSeek's open-source AI as \"super impressive\": \"We should take the developments out of China very, very seriously\"\n",
      "\"\n",
      ".\n",
      "Windows Central\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Nazzaro, Miranda (28 January 2025).\n",
      "\"OpenAI's Sam Altman calls DeepSeek model 'impressive'\n",
      "\"\n",
      ".\n",
      "The Hill\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Dou, Eva; Gregg, Aaron; Zakrzewski, Cat; Tiku, Nitasha; Najmabadi, Shannon (28 January 2025).\n",
      "\"Trump calls China's DeepSeek AI app a 'wake-up call' after tech stocks slide\"\n",
      ".\n",
      "The Washington Post\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Habeshian, Sareen (28 January 2025).\n",
      "\"Johnson bashes China on AI, Trump calls DeepSeek development \"positive\"\n",
      "\"\n",
      ".\n",
      "Axios\n",
      ".\n",
      "^\n",
      "Karaian, Jason; Rennison, Joe (27 January 2025).\n",
      "\"China's A.I. Advances Spook Big Tech Investors on Wall Street\"\n",
      "– via NYTimes.com.\n",
      "^\n",
      "Sharma, Manoj (6 January 2025).\n",
      "\"Musk dismisses, Altman applauds: What leaders say on DeepSeek's disruption\"\n",
      ".\n",
      "Fortune India\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Elon Musk 'questions' DeepSeek's claims, suggests massive Nvidia GPU infrastructure\"\n",
      ".\n",
      "Financialexpress\n",
      ". 28 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Kim, Eugene.\n",
      "\"Big AWS customers, including Stripe and Toyota, are hounding the cloud giant for access to DeepSeek AI models\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Kerr, Dara (27 January 2025).\n",
      "\"DeepSeek hit with 'large-scale' cyber-attack after AI chatbot tops app stores\"\n",
      ".\n",
      "The Guardian\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Tweedie, Steven; Altchek, Ana.\n",
      "\"DeepSeek temporarily limited new sign-ups, citing 'large-scale malicious attacks'\n",
      "\"\n",
      ".\n",
      "Business Insider\n",
      ".\n",
      "^\n",
      "Field, Matthew; Titcomb, James (27 January 2025).\n",
      "\"Chinese AI has sparked a $1 trillion panic – and it doesn't care about free speech\"\n",
      ".\n",
      "The Daily Telegraph\n",
      ".\n",
      "ISSN\n",
      "0307-1235\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Steinschaden, Jakob (27 January 2025).\n",
      "\"DeepSeek: This is what live censorship looks like in the Chinese AI chatbot\"\n",
      ".\n",
      "Trending Topics\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "a\n",
      "b\n",
      "Lu, Donna (28 January 2025).\n",
      "\"We tried out DeepSeek. It worked well, until we asked it about Tiananmen Square and Taiwan\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "30 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"The Guardian view on a global AI race: geopolitics, innovation and the rise of chaos\"\n",
      ".\n",
      "The Guardian\n",
      ". 26 January 2025.\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Yang, Angela; Cui, Jasmine (27 January 2025).\n",
      "\"Chinese AI DeepSeek jolts Silicon Valley, giving the AI race its 'Sputnik moment'\n",
      "\"\n",
      ".\n",
      "NBC News\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Kimery, Anthony (26 January 2025).\n",
      "\"China's DeepSeek AI poses formidable cyber, data privacy threats\"\n",
      ".\n",
      "Biometric Update\n",
      ". Retrieved\n",
      "27 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Booth, Robert; Milmo, Dan (28 January 2025).\n",
      "\"Experts urge caution over use of Chinese AI DeepSeek\"\n",
      ".\n",
      "The Guardian\n",
      ".\n",
      "ISSN\n",
      "0261-3077\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Hornby, Rael (28 January 2025).\n",
      "\"DeepSeek's success has painted a huge TikTok-shaped target on its back\"\n",
      ".\n",
      "LaptopMag\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Burgess, Matt; Newman, Lily Hay (27 January 2025).\n",
      "\"DeepSeek's Popular AI App Is Explicitly Sending US Data to China\"\n",
      ".\n",
      "Wired\n",
      ".\n",
      "ISSN\n",
      "1059-1028\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Italy regulator seeks information from DeepSeek on data protection\"\n",
      ".\n",
      "Reuters\n",
      ". 28 January 2025\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "Shalal, Andrea; Shepardson, David (28 January 2025).\n",
      "\"White House evaluates effect of China AI app DeepSeek on national security, official says\"\n",
      ".\n",
      "Reuters\n",
      ". Retrieved\n",
      "28 January\n",
      "2025\n",
      ".\n",
      "^\n",
      "\"Taiwan says government departments should not use DeepSeek, citing security concerns\"\n",
      ".\n",
      "Reuters\n",
      ". 31 January 2025\n",
      ". Retrieved\n",
      "31 January\n",
      "2025\n",
      ".\n",
      "External links\n",
      "[\n",
      "edit\n",
      "]\n",
      "Free and open-source software portal\n",
      "Wikimedia Commons has media related to\n",
      "DeepSeek\n",
      ".\n",
      "Official website\n",
      "DeepSeek\n",
      "on\n",
      "GitHub\n",
      "DeepSeek\n",
      "on\n",
      "Hugging Face\n",
      "Official API documentation\n",
      "Anthology of DeepSeek papers\n",
      "v\n",
      "t\n",
      "e\n",
      "Generative AI\n",
      "chatbots\n",
      "United States\n",
      "ChatGPT\n",
      "Claude\n",
      "Copilot\n",
      "Gemini\n",
      "Grok\n",
      "Poe\n",
      "Replika\n",
      "You.com\n",
      "Russia\n",
      "YandexGPT\n",
      "China\n",
      "DeepSeek\n",
      "Qwen\n",
      "Europe\n",
      "Mistral\n",
      "(France)\n",
      "Korea\n",
      "Galaxy AI\n",
      "Defunct\n",
      "Bard\n",
      "Related\n",
      "Large language models\n",
      "Category\n",
      "v\n",
      "t\n",
      "e\n",
      "Generative AI\n",
      "Concepts\n",
      "Autoencoder\n",
      "Deep learning\n",
      "Generative adversarial network\n",
      "Generative pre-trained transformer\n",
      "Large language model\n",
      "Neural network\n",
      "Prompt engineering\n",
      "Retrieval-augmented generation\n",
      "Reinforcement learning from human feedback\n",
      "Self-supervised learning\n",
      "Transformer\n",
      "Variational autoencoder\n",
      "Vision transformer\n",
      "Word embedding\n",
      "Models\n",
      "Text\n",
      "Claude\n",
      "DBRX\n",
      "DeepSeek\n",
      "Gemini\n",
      "GPT\n",
      "1\n",
      "2\n",
      "3\n",
      "J\n",
      "ChatGPT\n",
      "4\n",
      "4o\n",
      "o1\n",
      "o3\n",
      "Grok\n",
      "Granite\n",
      "Llama\n",
      "Mistral Large\n",
      "PanGu-Σ\n",
      "Qwen\n",
      "Image\n",
      "Aurora\n",
      "DALL-E\n",
      "Firefly\n",
      "Flux\n",
      "Ideogram\n",
      "Midjourney\n",
      "Stable Diffusion\n",
      "Video\n",
      "Dream Machine\n",
      "Gen-3 Alpha\n",
      "Hailuo AI\n",
      "Kling\n",
      "Sora\n",
      "Veo\n",
      "VideoPoet\n",
      "Music\n",
      "Udio\n",
      "Suno AI\n",
      "Companies\n",
      "List of artificial intelligence companies\n",
      "Category\n",
      "Commons\n",
      "Retrieved from \"\n",
      "https://en.wikipedia.org/w/index.php?title=DeepSeek&oldid=1273198562\n",
      "\"\n",
      "Categories\n",
      ":\n",
      "2023 establishments in China\n",
      "Artificial intelligence companies\n",
      "Artificial intelligence laboratories\n",
      "Companies based in Hangzhou\n",
      "Technology companies established in 2023\n",
      "Hidden categories:\n",
      "Articles with short description\n",
      "Short description matches Wikidata\n",
      "Use dmy dates from January 2025\n",
      "Articles containing Chinese-language text\n",
      "Articles containing simplified Chinese-language text\n",
      "Wikipedia articles that are too technical from January 2025\n",
      "All articles that are too technical\n",
      "Commons category link from Wikidata\n",
      "This page was last edited on 1 February 2025, at 06:51\n",
      "(UTC)\n",
      ".\n",
      "Text is available under the\n",
      "Creative Commons Attribution-ShareAlike 4.0 License\n",
      ";\n",
      "additional terms may apply. By using this site, you agree to the\n",
      "Terms of Use\n",
      "and\n",
      "Privacy Policy\n",
      ". Wikipedia® is a registered trademark of the\n",
      "Wikimedia Foundation, Inc.\n",
      ", a non-profit organization.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Code of Conduct\n",
      "Developers\n",
      "Statistics\n",
      "Cookie statement\n",
      "Mobile view\n",
      "Search\n",
      "Search\n",
      "Toggle the table of contents\n",
      "DeepSeek\n",
      "55 languages\n",
      "Add topic\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt_for(web))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages object :\n",
    "- It's just a dictionary which has role and content, system and system, message user and the user message. \n",
    "- It has many roles which we can play with as per the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an assistant that analyzes the content of a website     and provides a short summary, ignoring text that might be navigation related     or any other irrelevant advirtisements. Respond in markdown.'},\n",
       " {'role': 'user',\n",
       "  'content': 'You are looking at a website titled DeepSeek - Wikipedia\\n The contents of this website is as follows:         please provide a short summary of this website in markdown.         If it includes news or announcements, then summarize these too. \\n\\nJump to content\\nMain menu\\nMain menu\\nmove to sidebar\\nhide\\nNavigation\\nMain page\\nContents\\nCurrent events\\nRandom article\\nAbout Wikipedia\\nContact us\\nContribute\\nHelp\\nLearn to edit\\nCommunity portal\\nRecent changes\\nUpload file\\nSearch\\nSearch\\nAppearance\\nDonate\\nCreate account\\nLog in\\nPersonal tools\\nDonate\\nCreate account\\nLog in\\nPages for logged out editors\\nlearn more\\nContributions\\nTalk\\nContents\\nmove to sidebar\\nhide\\n(Top)\\n1\\nBackground\\n2\\nDevelopment and release history\\nToggle Development and release history subsection\\n2.1\\nDeepSeek LLM\\n2.2\\nV2\\n2.3\\nV3\\n2.4\\nR1\\n3\\nAssessment and reactions\\n4\\nConcerns\\nToggle Concerns subsection\\n4.1\\nCensorship\\n4.2\\nSecurity and privacy\\n5\\nSee also\\n6\\nNotes\\n7\\nReferences\\n8\\nExternal links\\nToggle the table of contents\\nDeepSeek\\n55 languages\\nAfrikaans\\nالعربية\\nAragonés\\nঅসমীয়া\\nAzərbaycanca\\nবাংলা\\nБългарски\\nCatalà\\nČeština\\nDansk\\nالدارجة\\nDeutsch\\nΕλληνικά\\nEspañol\\nEsperanto\\nEuskara\\nفارسی\\nFrançais\\nFrysk\\nFulfulde\\nGaeilge\\nGalego\\n한국어\\nBahasa Indonesia\\nItaliano\\nעברית\\nKiswahili\\nMagyar\\nМакедонски\\nമലയാളം\\nNederlands\\nनेपाली\\n日本語\\nOʻzbekcha / ўзбекча\\nPolski\\nPortuguês\\nQaraqalpaqsha\\nRomână\\nРусский\\nSimple English\\nСрпски / srpski\\nSuomi\\nSvenska\\nTagalog\\nதமிழ்\\nၽႃႇသႃႇတႆး\\nไทย\\nTürkçe\\nУкраїнська\\nاردو\\nئۇيغۇرچە / Uyghurche\\nTiếng Việt\\n吴语\\n粵語\\n中文\\nEdit links\\nArticle\\nTalk\\nEnglish\\nRead\\nEdit\\nView history\\nTools\\nTools\\nmove to sidebar\\nhide\\nActions\\nRead\\nEdit\\nView history\\nGeneral\\nWhat links here\\nRelated changes\\nUpload file\\nSpecial pages\\nPermanent link\\nPage information\\nCite this page\\nGet shortened URL\\nDownload QR code\\nPrint/export\\nDownload as PDF\\nPrintable version\\nIn other projects\\nWikimedia Commons\\nWikidata item\\nAppearance\\nmove to sidebar\\nhide\\nFrom Wikipedia, the free encyclopedia\\nChinese artificial intelligence company\\nHangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\\nNative name\\n杭州深度求索人工智能基础技术研究有限公司\\nCompany type\\nPrivate\\nIndustry\\nInformation technology\\nArtificial intelligence\\nFounded\\nMay\\xa016, 2023\\n; 20 months ago\\n(\\n2023-05-16\\n)\\nFounder\\nLiang Wenfeng\\nHeadquarters\\nHangzhou\\n,\\nZhejiang\\n, China\\nKey people\\nLiang Wenfeng (CEO)\\nOwner\\nHigh-Flyer\\nNumber of employees\\nUnder 200\\nWebsite\\nwww\\n.deepseek\\n.com\\nHangzhou DeepSeek Artificial Intelligence Basic Technology Research Co., Ltd.\\n, commonly referred to as\\nDeepSeek\\n, (\\nChinese\\n:\\n深度求索\\n;\\npinyin\\n:\\nShēndù Qiúsuǒ\\n) is a Chinese\\nartificial intelligence\\ncompany that develops\\nopen-source\\nlarge language models\\n(LLMs). Based in\\nHangzhou, Zhejiang\\n, it is owned and funded by Chinese hedge fund\\nHigh-Flyer\\n, whose co-founder,\\nLiang Wenfeng\\n, established the company in 2023 and serves as its\\nCEO\\n.\\nThe DeepSeek-R1 model provides responses comparable to other contemporary\\nlarge language models\\n, such as\\nOpenAI\\n\\'s\\nGPT-4o\\nand\\no1\\n.\\n[\\n1\\n]\\nIt is\\ntrained\\nat a significantly lower cost—stated at\\nUS$\\n6 million compared to $100 million for OpenAI\\'s\\nGPT-4\\nin 2023\\n[\\n2\\n]\\n—and requires a tenth of the computing power of a comparable LLM.\\n[\\n2\\n]\\n[\\n3\\n]\\n[\\n4\\n]\\nDeepSeek\\'s AI models were developed amid United States sanctions on China for\\nNvidia\\nchips,\\n[\\n5\\n]\\nwhich were intended to restrict the ability of the country to develop advanced AI systems.\\n[\\n6\\n]\\n[\\n7\\n]\\nOn 10 January 2025, DeepSeek released its first free\\nchatbot app\\n, based on the DeepSeek-R1 model, for\\niOS\\nand\\nAndroid\\n; by 27 January, DeepSeek-R1 had surpassed\\nChatGPT\\nas the most-downloaded free app on the\\niOS App Store\\nin the United States,\\n[\\n8\\n]\\ncausing Nvidia\\'s share price to drop by 18%.\\n[\\n9\\n]\\n[\\n10\\n]\\nDeepSeek\\'s success against larger and more established rivals has been described as \"upending AI\",\\n[\\n8\\n]\\nconstituting \"the first shot at what is emerging as a global AI space race\",\\n[\\n11\\n]\\nand ushering in \"a new era of AI\\nbrinkmanship\\n\".\\n[\\n12\\n]\\nDeepSeek makes its\\ngenerative artificial intelligence\\nalgorithms, models, and training details open-source, allowing its code to be freely available for use, modification, viewing, and designing documents for building purposes.\\n[\\n13\\n]\\nThe company reportedly vigorously recruits young AI researchers from top Chinese universities,\\n[\\n8\\n]\\nand hires from outside the\\ncomputer science\\nfield to diversify its models\\' knowledge and abilities.\\n[\\n3\\n]\\nBackground\\n[\\nedit\\n]\\nIn February 2016, High-Flyer was co-founded by AI enthusiast Liang Wenfeng, who had been trading since the\\n2007–2008 financial crisis\\nwhile attending\\nZhejiang University\\n.\\n[\\n14\\n]\\nBy 2019, he established High-Flyer as a hedge fund focused on developing and using AI trading algorithms. By 2021, High-Flyer exclusively used AI in trading\\n[\\n15\\n]\\n, often using Nvidia chips.\\n[\\n16\\n]\\nDeepSeek has made its\\ngenerative artificial intelligence\\nchatbot\\nopen source\\n, meaning its code is freely available for use, modification, and viewing. This includes permission to access and use the source code, as well as design documents, for building purposes.\\n[\\n13\\n]\\nIn 2021, while running High-Flyer, Liang began stockpiling Nvidia GPUs for an AI project.\\n[\\n16\\n]\\nAccording to\\n36Kr\\n, Liang had built up a store of 10,000\\nNvidia A100\\nGPUs, which are used to train AI,\\n[\\n17\\n]\\nbefore the United States federal government imposed AI chip restrictions on China.\\n[\\n15\\n]\\nIn April 2023, High-Flyer started an\\nartificial general intelligence\\nlab dedicated to research developing AI tools separate from High-Flyer\\'s financial business.\\n[\\n18\\n]\\n[\\n19\\n]\\nIn May 2023, with High-Flyer as one of the investors, the lab became its own company, DeepSeek.\\n[\\n15\\n]\\n[\\n20\\n]\\n[\\n19\\n]\\nVenture capital\\nfirms were reluctant in providing funding as it was unlikely that it would be able to generate an\\nexit\\nin a short period of time.\\n[\\n15\\n]\\nAfter releasing DeepSeek-V2 in May 2024, which offered strong performance for a low price, DeepSeek became known as the catalyst for China\\'s AI model\\nprice war\\n. It was quickly dubbed the \"\\nPinduoduo\\nof AI\", and other major tech giants such as\\nByteDance\\n,\\nTencent\\n,\\nBaidu\\n, and\\nAlibaba\\nbegan to cut the price of their AI models to compete with the company. Despite the low price charged by DeepSeek, it was profitable compared to its rivals that were losing money.\\n[\\n21\\n]\\nDeepSeek is focused on research and has no detailed plans for commercialization;\\n[\\n21\\n]\\nthis also allows its technology to avoid the most stringent provisions of China\\'s AI regulations, such as requiring consumer-facing technology to comply with the government’s controls on information.\\n[\\n3\\n]\\nDeepSeek\\'s hiring preferences target technical abilities rather than work experience, resulting in most new hires being either recent university graduates or developers whose AI careers are less established.\\n[\\n19\\n]\\n[\\n3\\n]\\nLikewise, the company recruits individuals without any computer science background to help its technology understand other topics and knowledge areas, including being able to generate poetry and perform well on the notoriously difficult\\nChinese college admissions exams (Gaokao)\\n.\\n[\\n3\\n]\\nDevelopment and release history\\n[\\nedit\\n]\\nThis section\\nmay be too technical for most readers to understand\\n.\\nPlease\\nhelp improve it\\nto\\nmake it understandable to non-experts\\n, without removing the technical details.\\n(\\nJanuary 2025\\n)\\n(\\nLearn how and when to remove this message\\n)\\nDeepSeek LLM\\n[\\nedit\\n]\\nOn 2 November 2023, DeepSeek released its first series of model,\\nDeepSeek-Coder\\n, which is available for free to both researchers and commercial users. The code for the model was made open-source under the\\nMIT license\\n, with an additional license agreement (\"DeepSeek license\") regarding \"open and responsible downstream usage\" for the model itself.\\n[\\n22\\n]\\nThey are of the same architecture as DeepSeek LLM detailed below. The series includes 8 models, 4 pretrained (\\nBase\\n) and 4 instruction-finetuned (\\nInstruct\\n). They all have 16K context lengths. The\\ntraining\\nwas as follows:\\n[\\n23\\n]\\n[\\n24\\n]\\n[\\n25\\n]\\nPretraining: 1.8T tokens (87% source code, 10% code-related English (GitHub markdown and\\nStack Exchange\\n), and 3% code-unrelated Chinese).\\nLong-context pretraining: 200B tokens. This extends the context length from 4K to 16K. This produced the\\nBase\\nmodels.\\nSupervised\\nfinetuning\\n(SFT): 2B tokens of instruction data. This produced the\\nInstruct\\nmodels.\\nThey were trained on clusters of A100 and\\nH800\\nNvidia GPUs, connected by\\nInfiniBand\\n,\\nNVLink\\n,\\nNVSwitch\\n.\\n[\\n23\\n]\\nDeepSeek Coder properties\\n[\\n23\\n]\\n:\\u200aTable 2\\n[\\n26\\n]\\nParams\\n.\\nn\\nlayers\\n{\\\\displaystyle n_{\\\\text{layers}}}\\nd\\nmodel\\n{\\\\displaystyle d_{\\\\text{model}}}\\nd\\nintermediate\\n{\\\\displaystyle d_{\\\\text{intermediate}}}\\nn\\nheads\\n{\\\\displaystyle n_{\\\\text{heads}}}\\nn\\nkv-heads\\n{\\\\displaystyle n_{\\\\text{kv-heads}}}\\n1.3B\\n24\\n2048\\n5504\\n16\\n16\\n5.7B\\n32\\n4096\\n11008\\n32\\n1\\n[\\nnote 1\\n]\\n6.7B\\n32\\n4096\\n11008\\n32\\n32\\n33B\\n62\\n7168\\n19200\\n56\\n7\\n[\\nnote 1\\n]\\nOn 29 November 2023, DeepSeek released the\\nDeepSeek-LLM\\nseries of models, with 7B and 67B parameters in both\\nBase\\nand\\nChat\\nforms (no\\nInstruct\\nwas released). It was developed to compete with other LLMs available at the time. The paper claimed benchmark results higher than most open source LLMs at the time, especially Llama 2.\\n[\\n27\\n]\\n:\\u200asection 5\\nLike DeepSeek Coder, the code for the model was under MIT license, with DeepSeek license for the model itself.\\n[\\n28\\n]\\nThe architecture was essentially the same as those of the\\nLlama\\nseries. They used the\\npre-norm\\ndecoder-only Transformer\\nwith\\nRMSNorm\\nas the normalization,\\nSwiGLU\\nin the feedforward layers,\\nrotary positional embedding\\n(RoPE), and\\ngrouped-query attention\\n(GQA). Both had vocabulary size 102,400 (\\nbyte-level BPE\\n) and context length of 4096. They trained on 2 trillion tokens of English and Chinese text obtained by deduplicating the\\nCommon Crawl\\n.\\n[\\n27\\n]\\nDeepSeek LLM properties\\n[\\n27\\n]\\n:\\u200aTable 2\\nParams\\n.\\nn\\nlayers\\n{\\\\displaystyle n_{\\\\text{layers}}}\\nd\\nmodel\\n{\\\\displaystyle d_{\\\\text{model}}}\\nd\\nintermediate\\n{\\\\displaystyle d_{\\\\text{intermediate}}}\\nn\\nheads\\n{\\\\displaystyle n_{\\\\text{heads}}}\\nn\\nkv-heads\\n{\\\\displaystyle n_{\\\\text{kv-heads}}}\\n7B\\n30\\n4096\\n11008\\n32\\n32\\n67B\\n95\\n8192\\n22016\\n64\\n8\\n[\\nnote 1\\n]\\nThe\\nChat\\nversions of the two\\nBase\\nmodels was also released concurrently, obtained by training\\nBase\\nby\\nsupervised finetuning (SFT) followed by direct policy optimization (DPO)\\n.\\n[\\n27\\n]\\nOn 9 January 2024, they released 2\\nDeepSeek-MoE\\nmodels (\\nBase\\n,\\nChat\\n), each of 16B parameters (2.7B activated per token, 4K context length). The training was essentially the same as\\nDeepSeek-LLM 7B\\n, and was trained on a part of its training dataset. They claimed comparable performance with a 16B MoE as a 7B non-MoE. In architecture, it is a variant of the standard\\nsparsely-gated MoE\\n, with \"shared experts\" that are always queried, and \"routed experts\" that might not be. They found this to help with expert balancing. In standard MoE, some experts can become overly relied on, while other experts might be rarely used, wasting parameters. Attempting to balance the experts so that they are equally used then causes experts to replicate the same capacity. They proposed the shared experts to learn core capacities that are often used, and let the routed experts to learn the peripheral capacities that are rarely used.\\n[\\n29\\n]\\nIn April 2024, they released 3\\nDeepSeek-Math\\nmodels specialized for doing math:\\nBase\\n,\\nInstruct\\n,\\nRL\\n. It was trained as follows:\\n[\\n30\\n]\\nInitialize with a previously pretrained\\nDeepSeek-Coder-Base-v1.5 7B\\n.\\nFurther pretrain with 500B tokens (6% DeepSeekMath Corpus, 4% AlgebraicStack, 10% arXiv, 20% GitHub code, 10% Common Crawl). This produced the\\nBase\\nmodel.\\nTrain an instruction-following model by SFT\\nBase\\nwith 776K math problems and their tool-use-integrated step-by-step solutions. This produced the\\nInstruct\\nmodel.\\nReinforcement learning\\n(RL): The reward model was a\\nprocess reward model\\n(PRM) trained from\\nBase\\naccording to the Math-Shepherd method.\\n[\\n31\\n]\\nThis reward model was then used to train\\nInstruct\\nusing\\ngroup relative policy optimization\\n(GRPO) on a dataset of 144K math questions \"related to GSM8K and MATH\". The reward model was continuously updated during training to avoid reward hacking. This resulted in the\\nRL\\nmodel.\\nV2\\n[\\nedit\\n]\\nIn May 2024, they released the\\nDeepSeek-V2\\nseries. The series includes 4 models, 2 base models (\\nDeepSeek-V2\\n,\\nDeepSeek-V2-Lite\\n) and 2 chatbots (\\n-Chat\\n). The two larger models were trained as follows:\\n[\\n32\\n]\\nPretrain on a dataset of 8.1T tokens, where Chinese tokens are 12% more than English ones.\\nExtend context length from 4K to 128K using YaRN.\\n[\\n33\\n]\\nThis resulted in\\nDeepSeek-V2\\n.\\nSFT with 1.2M instances for helpfulness and 0.3M for safety. This resulted in\\nDeepSeek-V2-Chat (SFT)\\nwhich was not released.\\nRL using GRPO in two stages. The first stage was trained to solve math and coding problems. This stage used 1 reward model, trained on compiler feedback (for coding) and ground-truth labels (for math). The second stage was trained to be helpful, safe, and follow rules. This stage used 3 reward models. The helpfulness and safety reward models were trained on human preference data. The rule-based reward model was manually programmed. All trained reward models were initialized from\\nDeepSeek-V2-Chat (SFT)\\n. This resulted in the released version of\\nDeepSeek-V2-Chat\\n.\\nThey opted for 2-staged RL, because they found that RL on reasoning data had \"unique characteristics\" different from RL on general data. For example, RL on reasoning could improve over more training steps.\\n[\\n32\\n]\\nThe two\\nV2-Lite\\nmodels were smaller, and trained similarly, though\\nDeepSeek-V2-Lite-Chat\\nonly underwent SFT, not RL. They trained the Lite version to help \"further research and development on MLA and DeepSeekMoE\".\\n[\\n32\\n]\\nArchitecturally, the V2 models were significantly modified from the DeepSeek LLM series. They changed the standard attention mechanism by a\\nlow-rank approximation\\ncalled\\nmulti-head latent attention\\n(MLA), and used the\\nmixture of experts\\n(MoE) variant previously published in January.\\n[\\n29\\n]\\nDeepSeek V2 properties\\n[\\n32\\n]\\n:\\u200aSection 3.1.2, Appendix B\\n[\\n34\\n]\\n[\\n35\\n]\\nName\\nParams\\n.\\nActive\\nparams\\nn\\nlayers\\n{\\\\displaystyle n_{\\\\text{layers}}}\\nContext length\\nn\\nshared experts\\n{\\\\displaystyle n_{\\\\text{shared experts}}}\\nn\\nrouted experts\\n{\\\\displaystyle n_{\\\\text{routed experts}}}\\nV2-Lite\\n15.7B\\n2.4B\\n27\\n32K\\n2\\n64\\nV2\\n236B\\n21B\\n60\\n128K\\n2\\n160\\nThe\\nFinancial Times\\nreported that it was cheaper than its peers with a price of 2\\nRMB\\nfor every million output tokens. The\\nUniversity of Waterloo\\nTiger Lab\\'s leaderboard ranked DeepSeek-V2 seventh on its LLM ranking.\\n[\\n20\\n]\\nIn June 2024, they released 4 models in the\\nDeepSeek-Coder-V2\\nseries:\\nV2-Base\\n,\\nV2-Lite-Base\\n,\\nV2-Instruct\\n,\\nV2-Lite-Instruct\\n. They were trained as follows:\\n[\\n36\\n]\\n[\\nnote 2\\n]\\nThe\\nBase\\nmodels were initialized from corresponding\\nintermediate\\ncheckpoints after pretraining on 4.2T tokens (not the version at the end of pretraining), then pretrained further for 6T tokens, then context-extended to 128K context length. This produced the\\nBase\\nmodels.\\nDeepSeek-Coder\\nand\\nDeepSeek-Math\\nwere used to generate 20K code-related and 30K math-related instruction data, then combined with an instruction dataset of 300M tokens. This was used for SFT.\\nRL with GRPO. The reward for math problems was computed by comparing with the ground-truth label. The reward for code problems was generated by a reward model trained to predict whether a program would pass the unit tests.\\nDeepSeek-V2.5\\nwas released in September and updated in December 2024. It was made by combining\\nDeepSeek-V2-Chat\\nand\\nDeepSeek-Coder-V2-Instruct\\n.\\n[\\n37\\n]\\nV3\\n[\\nedit\\n]\\nIn December 2024, they released a base model\\nDeepSeek-V3-Base\\nand a chat model\\nDeepSeek-V3\\n. The model architecture is essentially the same as V2. They were trained as follows:\\n[\\n38\\n]\\nPretraining on 14.8T tokens of a multilingual corpus, mostly English and Chinese. It contained a higher ratio of math and programming than the pretraining dataset of V2.\\nExtend context length twice, from 4K to 32K and then to 128K, using YaRN.\\n[\\n33\\n]\\nThis produced\\nDeepSeek-V\\n3-Base\\n.\\nSFT for 2 epochs on 1.5M samples of reasoning (math, programming, logic) and non-reasoning (creative writing, roleplay, simple question answering) data. Reasoning data was generated by \"expert models\". Non-reasoning data was generated by\\nDeepSeek-V2.5\\nand checked by humans.\\nThe \"expert models\" were trained by starting with an unspecified base model, then SFT on both\\n<problem, original response>\\ndata, and synthetic\\n<system prompt, problem, R1 response>\\ndata generated by an internal\\nDeepSeek-R1\\nmodel. The system prompt asked the\\nR1\\nto reflect and verify during thinking. Then the expert models were RL using an unspecified reward function.\\nEach expert model was trained to generate just synthetic reasoning data in one specific domain (math, programming, logic).\\nExpert models were used, instead of\\nR1\\nitself, since the output from\\nR1\\nitself suffered \"overthinking, poor formatting, and excessive length\".\\nModel-based reward models were made by starting with a SFT checkpoint of\\nV3\\n, then finetuning on human preference data containing both final reward and chain-of-thought leading to the final reward. The reward model produced reward signals for both questions with objective but free-form answers, and questions without objective answers (such as creative writing).\\nA SFT checkpoint of\\nV3\\nwas trained by GRPO using both reward models and rule-based reward. The rule-based reward was computed for math problems with a final answer (put in a box), and for programming problems by unit tests. This produced\\nDeepSeek-V3\\n.\\nDeepSeek V3 properties\\n[\\n38\\n]\\n:\\u200aSection 4.2\\n[\\n39\\n]\\nName\\nParams\\n.\\nActive\\nparams\\nn\\nlayers\\n{\\\\displaystyle n_{\\\\text{layers}}}\\nContext length\\nn\\nshared experts\\n{\\\\displaystyle n_{\\\\text{shared experts}}}\\nn\\nrouted experts\\n{\\\\displaystyle n_{\\\\text{routed experts}}}\\nV3\\n671B\\n37B\\n61\\n128K\\n1\\n256\\nThe DeepSeek team performed extensive low-level engineering to achieve efficiency. They used\\nmixed-precision arithmetic\\n. Much of the forward pass was performed in\\n8-bit floating point numbers\\n(5E2M: 5-bit exponent and 2-bit\\nmantissa\\n) rather than the standard\\n32-bit\\n, requiring special\\nGEMM\\nroutines to accumulate accurately. They used a custom 12-bit float (E5M6) for\\nonly\\nthe inputs to the linear layers after the attention modules. Optimizer states were in 16-bit (\\nBF16\\n). They minimized the communication latency by overlapping extensively computation and communication, such as dedicating 20\\nstreaming multiprocessors\\nout of 132 per H800 for only inter-GPU communication. They lowered communication by rearranging (every 10 minutes) the exact machine each expert was on in order to avoid certain machines being queried more often than the others, adding auxiliary load-balancing losses to the training loss function, and other load-balancing techniques.\\n[\\n38\\n]\\nAfter training, it was deployed on H800 clusters. The H800 cards within a cluster are connected by NVLink, and the clusters are connected by InfiniBand.\\n[\\n38\\n]\\nTotal cost of training the DeepSeek-V3 model\\n[\\n38\\n]\\n:\\u200aTable 1\\nStage\\nCost (in one thousand GPU hours)\\nCost (in one million USD$)\\nPre-training\\n2,664\\n5.328\\nContext extension\\n119\\n0.24\\nFine-tuning\\n5\\n0.01\\nTotal\\n2,788\\n5.576\\nBenchmark tests show that DeepSeek-V3 outperformed\\nLlama\\n3.1 and\\nQwen\\n2.5 whilst matching\\nGPT-4o\\nand\\nClaude\\n3.5 Sonnet.\\n[\\n19\\n]\\n[\\n40\\n]\\n[\\n41\\n]\\n[\\n42\\n]\\nR1\\n[\\nedit\\n]\\nOn 20 November 2024,\\nDeepSeek-R1-Lite-Preview\\nbecame accessible via DeepSeek\\'s API, as well as via a chat interface after logging in.\\n[\\n43\\n]\\n[\\n44\\n]\\n[\\nnote 3\\n]\\nIt was trained for logical inference, mathematical reasoning, and real-time problem-solving. DeepSeek claimed that it exceeded performance of\\nOpenAI o1\\non benchmarks such as\\nAmerican Invitational Mathematics Examination\\n(AIME) and MATH.\\n[\\n45\\n]\\nHowever,\\nThe Wall Street Journal\\nstated when it used 15 problems from the 2024 edition of AIME, the o1 model reached a solution faster than\\nDeepSeek-R1-Lite-Preview\\n.\\n[\\n46\\n]\\nOn 20 January 2025, DeepSeek released\\nDeepSeek-R1\\nand\\nDeepSeek-R1-Zero\\n.\\n[\\n47\\n]\\nBoth were initialized from\\nDeepSeek-V3-Base\\n, and share its architecture. The company also released some \"\\nDeepSeek-R1-Distill\\n\" models, which are\\nnot\\ninitialized on\\nV3-Base\\n, but instead are initialized from other pretrained open-weight models, including\\nLLaMA\\nand\\nQwen\\n, then fine-tuned on\\nsynthetic data\\ngenerated by\\nR1\\n.\\n[\\n48\\n]\\nTemplate for\\nDeepSeek-R1-Zero\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: <prompt>. Assistant:\\n– <prompt> is replaced with the specific reasoning question during training.\\nDeepSeek-R1-Zero\\nwas trained exclusively using GRPO RL without SFT. Unlike previous versions, they used no model-based reward. All reward functions were rule-based, \"mainly\" of two types (other types were not specified): accuracy rewards and format rewards. Accuracy reward was checking whether a boxed answer is correct (for math) or whether a code passes tests (for programming). Format reward was checking whether the model puts its thinking trace within\\n<think>...</think>\\n.\\n[\\n48\\n]\\nAs\\nR1-Zero\\nhas issues with readability and mixing languages,\\nR1\\nwas trained to address these issues and further improve reasoning:\\n[\\n48\\n]\\nSFT\\nDeepSeek-V3-Base\\non \"thousands\" of \"cold-start\" data all with the standard format of\\n|special_token|<reasoning_process>|special_token|summary>\\n.\\nApply the same RL process as\\nR1-Zero\\n, but also with a \"language consistency reward\" to encourage it to respond monolingually. This produced an internal model not released.\\nSynthesize 600K reasoning data from the internal model, with rejection sampling (i.e. if the generated reasoning had a wrong final answer, then it is removed). Synthesize 200K non-reasoning data (writing, factual QA, self-cognition, translation) using\\nDeepSeek-V3\\n.\\nSFT\\nDeepSeek-V3-Base\\non the 800K synthetic data for 2 epochs.\\nGRPO RL with rule-based reward (for reasoning tasks) and model-based reward (for non-reasoning tasks, helpfulness, and harmlessness). This produced\\nDeepSeek-R1\\n.\\nDistilled models were trained by SFT on 800K data synthesized from\\nDeepSeek-R1\\n, in a similar way as step 3 above. They were not trained with RL.\\n[\\n48\\n]\\nAssessment and reactions\\n[\\nedit\\n]\\nDeepSeek released its\\nAI Assistant\\n, which uses the V3 model as a\\nchatbot app\\nfor\\nApple IOS\\nand\\nAndroid\\n. By January 27, 2025, the app had surpassed ChatGPT as the highest-rated free app on the iOS App Store in the United States. Its chatbot reportedly answers questions, solves logic problems, and writes computer programs on par with other chatbots on the market, according to benchmark tests used by American AI companies.\\n[\\n3\\n]\\nDeepSeek-V3 uses significantly fewer resources compared to its peers; for example, whereas the world\\'s leading AI companies train their chatbots with\\nsupercomputers\\nusing as many as 16,000\\ngraphics processing units (GPUs)\\n, if not more, DeepSeek claims to have needed only about 2,000 GPUs, namely the H800 series chip from\\nNvidia\\n.\\n[\\n38\\n]\\nIt was trained in around 55 days at a cost of US$5.58\\xa0million,\\n[\\n38\\n]\\nwhich is roughly one tenth of what United States tech giant\\nMeta\\nspent building its latest AI technology.\\n[\\n3\\n]\\nDeepSeek\\'s competitive performance at relatively minimal cost has been recognized as potentially challenging the global dominance of American AI models.\\n[\\n49\\n]\\nVarious publications and news media, such as\\nThe Hill\\nand\\nThe Guardian\\n,\\ndescribed the release of its chatbot as a \"\\nSputnik moment\\n\" for American AI.\\n[\\n50\\n]\\n[\\n51\\n]\\nThe performance of its\\nR1\\nmodel was reportedly \"on par with\" one of OpenAI\\'s latest models when used for tasks such as mathematics, coding, and natural language reasoning;\\n[\\n52\\n]\\nechoing other commentators, American Silicon Valley venture capitalist\\nMarc Andreessen\\nlikewise described\\nR1\\nas \"AI\\'s Sputnik moment\".\\n[\\n52\\n]\\nDeepSeek\\'s founder, Liang Wenfeng has been compared to Open AI CEO\\nSam Altman\\n, with\\nCNN\\ncalling him the Sam Altman of China and an evangelist for AI.\\n[\\n53\\n]\\nChinese\\nstate media\\nwidely praised DeepSeek as a national asset.\\n[\\n54\\n]\\n[\\n55\\n]\\nOn 20 January 2025, China\\'s Premier\\nLi Qiang\\ninvited Liang Wenfeng to his symposium with experts and asked him to provide opinions and suggestions on a draft for comments of the annual 2024 government work report.\\n[\\n56\\n]\\nDeepSeek\\'s optimization of limited resources has highlighted potential limits of United States sanctions on China\\'s AI development, which include export restrictions on advanced AI chips to China.\\n[\\n19\\n]\\n[\\n57\\n]\\nThe success of the company\\'s AI models consequently \"sparked market turmoil\"\\n[\\n58\\n]\\nand caused shares in major global technology companies to plunge on 27 January 2025: Nvidia\\'s stock fell by as much as 17–18%,\\n[\\n59\\n]\\nas did the stock of rival\\nBroadcom\\n. Other tech firms also sank, including\\nMicrosoft\\n(down 2.5%),\\nGoogle\\n\\'s owner\\nAlphabet\\n(down over 4%), and Dutch chip equipment maker\\nASML\\n(down over 7%).\\n[\\n52\\n]\\nA global selloff of technology stocks on\\nNasdaq\\n, prompted by the release of the\\nR1\\nmodel, had led to record losses of about $593 billion in the market capitalizations of AI and computer hardware companies;\\n[\\n60\\n]\\nby 28 January 2025, a total of $1 trillion of value was wiped off American stocks.\\n[\\n51\\n]\\nThe login error DeepSeek gave on 28 Jan 2025 following a cyberattack\\nLeading figures in the American AI sector had mixed reactions to DeepSeek\\'s success and performance.\\n[\\n61\\n]\\nMicrosoft CEO\\nSatya Nadella\\nand OpenAI CEO Sam Altman—whose companies are involved in the United States government-backed \"\\nStargate Project\\n\" to develop American AI infrastructure—both called DeepSeek \"super impressive\".\\n[\\n62\\n]\\n[\\n63\\n]\\nAmerican President Donald Trump, who announced The Stargate Project, called DeepSeek a wake-up call\\n[\\n64\\n]\\nand a positive development.\\n[\\n65\\n]\\n[\\n51\\n]\\n[\\n52\\n]\\n[\\n66\\n]\\nOther leaders in the field, including\\nScale AI\\nCEO Alexandr Wang,\\nAnthropic\\ncofounder and CEO\\nDario Amodei\\n, and\\nElon Musk\\nexpressed skepticism of the app\\'s performance or of the sustainability of its success.\\n[\\n61\\n]\\n[\\n67\\n]\\n[\\n68\\n]\\nVarious companies, including\\nAmazon Web Services\\n,\\nToyota\\n, and\\nStripe\\n, are seeking to use the model in their program.\\n[\\n69\\n]\\nOn 27 January 2025, DeepSeek limited its new user registration to phone numbers from mainland China, email addresses, or Google account logins, following a \"large-scale\"\\ncyberattack\\ndisrupted the proper functioning of its servers.\\n[\\n70\\n]\\n[\\n71\\n]\\nConcerns\\n[\\nedit\\n]\\nCensorship\\n[\\nedit\\n]\\nSee also:\\nChinese censorship abroad\\nand\\nCensorship in China\\nDeepSeek responses when asked about\\nXi Jinping\\nand\\nNarendra Modi\\nSome sources have observed that the official application programming interface (API) version of R1, which runs from servers located in China, uses\\ncensorship\\nmechanisms for topics that are considered politically sensitive for the\\ngovernment of China\\n. For example, the model refuses to answer questions about the\\n1989 Tiananmen Square massacre\\n,\\npersecution of Uyghurs\\n,\\ncomparisons between Xi Jinping and Winnie the Pooh\\n, and\\nhuman rights in China\\n.\\n[\\n72\\n]\\n[\\n73\\n]\\n[\\n74\\n]\\nThe AI may initially generate an answer, but then deletes it shortly afterwards and replaces it with a message such as: \"Sorry, that\\'s beyond my current scope. Let\\'s talk about something else.\"\\n[\\n73\\n]\\nThe integrated censorship mechanisms and restrictions can only be removed to a limited extent in the open-source version of the R1 model. If the \"\\ncore socialist values\\n\" defined by the\\nChinese Internet regulatory authorities\\nare touched upon, or the\\npolitical status of Taiwan\\nis raised, discussions are terminated.\\n[\\n75\\n]\\nWhen tested by\\nNBC News\\n, DeepSeek\\'s R1 described Taiwan as \"an inalienable part of China\\'s territory,\" and stated: \"We firmly oppose any form of \\'\\nTaiwan independence\\n\\' separatist activities and are committed to achieving the complete reunification of the motherland through peaceful means.\"\\n[\\n76\\n]\\nIn January 2025, Western researchers were able to trick DeepSeek into giving certain answers to some of these topics by requesting in its answer to swap certain letters for\\nsimilar-looking numbers\\n.\\n[\\n74\\n]\\nSecurity and privacy\\n[\\nedit\\n]\\nSee also:\\nChinese information operations and information warfare\\nSome experts fear that the government of China could use the AI system for foreign\\ninfluence operations\\n, spreading\\ndisinformation\\n,\\nsurveillance\\nand the development of\\ncyberweapons\\n.\\n[\\n77\\n]\\n[\\n78\\n]\\n[\\n79\\n]\\nDeepSeek\\'s privacy terms and conditions say \"We store the information we collect in secure servers located in the People\\'s Republic of China... We may collect your text or audio input, prompt, uploaded files, feedback, chat history, or other content that you provide to our model and Services\". According to a review by\\nWired\\n, DeepSeek also sends data to\\nBaidu\\n\\'s web analytics service and collects data from\\nByteDance\\n.\\n[\\n80\\n]\\nIn response, the Italian data protection authority is seeking additional information on DeepSeek\\'s collection and use of personal data, and the\\nUnited States National Security Council\\nannounced that it had started a national security review.\\n[\\n81\\n]\\n[\\n82\\n]\\nTaiwan\\'s government banned the use of DeepSeek at government ministries on security grounds and South Korea\\'s\\nPersonal Information Protection Commission\\nopened an inquiry into DeepSeek\\'s use of personal information.\\n[\\n83\\n]\\nSee also\\n[\\nedit\\n]\\nArtificial intelligence industry in China\\nNotes\\n[\\nedit\\n]\\n^\\na\\nb\\nc\\nThe number of heads does not equal the number of KV heads, due to GQA.\\n^\\nInexplicably, the model named\\nDeepSeek-Coder-V2 Chat\\nin the paper was released as\\nDeepSeek-Coder-V2-Instruct\\nin HuggingFace.\\n^\\nAt that time, the\\nR1-Lite-Preview\\nrequired selecting \"Deep Think enabled\", and every user could use it only 50 times a day.\\nReferences\\n[\\nedit\\n]\\n^\\nGibney, Elizabeth (23 January 2025).\\n\"China\\'s cheap, open AI model DeepSeek thrills scientists\"\\n.\\nNature\\n.\\ndoi\\n:\\n10.1038/d41586-025-00229-6\\n.\\nISSN\\n1476-4687\\n.\\nPMID\\n39849139\\n.\\n^\\na\\nb\\nVincent, James (28 January 2025).\\n\"The DeepSeek panic reveals an AI world ready to blow\"\\n.\\nThe Guardian\\n.\\n^\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nMetz, Cade; Tobin, Meaghan (23 January 2025).\\n\"How Chinese A.I. Start-Up DeepSeek Is Competing With Silicon Valley Giants\"\\n.\\nThe New York Times\\n.\\nISSN\\n0362-4331\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nCosgrove, Emma (27 January 2025).\\n\"DeepSeek\\'s cheaper models and weaker chips call into question trillions in AI infrastructure spending\"\\n.\\nBusiness Insider\\n.\\n^\\nMallick, Subhrojit (16 January 2024).\\n\"Biden admin\\'s cap on GPU exports may hit India\\'s AI ambitions\"\\n.\\nThe Economic Times\\n. Retrieved\\n29 January\\n2025\\n.\\n^\\nSaran, Cliff (10 December 2024).\\n\"Nvidia investigation signals widening of US and China chip war | Computer Weekly\"\\n.\\nComputer Weekly\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nSherman, Natalie (9 December 2024).\\n\"Nvidia targeted by China in new chip war probe\"\\n.\\nBBC\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\na\\nb\\nc\\nMetz, Cade (27 January 2025).\\n\"What is DeepSeek? And How Is It Upending A.I.?\"\\n.\\nThe New York Times\\n.\\nISSN\\n0362-4331\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nField, Hayden (27 January 2025).\\n\"China\\'s DeepSeek AI dethrones ChatGPT on App Store: Here\\'s what you should know\"\\n.\\nCNBC\\n.\\n^\\nPicchi, Aimee (27 January 2025).\\n\"What is DeepSeek, and why is it causing Nvidia and other stocks to slump?\"\\n.\\nCBS News\\n.\\n^\\nZahn, Max (27 January 2025).\\n\"Nvidia, Microsoft shares tumble as China-based AI app DeepSeek hammers tech giants\"\\n.\\nABC News\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nRoose, Kevin (28 January 2025).\\n\"Why DeepSeek Could Change What Silicon Valley Believe About A.I.\"\\nThe New York Times\\n.\\nISSN\\n0362-4331\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\na\\nb\\nRomero, Luis E. (28 January 2025).\\n\"ChatGPT, DeepSeek, Or Llama? Meta\\'s LeCun Says Open-Source Is The Key\"\\n.\\nForbes\\n.\\n^\\nChen, Caiwei (24 January 2025).\\n\"How a top Chinese AI model overcame US sanctions\"\\n.\\nMIT Technology Review\\n.\\nArchived\\nfrom the original on 25 January 2025\\n. Retrieved\\n25 January\\n2025\\n.\\n^\\na\\nb\\nc\\nd\\nOttinger, Lily (9 December 2024).\\n\"Deepseek: From Hedge Fund to Frontier Model Maker\"\\n.\\nChinaTalk\\n.\\nArchived\\nfrom the original on 28 December 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\na\\nb\\nOlcott, Eleanor; Wu, Zijing (24 January 2025).\\n\"How small Chinese AI start-up DeepSeek shocked Silicon Valley\"\\n.\\nFinancial Times\\n. Retrieved\\n31 January\\n2025\\n.\\n^\\nLeswing, Kif (23 February 2023).\\n\"Meet the $10,000 Nvidia chip powering the race for A.I.\"\\nCNBC\\n. Retrieved\\n30 January\\n2025\\n.\\n^\\nYu, Xu (17 April 2023).\\n\"[Exclusive] Chinese Quant Hedge Fund High-Flyer Won\\'t Use AGI to Trade Stocks, MD Says\"\\n.\\nYicai Global\\n.\\nArchived\\nfrom the original on 31 December 2023\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\na\\nb\\nc\\nd\\ne\\nJiang, Ben; Perezi, Bien (1 January 2025).\\n\"Meet DeepSeek: the Chinese start-up that is changing how AI models are trained\"\\n.\\nSouth China Morning Post\\n.\\nArchived\\nfrom the original on 22 January 2025\\n. Retrieved\\n1 January\\n2025\\n.\\n^\\na\\nb\\nMcMorrow, Ryan; Olcott, Eleanor (9 June 2024).\\n\"The Chinese quant fund-turned-AI pioneer\"\\n.\\nFinancial Times\\n.\\nArchived\\nfrom the original on 17 July 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\na\\nb\\nSchneider, Jordan (27 November 2024).\\n\"Deepseek: The Quiet Giant Leading China\\'s AI Race\"\\n.\\nChinaTalk\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\n\"DeepSeek-Coder/LICENSE-MODEL at main · deepseek-ai/DeepSeek-Coder\"\\n.\\nGitHub\\n.\\nArchived\\nfrom the original on 22 January 2025\\n. Retrieved\\n24 January\\n2025\\n.\\n^\\na\\nb\\nc\\nGuo, Daya; Zhu, Qihao; Yang, Dejian; Xie, Zhenda; Dong, Kai; Zhang, Wentao; Chen, Guanting; Bi, Xiao; Wu, Y. (26 January 2024),\\nDeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence\\n,\\narXiv\\n:\\n2401.14196\\n^\\n\"DeepSeek Coder\"\\n.\\ndeepseekcoder.github.io\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\ndeepseek-ai/DeepSeek-Coder\\n, DeepSeek, 27 January 2025\\n, retrieved\\n27 January\\n2025\\n^\\n\"deepseek-ai/deepseek-coder-5.7bmqa-base · Hugging Face\"\\n.\\nHugging Face\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\na\\nb\\nc\\nd\\nDeepSeek-AI; Bi, Xiao; Chen, Deli; Chen, Guanting; Chen, Shanhuang; Dai, Damai; Deng, Chengqi; Ding, Honghui; Dong, Kai (5 January 2024),\\nDeepSeek LLM: Scaling Open-Source Language Models with Longtermism\\n,\\narXiv\\n:\\n2401.02954\\n^\\ndeepseek-ai/DeepSeek-LLM\\n, DeepSeek, 27 January 2025\\n, retrieved\\n27 January\\n2025\\n^\\na\\nb\\nDai, Damai; Deng, Chengqi; Zhao, Chenggang; Xu, R. X.; Gao, Huazuo; Chen, Deli; Li, Jiashi; Zeng, Wangding; Yu, Xingkai (11 January 2024),\\nDeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\\n,\\narXiv\\n:\\n2401.06066\\n^\\nShao, Zhihong; Wang, Peiyi; Zhu, Qihao; Xu, Runxin; Song, Junxiao; Bi, Xiao; Zhang, Haowei; Zhang, Mingchuan; Li, Y. K. (27 April 2024),\\nDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\\n,\\narXiv\\n:\\n2402.03300\\n.\\n^\\nWang, Peiyi; Li, Lei; Shao, Zhihong; Xu, R. X.; Dai, Damai; Li, Yifei; Chen, Deli; Wu, Y.; Sui, Zhifang (19 February 2024),\\nMath-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations\\n,\\narXiv\\n:\\n2312.08935\\n.\\n^\\na\\nb\\nc\\nd\\nDeepSeek-AI; Liu, Aixin; Feng, Bei; Wang, Bin; Wang, Bingxuan; Liu, Bo; Zhao, Chenggang; Dengr, Chengqi; Ruan, Chong (19 June 2024),\\nDeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\\n,\\narXiv\\n:\\n2405.04434\\n.\\n^\\na\\nb\\nPeng, Bowen; Quesnelle, Jeffrey; Fan, Honglu; Shippole, Enrico (1 November 2023),\\nYaRN: Efficient Context Window Extension of Large Language Models\\n,\\narXiv\\n:\\n2309.00071\\n.\\n^\\n\"config.json · deepseek-ai/DeepSeek-V2-Lite at main\"\\n.\\nHugging Face\\n. 15 May 2024\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\n\"config.json · deepseek-ai/DeepSeek-V2 at main\"\\n.\\nHugging Face\\n. 6 May 2024\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nDeepSeek-AI; Zhu, Qihao; Guo, Daya; Shao, Zhihong; Yang, Dejian; Wang, Peiyi; Xu, Runxin; Wu, Y.; Li, Yukun (17 June 2024),\\nDeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence\\n,\\narXiv\\n:\\n2406.11931\\n^\\n\"deepseek-ai/DeepSeek-V2.5 · Hugging Face\"\\n.\\nHugging Face\\n. 3 January 2025\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nDeepSeek-AI; Liu, Aixin; Feng, Bei; Xue, Bing; Wang, Bingxuan; Wu, Bochao; Lu, Chengda; Zhao, Chenggang; Deng, Chengqi (27 December 2024),\\nDeepSeek-V3 Technical Report\\n,\\narXiv\\n:\\n2412.19437\\n^\\n\"config.json · deepseek-ai/DeepSeek-V3 at main\"\\n.\\nHugging Face\\n. 26 December 2024\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nJiang, Ben (27 December 2024).\\n\"Chinese start-up DeepSeek\\'s new AI model outperforms Meta, OpenAI products\"\\n.\\nSouth China Morning Post\\n.\\nArchived\\nfrom the original on 27 December 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\nSharma, Shubham (26 December 2024).\\n\"DeepSeek-V3, ultra-large open-source AI, outperforms Llama and Qwen on launch\"\\n.\\nVentureBeat\\n.\\nArchived\\nfrom the original on 27 December 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\nWiggers, Kyle (26 December 2024).\\n\"DeepSeek\\'s new AI model appears to be one of the best \\'open\\' challengers yet\"\\n.\\nTechCrunch\\n.\\nArchived\\nfrom the original on 2 January 2025\\n. Retrieved\\n31 December\\n2024\\n.\\n^\\n\"Deepseek Log in page\"\\n.\\nDeepSeek\\n. Retrieved\\n30 January\\n2025\\n.\\n^\\n\"News | DeepSeek-R1-Lite Release 2024/11/20: 🚀 DeepSeek-R1-Lite-Preview is now live: unleashing supercharged reasoning power!\"\\n.\\nDeepSeek API Docs\\n. Archived from\\nthe original\\non 20 November 2024\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nFranzen, Carl (20 November 2024).\\n\"DeepSeek\\'s first reasoning model R1-Lite-Preview turns heads, beating OpenAI o1 performance\"\\n.\\nVentureBeat\\n.\\nArchived\\nfrom the original on 22 November 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\nHuang, Raffaele (24 December 2024).\\n\"Don\\'t Look Now, but China\\'s AI Is Catching Up Fast\"\\n.\\nThe Wall Street Journal\\n.\\nArchived\\nfrom the original on 27 December 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\n\"Release DeepSeek-R1 · deepseek-ai/DeepSeek-R1@23807ce\"\\n.\\nGitHub\\n.\\nArchived\\nfrom the original on 21 January 2025\\n. Retrieved\\n21 January\\n2025\\n.\\n^\\na\\nb\\nc\\nd\\nDeepSeek-AI; Guo, Daya; Yang, Dejian; Zhang, Haowei; Song, Junxiao; Zhang, Ruoyu; Xu, Runxin; Zhu, Qihao; Ma, Shirong (22 January 2025),\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\\n,\\narXiv\\n:\\n2501.12948\\n^\\n\"Chinese AI startup DeepSeek overtakes ChatGPT on Apple App Store\"\\n.\\nReuters\\n. 27 January 2025\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nWade, David (6 December 2024).\\n\"American AI has reached its Sputnik moment\"\\n.\\nThe Hill\\n.\\nArchived\\nfrom the original on 8 December 2024\\n. Retrieved\\n25 January\\n2025\\n.\\n^\\na\\nb\\nc\\nMilmo, Dan; Hawkins, Amy; Booth, Robert; Kollewe, Julia (28 January 2025).\\n\"\\n\\'Sputnik moment\\': $1tn wiped off US stocks after Chinese firm unveils AI chatbot\"\\n– via The Guardian.\\n^\\na\\nb\\nc\\nd\\nHoskins, Peter; Rahman-Jones, Imran (27 January 2025).\\n\"Nvidia shares sink as Chinese AI app spooks markets\"\\n.\\nBBC\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nGoldman, David (27 January 2025).\\n\"What is DeepSeek, the Chinese AI startup that shook the tech world? | CNN Business\"\\n.\\nCNN\\n. Retrieved\\n29 January\\n2025\\n.\\n^\\n\"DeepSeek poses a challenge to Beijing as much as to Silicon Valley\"\\n.\\nThe Economist\\n. 29 January 2025.\\nISSN\\n0013-0613\\n. Retrieved\\n31 January\\n2025\\n.\\n^\\nPaul, Katie; Nellis, Stephen (30 January 2025).\\n\"Chinese state-linked accounts hyped DeepSeek AI launch ahead of US stock rout, Graphika says\"\\n.\\nReuters\\n. Retrieved\\n30 January\\n2025\\n.\\n^\\n澎湃新闻 (22 January 2025).\\n\"量化巨头幻方创始人梁文锋参加总理座谈会并发言，他还创办了\"AI界拼多多\"\\n\"\\n.\\nfinance.sina.com.cn\\n. Retrieved\\n31 January\\n2025\\n.\\n^\\nShilov, Anton (27 December 2024).\\n\"Chinese AI company\\'s AI model breakthrough highlights limits of US sanctions\"\\n.\\nTom\\'s Hardware\\n.\\nArchived\\nfrom the original on 28 December 2024\\n. Retrieved\\n28 December\\n2024\\n.\\n^\\n\"DeepSeek updates – Chinese AI chatbot sparks US market turmoil, wiping $500bn off Nvidia\"\\n.\\nBBC News\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nNazareth, Rita (26 January 2025).\\n\"Stock Rout Gets Ugly as Nvidia Extends Loss to 17%: Markets Wrap\"\\n.\\nBloomberg\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nCarew, Sinéad; Cooper, Amanda; Banerjee, Ankur (27 January 2025).\\n\"DeepSeek sparks global AI selloff, Nvidia losses about $593 billion of value\"\\n. Reuters.\\n^\\na\\nb\\nSherry, Ben (28 January 2025).\\n\"DeepSeek, Calling It \\'Impressive\\' but Staying Skeptical\"\\n.\\nInc.\\nRetrieved\\n29 January\\n2025\\n.\\n^\\nOkemwa, Kevin (28 January 2025).\\n\"Microsoft CEO Satya Nadella touts DeepSeek\\'s open-source AI as \"super impressive\": \"We should take the developments out of China very, very seriously\"\\n\"\\n.\\nWindows Central\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nNazzaro, Miranda (28 January 2025).\\n\"OpenAI\\'s Sam Altman calls DeepSeek model \\'impressive\\'\\n\"\\n.\\nThe Hill\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nDou, Eva; Gregg, Aaron; Zakrzewski, Cat; Tiku, Nitasha; Najmabadi, Shannon (28 January 2025).\\n\"Trump calls China\\'s DeepSeek AI app a \\'wake-up call\\' after tech stocks slide\"\\n.\\nThe Washington Post\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nHabeshian, Sareen (28 January 2025).\\n\"Johnson bashes China on AI, Trump calls DeepSeek development \"positive\"\\n\"\\n.\\nAxios\\n.\\n^\\nKaraian, Jason; Rennison, Joe (27 January 2025).\\n\"China\\'s A.I. Advances Spook Big Tech Investors on Wall Street\"\\n– via NYTimes.com.\\n^\\nSharma, Manoj (6 January 2025).\\n\"Musk dismisses, Altman applauds: What leaders say on DeepSeek\\'s disruption\"\\n.\\nFortune India\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\n\"Elon Musk \\'questions\\' DeepSeek\\'s claims, suggests massive Nvidia GPU infrastructure\"\\n.\\nFinancialexpress\\n. 28 January 2025\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nKim, Eugene.\\n\"Big AWS customers, including Stripe and Toyota, are hounding the cloud giant for access to DeepSeek AI models\"\\n.\\nBusiness Insider\\n.\\n^\\nKerr, Dara (27 January 2025).\\n\"DeepSeek hit with \\'large-scale\\' cyber-attack after AI chatbot tops app stores\"\\n.\\nThe Guardian\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nTweedie, Steven; Altchek, Ana.\\n\"DeepSeek temporarily limited new sign-ups, citing \\'large-scale malicious attacks\\'\\n\"\\n.\\nBusiness Insider\\n.\\n^\\nField, Matthew; Titcomb, James (27 January 2025).\\n\"Chinese AI has sparked a $1 trillion panic – and it doesn\\'t care about free speech\"\\n.\\nThe Daily Telegraph\\n.\\nISSN\\n0307-1235\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\na\\nb\\nSteinschaden, Jakob (27 January 2025).\\n\"DeepSeek: This is what live censorship looks like in the Chinese AI chatbot\"\\n.\\nTrending Topics\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\na\\nb\\nLu, Donna (28 January 2025).\\n\"We tried out DeepSeek. It worked well, until we asked it about Tiananmen Square and Taiwan\"\\n.\\nThe Guardian\\n.\\nISSN\\n0261-3077\\n. Retrieved\\n30 January\\n2025\\n.\\n^\\n\"The Guardian view on a global AI race: geopolitics, innovation and the rise of chaos\"\\n.\\nThe Guardian\\n. 26 January 2025.\\nISSN\\n0261-3077\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nYang, Angela; Cui, Jasmine (27 January 2025).\\n\"Chinese AI DeepSeek jolts Silicon Valley, giving the AI race its \\'Sputnik moment\\'\\n\"\\n.\\nNBC News\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nKimery, Anthony (26 January 2025).\\n\"China\\'s DeepSeek AI poses formidable cyber, data privacy threats\"\\n.\\nBiometric Update\\n. Retrieved\\n27 January\\n2025\\n.\\n^\\nBooth, Robert; Milmo, Dan (28 January 2025).\\n\"Experts urge caution over use of Chinese AI DeepSeek\"\\n.\\nThe Guardian\\n.\\nISSN\\n0261-3077\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nHornby, Rael (28 January 2025).\\n\"DeepSeek\\'s success has painted a huge TikTok-shaped target on its back\"\\n.\\nLaptopMag\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nBurgess, Matt; Newman, Lily Hay (27 January 2025).\\n\"DeepSeek\\'s Popular AI App Is Explicitly Sending US Data to China\"\\n.\\nWired\\n.\\nISSN\\n1059-1028\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\n\"Italy regulator seeks information from DeepSeek on data protection\"\\n.\\nReuters\\n. 28 January 2025\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\nShalal, Andrea; Shepardson, David (28 January 2025).\\n\"White House evaluates effect of China AI app DeepSeek on national security, official says\"\\n.\\nReuters\\n. Retrieved\\n28 January\\n2025\\n.\\n^\\n\"Taiwan says government departments should not use DeepSeek, citing security concerns\"\\n.\\nReuters\\n. 31 January 2025\\n. Retrieved\\n31 January\\n2025\\n.\\nExternal links\\n[\\nedit\\n]\\nFree and open-source software portal\\nWikimedia Commons has media related to\\nDeepSeek\\n.\\nOfficial website\\nDeepSeek\\non\\nGitHub\\nDeepSeek\\non\\nHugging Face\\nOfficial API documentation\\nAnthology of DeepSeek papers\\nv\\nt\\ne\\nGenerative AI\\nchatbots\\nUnited States\\nChatGPT\\nClaude\\nCopilot\\nGemini\\nGrok\\nPoe\\nReplika\\nYou.com\\nRussia\\nYandexGPT\\nChina\\nDeepSeek\\nQwen\\nEurope\\nMistral\\n(France)\\nKorea\\nGalaxy AI\\nDefunct\\nBard\\nRelated\\nLarge language models\\nCategory\\nv\\nt\\ne\\nGenerative AI\\nConcepts\\nAutoencoder\\nDeep learning\\nGenerative adversarial network\\nGenerative pre-trained transformer\\nLarge language model\\nNeural network\\nPrompt engineering\\nRetrieval-augmented generation\\nReinforcement learning from human feedback\\nSelf-supervised learning\\nTransformer\\nVariational autoencoder\\nVision transformer\\nWord embedding\\nModels\\nText\\nClaude\\nDBRX\\nDeepSeek\\nGemini\\nGPT\\n1\\n2\\n3\\nJ\\nChatGPT\\n4\\n4o\\no1\\no3\\nGrok\\nGranite\\nLlama\\nMistral Large\\nPanGu-Σ\\nQwen\\nImage\\nAurora\\nDALL-E\\nFirefly\\nFlux\\nIdeogram\\nMidjourney\\nStable Diffusion\\nVideo\\nDream Machine\\nGen-3 Alpha\\nHailuo AI\\nKling\\nSora\\nVeo\\nVideoPoet\\nMusic\\nUdio\\nSuno AI\\nCompanies\\nList of artificial intelligence companies\\nCategory\\nCommons\\nRetrieved from \"\\nhttps://en.wikipedia.org/w/index.php?title=DeepSeek&oldid=1273198562\\n\"\\nCategories\\n:\\n2023 establishments in China\\nArtificial intelligence companies\\nArtificial intelligence laboratories\\nCompanies based in Hangzhou\\nTechnology companies established in 2023\\nHidden categories:\\nArticles with short description\\nShort description matches Wikidata\\nUse dmy dates from January 2025\\nArticles containing Chinese-language text\\nArticles containing simplified Chinese-language text\\nWikipedia articles that are too technical from January 2025\\nAll articles that are too technical\\nCommons category link from Wikidata\\nThis page was last edited on 1 February 2025, at 06:51\\n(UTC)\\n.\\nText is available under the\\nCreative Commons Attribution-ShareAlike 4.0 License\\n;\\nadditional terms may apply. By using this site, you agree to the\\nTerms of Use\\nand\\nPrivacy Policy\\n. Wikipedia® is a registered trademark of the\\nWikimedia Foundation, Inc.\\n, a non-profit organization.\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\nSearch\\nSearch\\nToggle the table of contents\\nDeepSeek\\n55 languages\\nAdd topic'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_for(web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to bring it together : Lets call openai completion api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages= messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(url):\n",
    "#     website = Website(url)\n",
    "    \n",
    "#     if not website or not hasattr(website, \"content\") or website.content is None:\n",
    "#         return \"Failed to retrieve website content.\"\n",
    "\n",
    "#     messages = messages_for(website)\n",
    "    \n",
    "#     if not messages or any(msg.get(\"content\") is None for msg in messages):\n",
    "#         return \"Invalid messages format, content is missing.\"\n",
    "\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=messages\n",
    "#     )\n",
    "#     return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# DeepSeek - Overview\\n\\nDeepSeek is a Chinese artificial intelligence company established in May 2023, based in Hangzhou, Zhejiang. It focuses on developing open-source large language models (LLMs) and is funded by the hedge fund High-Flyer. The founder, Liang Wenfeng, also serves as the CEO. The company aims to provide competitive AI models at a significantly lower cost compared to its rivals.\\n\\n## Key Models and Releases:\\n- **DeepSeek-R1**: The flagship model comparable to OpenAI\\'s GPT-4, designed to be cost-effective and efficient in resource usage.\\n- **DeepSeek-V2 and V3**: Subsequent models that have improved upon the previous versions, with advancements in training methods and model architectures.\\n- **DeepSeek AI Assistant**: A chatbot released in January 2025 that quickly became popular, surpassing ChatGPT in downloads in the iOS App Store by the end of January 2025.\\n\\n## Achievements and Market Impact:\\n- By January 2025, DeepSeek-R1 had become the most downloaded free app in the iOS App Store in the U.S.\\n- The success of DeepSeek prompted a significant drop in the stock prices of major tech companies, notably Nvidia, which saw an 18% decline following the app\\'s release.\\n\\n## Concerns:\\nThere are several documented concerns surrounding DeepSeek:\\n- **Censorship**: The AI has built-in mechanisms to avoid sensitive political topics, reflecting government regulations in China.\\n- **Security and Privacy**: Fears exist regarding the potential use of DeepSeek\\'s models for disinformation campaigns or surveillance, alongside concerns of data collection by the service.\\n\\n## Reactions to DeepSeek:\\nThe release and performance of DeepSeek’s models have drawn attention from global tech leaders, sparking comparisons to critical moments in the AI industry, such as the \"Sputnik moment\" in context to AI development. Responses from different sectors range from admiration for its capabilities to skepticism regarding its sustainability and implications.\\n\\nOverall, DeepSeek represents a significant challenge in the AI sector, especially for established American companies, highlighting the potential influence of Chinese technologies in the global market.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://en.wikipedia.org/wiki/DeepSeek\")\n",
    "# summarize(\"https://edwarddonner.com\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE can use feature of jupyter to nicely display the content of markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DeepSeek - Overview\n",
       "\n",
       "**DeepSeek** is a Chinese artificial intelligence company founded in May 2023 by Liang Wenfeng, based in Hangzhou, Zhejiang. The company specializes in developing open-source large language models (LLMs). DeepSeek is funded by the hedge fund High-Flyer and has reported under 200 employees. \n",
       "\n",
       "## Key Features\n",
       "- **Models**: The DeepSeek-R1 model competes with notable LLMs like OpenAI's GPT-4, offering similar performance at significantly reduced costs—approximately $6 million compared to $100 million for GPT-4.\n",
       "- **Open-source**: All models and their training methodologies are open-source, promoting transparency and facilitating broader development and research.\n",
       "- **Innovations**: DeepSeek has released several models, including DeepSeek-V2 and V3, which are engineered for enhanced performance and economics compared to competitors.\n",
       "\n",
       "## Recent Developments\n",
       "1. **Chatbot Release**: On January 10, 2025, DeepSeek launched a free chatbot application for iOS and Android based on the DeepSeek-R1 model, which quickly became the top free app on the iOS App Store in the United States.\n",
       "2. **Market Impact**: Following the success of DeepSeek's app, shares of major tech companies like Nvidia and Microsoft saw significant declines, contributing to a broader sell-off in technology stocks which saw a loss of about $593 billion in market capitalization.\n",
       "3. **Cybersecurity Concerns**: A cyberattack on January 27, 2025, led DeepSeek to limit new user registrations and caused service slowdowns.\n",
       "\n",
       "## Concerns\n",
       "- **Censorship**: The company has faced scrutiny over political sensitivities, with reports indicating that its AI models employ censorship on certain topics considered politically sensitive in China.\n",
       "- **Privacy**: There are concerns about data privacy, with implications regarding how personal data is handled and potential misuse of AI technology for government surveillance or influence operations.\n",
       "\n",
       "## Conclusion\n",
       "DeepSeek’s rapid rise and innovative AI technologies mark significant developments in the AI sector, particularly against the backdrop of global tensions and competition in technology. Its impact has been likened to a \"Sputnik moment\" for American AI, indicating a shift in the balance of AI power towards China."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://en.wikipedia.org/wiki/DeepSeek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their are some websites which cannot be scrapped with BeautifulSoap as they uses JavaScript to render the webpage, The solution for such a web scrapping is Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WebPage Summarizer with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = ollama.chat(\n",
    "        model = MODEL,\n",
    "        messages= messages_for(website)\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a summary of the article about DeepSeek:\\n\\n**What is DeepSeek?**\\n\\nDeepSeek is an open-source, generative AI chatbot developed in China. It was launched in 2023 and has gained significant attention due to its impressive language generation capabilities.\\n\\n**Key Features**\\n\\nDeepSeek is built on top of transformer-based architectures and uses a combination of self-supervised learning and reinforcement learning from human feedback to improve its performance.\\n\\n**Capabilities**\\n\\nDeepSeek can generate human-like text, answer questions, and even engage in conversations. It has been trained on a large dataset of texts and has demonstrated impressive capabilities in tasks such as:\\n\\n* Text summarization\\n* Question answering\\n* Sentiment analysis\\n* Dialogue generation\\n\\n**Concerns and Controversies**\\n\\nThere have been concerns raised about the potential risks of using DeepSeek, including:\\n\\n* **Data privacy**: Some experts have expressed concerns that DeepSeek may be collecting user data without their consent.\\n* **Security**: There are worries that DeepSeek's open-source nature makes it vulnerable to cyber attacks and exploitation by malicious actors.\\n* **Bias**: Some researchers have raised concerns about the potential for bias in DeepSeek's training data, which could result in biased or discriminatory outputs.\\n\\n**Reactions from Experts**\\n\\nMany experts in the field of natural language processing (NLP) and AI have expressed interest in exploring the capabilities and limitations of DeepSeek. However, others have raised important questions about its safety and ethics.\\n\\nOverall, DeepSeek represents an exciting development in the field of generative AI, but it also highlights the need for careful consideration and regulation of AI technologies to ensure they are used responsibly and with respect for human values.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://en.wikipedia.org/wiki/DeepSeek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\"DeepSeek\" is a Chinese artificial intelligence (AI) company that has gained significant attention for its generative AI technology. Here's an overview:\n",
       "\n",
       "**Founding and Background**\n",
       "\n",
       "DeepSeek was founded in 2023, but the exact date of founding is not publicly available. The company is based in Hangzhou, China.\n",
       "\n",
       "**Technology and Capabilities**\n",
       "\n",
       "DeepSeek's technology focuses on generating text, images, and videos using deep learning techniques. Their platform can produce high-quality content, including articles, blog posts, social media posts, product descriptions, and more.\n",
       "\n",
       "**Key Features**\n",
       "\n",
       "Some key features of DeepSeek's AI technology include:\n",
       "\n",
       "1. **Generative Adversarial Network (GAN)**: A technique used to generate realistic images, videos, or text.\n",
       "2. **Large Language Model**: Capable of processing vast amounts of text data and generating coherent, context-specific responses.\n",
       "\n",
       "**Applications and Use Cases**\n",
       "\n",
       "DeepSeek's technology has various applications across industries, such as:\n",
       "\n",
       "1. **Content Generation**: Create high-quality content for websites, blogs, social media platforms, and more.\n",
       "2. **Chatbots and Virtual Assistants**: Develop conversational AI systems that can engage with users in a natural, human-like manner.\n",
       "3. **Product Description and Marketing**: Generate compelling product descriptions, marketing materials, and sales copy.\n",
       "\n",
       "**Concerns and Controversies**\n",
       "\n",
       "As with any emerging AI technology, DeepSeek's platform has raised concerns about:\n",
       "\n",
       "1. **Data Security**: The potential for user data to be compromised or misused by the company.\n",
       "2. **Content Quality**: The possibility of generated content being low-quality, biased, or factually inaccurate.\n",
       "\n",
       "**Reception and Impact**\n",
       "\n",
       "DeepSeek's technology has received attention from various stakeholders, including:\n",
       "\n",
       "1. **Media Outlets**: Featured in prominent publications, such as Reuters, CNN, and Forbes.\n",
       "2. **Industry Experts**: Praised for its potential to transform content creation and customer engagement.\n",
       "3. **Concerned Groups**: Cited concerns about data security, content quality, and the implications of large language models on society.\n",
       "\n",
       "Overall, DeepSeek's emergence marks an exciting development in the field of generative AI, with potential applications across various industries. However, it also highlights the need for careful consideration of the technologies' limitations and risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://en.wikipedia.org/wiki/DeepSeek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
