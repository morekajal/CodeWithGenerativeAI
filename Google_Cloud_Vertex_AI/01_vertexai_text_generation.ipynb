{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import vertexai\n",
    "from google.auth.transport.requests import Request\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "LOCATION = os.getenv(\"LOCATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is important step to connect API key with library, without this it was giving PErmission denied error.\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the library with your API key\n",
    "genai.configure(api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vertex ai\n",
    "vertexai.init(project=PROJECT_ID,\n",
    "              location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gemini-1.5-flash-001\"\n",
    "model = \"gemini-2.0-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(\"What are best features of Gemini-2.0-Flash model over other gemini models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini 2.0 Flash is designed for speed and efficiency, making it stand out from other Gemini models in several key areas:\n",
      "\n",
      "*   **Speed:** Gemini 2.0 Flash prioritizes faster response times. It's engineered to be quicker at processing prompts and generating outputs compared to the larger, more complex Gemini models. This is crucial for applications where low latency is essential.\n",
      "*   **Cost-Effectiveness:** Given its focus on efficiency, Gemini 2.0 Flash is generally more cost-effective to use. It consumes fewer computational resources per request, making it a more economical choice for high-volume applications or projects with budget constraints.\n",
      "*   **Ideal for Automation and High-Throughput Tasks:** Its speed and cost benefits make Gemini 2.0 Flash well-suited for automating repetitive tasks, processing large quantities of data quickly, and integrating into systems where rapid responses are needed.\n",
      "*   **Suitable for Simple Tasks:** While not as capable as larger models in handling highly complex or nuanced tasks, Gemini 2.0 Flash excels at tasks that require quick, straightforward responses, such as basic summarization, data extraction, or simple question answering.\n",
      "*   **Smaller Model Size:** A smaller model size typically translates to lower memory requirements and easier deployment on various platforms, including edge devices or resource-constrained environments.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Indian Rupee (INR) and the U.S. Dollar (USD) have a complex and dynamic relationship in the global market. Here's a comparison:\n",
      "\n",
      "**Key Aspects of the Comparison:**\n",
      "\n",
      "*   **Exchange Rate:** This is the most direct comparison. It tells you how many Indian Rupees are needed to buy one U.S. Dollar. This rate fluctuates constantly based on market forces.\n",
      "*   **Factors Influencing the Exchange Rate:**\n",
      "    *   **Economic Growth:** Stronger economic growth in the U.S. tends to strengthen the USD, while stronger growth in India can strengthen the INR. However, relative growth matters more. If the U.S. economy is growing faster than India's, the USD is likely to appreciate against the INR.\n",
      "    *   **Inflation:** Higher inflation in India relative to the U.S. generally weakens the INR. This is because inflation erodes the purchasing power of the currency.\n",
      "    *   **Interest Rates:** Higher interest rates in the U.S. can attract foreign investment, increasing demand for the USD and strengthening it. Conversely, higher interest rates in India can attract investment and strengthen the INR.\n",
      "    *   **Trade Balance:** A trade deficit for India (importing more than exporting) can put downward pressure on the INR, as more Rupees are needed to buy foreign goods and services.\n",
      "    *   **Capital Flows:** Large inflows of foreign investment into India can strengthen the INR, while outflows can weaken it.\n",
      "    *   **Geopolitical Events:** Global events, political instability, and investor sentiment can all impact the exchange rate.\n",
      "    *   **Central Bank Intervention:** The Reserve Bank of India (RBI) can intervene in the foreign exchange market to manage volatility and influence the exchange rate. The U.S. Federal Reserve can also influence the exchange rate, though it typically intervenes less directly.\n",
      "*   **Volatility:** The INR is generally considered more volatile than the USD. This means its exchange rate can fluctuate more significantly in shorter periods.\n",
      "*   **Reserve Currency Status:** The USD is the world's primary reserve currency. This means that many countries hold large reserves of USD. The INR is not a reserve currency.\n",
      "*   **Global Trade and Finance:** The USD is the dominant currency for international trade and finance. While the INR is used in some regional trade, its role is much smaller.\n",
      "*   **Impact on Indian Economy:** A weaker INR can make Indian exports more competitive but\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look at some configuration parameters\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature = 0.5,\n",
    "    top_p = 0.8,\n",
    "    top_k = 32,\n",
    "    candidate_count = 1,\n",
    "    max_output_tokens = 512,\n",
    ")\n",
    "\n",
    "response = genai.GenerativeModel(\"gemini-2.0-flash\").generate_content(\"What is the comparison of Indian rupees with U.S dollers in global market?\",\n",
    "                                                                      generation_config=generation_config)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make simple chat application with gemini models, we can directly use the inbuid method start_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"2 + 2 = 4 \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 66,\n",
       "        \"candidates_token_count\": 7,\n",
       "        \"total_token_count\": 73\n",
       "      },\n",
       "      \"model_version\": \"gemini-1.5-flash-001\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.send_message('Hi, what is 2+2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_output = chat.send_message('Hi, what is 2+2')\n",
    "print(text_output.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Hi, what is 2+2\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"2 + 2 = 4 \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Hi, what is 2+2\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"2 + 2 = 4 \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Hi, what is 2+2\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"2 + 2 = 4 \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Hi, what is 2+2\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"2 + 2 = 4 \\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Hi, what is 2+2\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"2 + 2 = 4 \\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "conv1 = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a comprehensive guide to learning GCP Vertex AI Operations for GenAI developers, along with key concepts and resources:\n",
      "\n",
      "**Understanding GCP Vertex AI Operations**\n",
      "\n",
      "Vertex AI Operations is a crucial aspect of deploying and managing your GenAI models in production. It provides the tools and infrastructure to:\n",
      "\n",
      "* **Model Deployment:** Deploy trained GenAI models (like large language models, image generators, etc.) to a production environment.\n",
      "* **Model Monitoring:** Continuously track your model's performance, identify potential issues, and ensure its accuracy and reliability over time.\n",
      "* **Model Management:** Organize, version, and control access to your GenAI models, streamlining their lifecycle.\n",
      "* **Model Explainability:** Understand the rationale behind your model's predictions, which is essential for trust and responsible AI.\n",
      "* **Model Optimization:** Fine-tune and optimize your models for better performance and efficiency.\n",
      "\n",
      "**Steps to Get Started**\n",
      "\n",
      "1. **Familiarize Yourself with GCP Concepts:**\n",
      "\n",
      "   * **Compute Engine:** Understand how virtual machines and instances work.\n",
      "   * **Cloud Storage:** Learn about storing and managing your data.\n",
      "   * **Cloud Run:** Gain knowledge about deploying containerized applications.\n",
      "   * **Kubernetes Engine (GKE):** Familiarize yourself with container orchestration.\n",
      "   * **APIs:** Learn how to interact with GCP services programmatically.\n",
      "\n",
      "2. **Focus on Vertex AI's GenAI Capabilities:**\n",
      "\n",
      "   * **Vertex AI Workbench:** This is your central hub for managing your GenAI projects.\n",
      "   * **Vertex AI Model Garden:** Explore pre-trained models and customize them for your use case.\n",
      "   * **Vertex AI Model Deployment:** Learn to deploy your models to different environments (e.g., online predictions, batch predictions).\n",
      "   * **Vertex AI Monitoring:** Understand how to monitor your model's performance, identify drifts, and retrain as needed.\n",
      "   * **Vertex AI Explainability:** Explore tools for understanding your model's decisions.\n",
      "\n",
      "3. **Learn Through Official Resources:**\n",
      "\n",
      "   * **Google Cloud Documentation:** The definitive source for documentation, tutorials, and API reference: [https://cloud.google.com/vertex-ai/docs](https://cloud.google.com/vertex-ai/docs)\n",
      "   * **Vertex AI Tutorials:** Get hands-on experience with practical examples: [https://cloud.google.com/vertex-ai/docs/tutorials](https://cloud.google.com/vertex-ai/docs/tutorials)\n",
      "   * **Qwiklabs:** Interactive labs to reinforce your learning: [https://www.qwiklabs.com/](https://www.qwiklabs.com/)\n",
      "   * **Google Cloud Skills Boost:** Learn at your own pace with courses and quizzes: [https://cloud.google.com/training/paths](https://cloud.google.com/training/paths)\n",
      "\n",
      "4. **Use the Vertex AI SDK:**\n",
      "\n",
      "   * **Python:** The primary language for interacting with Vertex AI.\n",
      "   * **Sample Code:** Google provides numerous code examples to get you started: [https://cloud.google.com/vertex-ai/docs/samples](https://cloud.google.com/vertex-ai/docs/samples)\n",
      "\n",
      "5. **Engage with the Community:**\n",
      "\n",
      "   * **Stack Overflow:** Ask questions and get help from other developers: [https://stackoverflow.com/](https://stackoverflow.com/)\n",
      "   * **Google Cloud Community:** Join discussions and connect with experts: [https://cloud.google.com/community](https://cloud.google.com/community)\n",
      "\n",
      "**Key Concepts for GenAI Developers**\n",
      "\n",
      "* **Model Serving:** Deploying a model so it can process new data and generate predictions.\n",
      "* **Latency and Throughput:** Understanding how quickly your model responds and how many requests it can handle.\n",
      "* **Scalability:** Ensuring your model can handle increasing demands.\n",
      "* **Cost Optimization:** Balancing performance with cost considerations.\n",
      "* **Security:** Protecting your model from unauthorized access and attacks.\n",
      "* **Data Governance:** Complying with regulations and best practices for data privacy and fairness.\n",
      "\n",
      "**Example Scenario:**\n",
      "\n",
      "Imagine you've trained a GenAI model to generate creative text formats (like poems or scripts). You've deployed it to a production environment using Vertex AI Operations. Here's how you might use these tools:\n",
      "\n",
      "* **Model Deployment:** You deploy the model to a Cloud Run instance for efficient serving.\n",
      "* **Model Monitoring:** You set up alerts to monitor for changes in the model's performance, such as a decline in the quality of the generated text.\n",
      "* **Model Management:** You create different versions of the model as you refine it and ensure seamless transitions between versions.\n",
      "* **Model Explainability:** You use Vertex AI Explainability to understand how the model makes its decisions, enhancing trust and responsible AI.\n",
      "* **Model Optimization:** You experiment with different model configurations and hyperparameters to improve the quality and efficiency of the generated text.\n",
      "\n",
      "**Remember:**\n",
      "\n",
      "* **Start small and iterate:** Begin with simple deployments and gradually increase complexity.\n",
      "* **Embrace best practices:** Use secure development practices and adhere to GCP guidelines.\n",
      "* **Stay up-to-date:** GenAI technology evolves rapidly. Keep learning and adapting your skills. \n",
      "\n",
      "By following these steps and leveraging Google Cloud's resources, you can effectively learn GCP Vertex AI Operations and become a skilled GenAI developer who can successfully deploy and manage cutting-edge AI models in the cloud.\n"
     ]
    }
   ],
   "source": [
    "res = conv1.send_message(\"I want to learn GCP vertexai operations as a GenAI developer, how do I start?\")\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a more in-depth look at the Model Deployment aspect of GCP Vertex AI Operations, especially relevant for GenAI developers:\n",
      "\n",
      "**Model Deployment in Vertex AI**\n",
      "\n",
      "The process of deploying a GenAI model in Vertex AI is about making your trained model accessible and ready to handle real-world requests. Here's a breakdown:\n",
      "\n",
      "1. **Choose the Deployment Target:**\n",
      "\n",
      "   * **Vertex AI Endpoint:** Best for online prediction scenarios where you need real-time responses. You can deploy your model to an endpoint and send requests to it using an API. Think of it as a web service specifically for your model.\n",
      "   * **Batch Prediction:** Ideal for large-scale tasks where you want to process a significant amount of data at once, like generating content for a bulk email campaign. Vertex AI will process your data and deliver the results to a chosen location.\n",
      "\n",
      "2. **Prepare Your Model:**\n",
      "\n",
      "   * **Serialization:** Save your trained model in a format compatible with Vertex AI. This might involve saving the weights and architecture of a neural network.\n",
      "   * **Containerization (Optional):** Package your model and any dependencies (like Python libraries) into a Docker container. This ensures a consistent runtime environment across different environments and facilitates deployment.\n",
      "   * **Deployment Configuration:** Define settings for the deployment, including the number of replicas (instances of your model running), memory and CPU resources, and custom environment variables.\n",
      "\n",
      "3. **Deployment Process:**\n",
      "\n",
      "   * **Vertex AI Workbench:**  This is the user interface for deployment.  You can use its intuitive workflow to upload your model, choose your deployment target (Endpoint or Batch Prediction), and set up configurations.\n",
      "   * **Vertex AI SDK:** For programmatic deployment, use the Python SDK to create deployment objects, configure settings, and submit the deployment to Vertex AI. This gives you granular control over the process.\n",
      "\n",
      "4. **Deployment Types (for Endpoints):**\n",
      "\n",
      "   * **Online Prediction:**  Designed for real-time, low-latency requests. Ideal for applications like chatbots, recommendation engines, and text generation.\n",
      "   * **Explainability:** You can enable explainability features during deployment to get insights into your model's decisions, which is especially important for GenAI models that generate content or make creative choices.\n",
      "\n",
      "**Key Considerations for GenAI Model Deployment:**\n",
      "\n",
      "* **Latency:**  GenAI models often involve complex computations, which can affect the speed of your model's predictions. Consider the latency requirements of your application.\n",
      "* **Throughput:**  How many requests can your model handle per second?  This is crucial for ensuring your service can scale to meet demand.\n",
      "* **Scalability:**  The ability of your model to handle an increase in requests or data volume. Vertex AI can automatically scale your deployments to meet demand.\n",
      "* **Resource Optimization:**  Efficiently allocate resources (CPU, memory, etc.) to your model to balance performance and cost.\n",
      "* **Security:**  Implement security measures to protect your model, data, and the entire deployment infrastructure.\n",
      "* **Model Monitoring:**  Continuously monitor your model's performance in production. Track metrics like latency, throughput, and prediction accuracy.\n",
      "\n",
      "**Example (Python SDK for Endpoint Deployment):**\n",
      "\n",
      "```python\n",
      "from google.cloud import aiplatform\n",
      "\n",
      "# Create an endpoint\n",
      "endpoint = aiplatform.Endpoint.create(\n",
      "    display_name='my-genai-model-endpoint',\n",
      "    project='your-gcp-project-id',\n",
      "    location='us-central1',  # Choose your region\n",
      ")\n",
      "\n",
      "# Deploy a model to the endpoint\n",
      "model = aiplatform.Model.from_pretrained(\n",
      "    model_id='your-model-id'  # Get the model ID from Model Garden or your training process\n",
      ")\n",
      "\n",
      "deployed_model = endpoint.deploy(\n",
      "    model=model,\n",
      "    deployed_model_display_name='my-genai-model-deployment',\n",
      "    traffic_percentage=100,  # Allocate 100% of traffic to this deployment\n",
      ")\n",
      "\n",
      "# Now, your model is ready to handle requests. \n",
      "# Use the endpoint's predict() method to send requests.\n",
      "```\n",
      "\n",
      "**Further Tips:**\n",
      "\n",
      "* **Start with a small deployment:** Test your model in production with limited traffic initially to identify any issues before scaling up.\n",
      "* **Use canary releases:** Gradually roll out updates to your model, allowing you to monitor performance before migrating all traffic.\n",
      "* **Automate your deployment workflow:** Use tools like Continuous Integration and Continuous Deployment (CI/CD) to streamline model deployment.\n",
      "\n",
      "By understanding the nuances of model deployment in Vertex AI Operations, GenAI developers can confidently deliver their models into production environments and build robust and reliable AI-powered applications. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = conv1.send_message(\"Can you ellaborate more on Model deployment part\")\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
